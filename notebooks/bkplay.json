{"paragraphs":[{"text":"%sh\n\n# cell 1\n\n# STEP 1 OF 6 FOR ILLUSTRATION - just prove Kafka is running\n\n# make sure Kafka is running by listing available topics\n\ncd /tmp/datastax-spark-streaming-workshop/kafka_2.12-2.2.0/bin\n\n./kafka-topics.sh --list --bootstrap-server localhost:9092","dateUpdated":"2019-07-12T15:28:43+0000","config":{"colWidth":12,"editorMode":"ace/mode/sh","results":{},"enabled":true,"editorSetting":{"language":"sh","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1562945323805_1103298001","id":"20190630-151028_871499823","dateCreated":"2019-07-12T15:28:43+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:294"},{"text":"%sh\n\n# cell 2\n\n# THIS IS NOT A STEP FOR ILLUSTRATION (it's just a utility in case I need to prove I can consume kafka events)\n\n# This utility consumes events (which proves we are generating them)\n\ncd /tmp/datastax-spark-streaming-workshop/kafka_2.12-2.2.0/bin\n\n./kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic rating_modifiers ### --from-beginning ","dateUpdated":"2019-07-12T15:28:43+0000","config":{"tableHide":false,"editorSetting":{"language":"sh","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/sh","results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1562945323810_1090601287","id":"20190630-151549_1055871517","dateCreated":"2019-07-12T15:28:43+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:295"},{"text":"%sh\n\n# cell 3\n\n# STEP 2 OF 6 FOR ILLUSTRATION - start a *working* version of my event consumer... does NOT use DSE, uses .master(\"local[*]\")\n\n# This is BK's standalone Spark Streaming Event Consumer (does not use dse spark-submit... in fact, does not use spark-submit at all)\n\ncd /tmp/datastax-spark-streaming-workshop/executables\n\njava -cp datastax-spark-streaming-jar-with-dependencies.jar com.datastax.kellett.Lab2LinuxStandalone","dateUpdated":"2019-07-12T15:28:43+0000","config":{"colWidth":12,"editorMode":"ace/mode/sh","results":{},"enabled":true,"editorSetting":{"language":"sh","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1562945323810_1090601287","id":"20190707-201330_849802427","dateCreated":"2019-07-12T15:28:43+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:296"},{"text":"%sh\n\n# cell 4\n\n# STEP 3 OF 6 FOR ILLUSTRATION - begin generating events\n\n# This program generates events used for the workshop\n\ncd /tmp/datastax-spark-streaming-workshop\n\njava -jar executables/kafka-event-generator-1.0-SNAPSHOT-jar-with-dependencies.jar\n\n","dateUpdated":"2019-07-12T15:28:43+0000","config":{"tableHide":false,"editorSetting":{"language":"sh","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/sh","results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1562945323811_1090216538","id":"20190630-152714_2036339272","dateCreated":"2019-07-12T15:28:43+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:297"},{"text":"%md\n\ncell 5 \n\nSTEP 4 OF 6 FOR ILLUSTRATION\n\nAt this point, we should see that the standalone version of the event consumer is working (look at output in cell 3).  This proves that:\n- my code is good\n- my build is good in a non-DSE environment\n\n*__Once you see that things are working, kill the process in cell 3 (Problem Illustration Step 2 - standalone event consumer) by hitting the \"pause\" icon.__*\n","dateUpdated":"2019-07-12T15:28:43+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>cell 5 </p>\n<p>STEP 4 OF 6 FOR ILLUSTRATION</p>\n<p>At this point, we should see that the standalone version of the event consumer is working (look at output in cell 3). This proves that:<br/>- my code is good<br/>- my build is good in a non-DSE environment</p>\n<p><em><strong>Once you see that things are working, kill the process in cell 3 (Problem Illustration Step 2 - standalone event consumer) by hitting the &ldquo;pause&rdquo; icon.</strong></em></p>\n</div>"}]},"apps":[],"jobName":"paragraph_1562945323812_1088292794","id":"20190708-124743_240066369","dateCreated":"2019-07-12T15:28:43+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:298"},{"text":"%sh\n\n# cell 6\n\n# STEP 5 OF 6 FOR ILLUSTRATION - start the DSE version of event consumer in the DSE environment \n\n# This is BK's Spark Streaming Event Consumer using dse spark-submit\n\ncd /tmp/datastax-spark-streaming-workshop/executables\n\ndse spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.2.3,org.apache.spark:spark-streaming-kafka-0-10_2.11:2.2.3 --class com.datastax.kellett.Lab2theRightWay streaming-spitzer-compliant-0.1.jar","dateUpdated":"2019-07-12T15:28:43+0000","config":{"colWidth":12,"editorMode":"ace/mode/sh","results":{},"enabled":true,"editorSetting":{"language":"sh","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1562945323814_1089062292","id":"20190709-200352_812341603","dateCreated":"2019-07-12T15:28:43+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:299"},{"text":"%md\n\ncell 7 (ignore the dse.version text box above.  I don't know why the markdown is generating it.)\n\nSTEP 6 OF 6 FOR  ILLUSTRATION\n\n**_Now kill the process in cell 4 (Problem Illustration Step 3 - event generator) by hitting the \"pause\" icon_**\n\n_Below is reference information._\n\nHere is the source code for the standalone version of the consumer:\n\n```\npackage com.datastax.kellett;\n\nimport org.apache.log4j.Level;\nimport org.apache.log4j.Logger;\nimport org.apache.spark.sql.Dataset;\nimport org.apache.spark.sql.Row;\nimport org.apache.spark.sql.SparkSession;\nimport org.apache.spark.sql.streaming.OutputMode;\nimport org.apache.spark.sql.streaming.StreamingQuery;\nimport org.apache.spark.sql.streaming.StreamingQueryException;\n\npublic class Lab2LinuxStandalone {\n\n    public static void main(String[] args) throws StreamingQueryException {\n\n        Logger.getLogger(\"org.apache\").setLevel(Level.WARN);\n        Logger.getLogger(\"org.apache.spark.storage\").setLevel(Level.ERROR);\n\n        // Connect to Spark\n        SparkSession session = SparkSession.builder()\n                .master(\"local[*]\")\n                .appName(\"Spark Structured Streaming Lab2 Linux Version\") // any name you like --- displays in Spark Management UI\n          //      .enableHiveSupport()    // Enables connection to a Hive metastore, support for hive SerDes and Hive user-defined functions\n                .getOrCreate();\n\n        // Read available incoming events into a data frame\n        Dataset<Row> df = session.readStream()\n                //       .format(\"kafka\") // This is supposed to work, but apparently the short-name reference resolves incorrectly\n                .format(\"org.apache.spark.sql.kafka010.KafkaSourceProvider\")\n                .option(\"kafka.bootstrap.servers\", \"localhost:9092\")\n                .option(\"subscribe\", \"rating_modifiers\")\n                .load();\n\n        // Register a table with Spark - this will hold the incoming events from Kafka\n        df.createOrReplaceTempView(\"rating_modifiers_incoming\");\n\n        // key, value, timestamp are the columns available to me\n        // Note that you must cast value to a string in order to get readable results\n        Dataset<Row> results = session.sql(\n                \"select cast(value as string) as event_detail from rating_modifiers_incoming\");\n\n        // Create a sink\n        // See https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#output-sinks for all sink types\n        StreamingQuery query = results.writeStream().format(\"console\")\n                .outputMode(OutputMode.Append()) // values are Complete, Update, and Append - use Append for non-aggregation queries\n                .option(\"truncate\", false)\n                .option(\"numRows\", 30)\n                .start();\n\n        // Rinse and repeat\n        query.awaitTermination();\n    }\n}\n\n```\n\nHere is the .pom for the standalone version of the consumer:\n\n```\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n\txsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\">\n\t<modelVersion>4.0.0</modelVersion>\n\t<url>http://maven.apache.org</url>\n\t\n    <!-- above is all boilerplate\n        \n\t\tNOTE that I generated my base pom and project directory structure with the following maven command (run from \n\t\tthe directory ABOVE where you want the project to be created... e.g. /maven):\n        mvn archetype:generate -DgroupId=com.datastax.kellett -DartifactId=datastax-spark-streaming -DarchetypeArtifactId=maven-archetype-quickstart -DinteractiveMode=false\n\n        The junit dependency was created automatically for me, but I pasted in the other dependencies (if needed)\n        and the <repositories> section (if needed) and the <properties> section (if needed)\n\t\tand the <build> section all BEFORE importing the project into IntelliJ.  \n\t\tI imported by pointing to the datastax-spark-sql-v2 directory, and imported as Maven project.\n    -->\t\n\t\n\t<groupId>com.datastax.kellett</groupId>\n\t<artifactId>datastax-spark-streaming</artifactId>\n\t<version>0.0.1-SNAPSHOT</version>\n\t<packaging>jar</packaging>\n\t<name>datastax-spark-streaming</name>\n\n\t<properties> <!-- Not currently used, but we keep them as doc and for possible future use -->\n\t\t<java.version>1.8</java.version>\n\t\t<dse.version>6.0.7</dse.version>\n        <scala.version>2.11.8</scala.version>\n\t</properties>\n\n    <dependencies>\n        <dependency>\n            <groupId>junit</groupId>\n            <artifactId>junit</artifactId>\n            <version>3.8.1</version>\n            <scope>test</scope>\n        </dependency>\n\n<!-- if below version (2.3.2) doesn't work, try 2.4.0 -->\n        <dependency> <!-- added 7/2/19 -->\n            <groupId>org.apache.spark</groupId>\n            <artifactId>spark-core_2.11</artifactId> <!-- 2.11 is Scala version used to build Spark -->\n            <version>2.3.2</version> \t\t\t\t<!-- this number refers to the Spark version -->\n        </dependency>  <!--see https://spark.apache.org/downloads.html to get latest info for both of the above -->\n\n        <dependency>\n            <groupId>org.apache.spark</groupId>\n            <artifactId>spark-sql_2.11</artifactId> <!-- 2.11 is Scala version used to build Spark -->\n            <version>2.3.2</version>  \t\t\t\t<!-- this number refers to the Spark version -->\n        </dependency>  <!--see https://spark.apache.org/downloads.html to get latest info for both of the above -->\n\t\t\n\t\t<dependency> <!-- This is for Spark Streaming (Dstream only???) -->\n\t\t\t<groupId>org.apache.spark</groupId>\n\t\t\t<artifactId>spark-streaming_2.11</artifactId> <!-- 2.11 is Scala version used to build Spark -->\n\t\t\t<version>2.3.2</version>  \t\t\t\t<!-- this number refers to the Spark version -->\n\t\t</dependency>  <!--see https://spark.apache.org/downloads.html to get latest info for both of the above -->\n\n\t\t<dependency> <!-- This is for Spark-Kafka integration -->\n\t\t\t<groupId>org.apache.spark</groupId>\n\t\t\t<artifactId>spark-streaming-kafka-0-10_2.11</artifactId> <!-- 2.11 is Scala version used to build Spark -->\n\t\t\t<version>2.3.2</version>  \t\t\t\t<!-- this number refers to the Spark version -->\n\t\t</dependency>   <!--see https://spark.apache.org/docs/2.3.2/streaming-kafka-integration.html to get latest info for the above -->\n\n\t\t<dependency> <!-- This is for Spark-Kafka STRUCTURED STREAMING -->\n\t\t\t<groupId>org.apache.spark</groupId>\n\t\t\t<artifactId>spark-sql-kafka-0-10_2.11</artifactId> <!-- 2.11 is Scala version used to build Spark -->\n\t\t\t<version>2.3.2</version>  \t\t\t\t<!-- this number refers to the Spark version -->\n\t\t</dependency>\n\n        <dependency> <!-- added 7/2/19 -->\n            <groupId>org.apache.hadoop</groupId>\n            <artifactId>hadoop-hdfs</artifactId>\n            <version>2.2.0</version>\n        </dependency>\n\t\t\n\t</dependencies>\n\n    <build>\n        <finalName>datastax-spark-streaming</finalName>\n        <plugins>\n            <plugin>\n                <groupId>org.apache.maven.plugins</groupId>\n                <artifactId>maven-assembly-plugin</artifactId>\n                <executions>\n                    <execution>\n                        <id>create-my-bundle</id>\n                        <phase>package</phase>\n                        <goals>\n                            <goal>single</goal>\n                        </goals>\n                        <configuration>\n\n                            <archive>\n                                <manifest>\n                                    <mainClass>\n                                        com.datastax.kellett.Lab2Windows\n                                    </mainClass>\n                                </manifest>\n                            </archive>\n\n                            <descriptorRefs>\n                                <descriptorRef>jar-with-dependencies</descriptorRef>\n                            </descriptorRefs>\n                        </configuration>\n                    </execution>\n                </executions>\n            </plugin>\n        </plugins>\n    </build>\n\n    <repositories>\n        <repository>\n            <id>DataStax-Repo</id>\n            <url>https://datastax.artifactoryonline.com/datastax/public-repos/</url>\n        </repository>\n    </repositories>\n\n</project>\n\n```\n\nHere is the source code for the DSE version of the consumer:\n\n```\npackage com.datastax.kellett;\n\nimport org.apache.log4j.Level;\nimport org.apache.log4j.Logger;\nimport org.apache.spark.sql.Dataset;\nimport org.apache.spark.sql.Row;\nimport org.apache.spark.sql.SparkSession;\nimport org.apache.spark.sql.streaming.OutputMode;\nimport org.apache.spark.sql.streaming.StreamingQuery;\nimport org.apache.spark.sql.streaming.StreamingQueryException;\n\npublic class Lab2theRightWay {\n\n    public static void main(String[] args) throws StreamingQueryException {\n\n        Logger.getLogger(\"org.apache\").setLevel(Level.WARN);\n        Logger.getLogger(\"org.apache.spark.storage\").setLevel(Level.ERROR);\n\n        // Connect to Spark\n        System.out.println(\"Lab2theRightWay test 001 - initial\");\n        SparkSession session = SparkSession.builder()\n                .master(\"local[*]\")\n                .appName(\"Spark Structured Streaming Lab2theRightWay\") // any name you like --- displays in Spark Management UI\n                //      .enableHiveSupport()    // Enables connection to a Hive metastore, support for hive SerDes and Hive user-defined functions\n                .getOrCreate();\n\n        // Read available incoming events into a data frame\n        Dataset<Row> df = session.readStream()\n                //       .format(\"kafka\") // This is supposed to work, but apparently the short-name reference resolves incorrectly\n                .format(\"org.apache.spark.sql.kafka010.KafkaSourceProvider\")\n                .option(\"kafka.bootstrap.servers\", \"localhost:9092\")\n                .option(\"subscribe\", \"rating_modifiers\")\n                .load();\n\n        // Register a table with Spark - this will hold the incoming events from Kafka\n        df.createOrReplaceTempView(\"rating_modifiers_incoming\");\n\n        // key, value, timestamp are the columns available to me\n        // Note that you must cast value to a string in order to get readable results\n      Dataset<Row> results = session.sql(\n              \"select cast(value as string) as event_detail from rating_modifiers_incoming\");\n\n        // Create a sink\n        // See https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#output-sinks for all sink types\n        StreamingQuery query = results.writeStream().format(\"console\")\n                .outputMode(OutputMode.Append()) // values are Complete, Update, and Append - use Append for non-aggregation queries\n                .option(\"truncate\", false)\n                .option(\"numRows\", 30)\n                .start();\n\n        // Rinse and repeat\n        query.awaitTermination();\n    }\n}\n\n```\n\nHere is the .pom for the DSE version of the consumer:\n\n```\n<project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n  xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\">\n  <modelVersion>4.0.0</modelVersion>\n\n  <groupId>com.datastax.kellett</groupId>\n  <artifactId>streaming-spitzer-compliant</artifactId>\n  <version>0.1</version>\n  <packaging>jar</packaging>\n  <name>streaming-spitzer-compliant</name>\n\n  <properties>\n    <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>\n    <dse.version>6.0.7</dse.version>\n  </properties>\n\n  <dependencies>\n\n    <dependency>\n      <groupId>junit</groupId>\n      <artifactId>junit</artifactId>\n      <version>3.8.1</version>\n      <scope>test</scope>\n    </dependency>\n\n    <dependency>\n      <groupId>com.datastax.dse</groupId>\n      <artifactId>dse-spark-dependencies</artifactId>\n      <version>${dse.version}</version>\n      <scope>provided</scope>\n    </dependency>\n    <!-- Your dependencies, 'provided' are not included in jar -->\n    <!--<dependency>-->\n      <!--<groupId>org.apache.commons</groupId>-->\n      <!--<artifactId>commons-math3</artifactId>-->\n      <!--<version>3.6.1</version>-->\n    <!--</dependency>-->\n    <!--<dependency>-->\n      <!--<groupId>org.apache.commons</groupId>-->\n      <!--<artifactId>commons-csv</artifactId>-->\n      <!--<version>1.0</version>-->\n    <!--</dependency>-->\n  </dependencies>\n\n  <repositories>\n    <repository>\n      <id>DataStax-Repo</id>\n      <url>https://repo.datastax.com/public-repos/</url>\n    </repository>\n  </repositories>\n\n  <build>\n    <plugins>\n      <plugin>\n        <groupId>org.apache.maven.plugins</groupId>\n        <artifactId>maven-compiler-plugin</artifactId>\n        <version>3.5.1</version>\n        <configuration>\n          <source>1.8</source>\n          <target>1.8</target>\n        </configuration>\n      </plugin>\n      <plugin>\n        <groupId>org.apache.maven.plugins</groupId>\n        <artifactId>maven-shade-plugin</artifactId>\n        <version>2.4.3</version>\n        <executions>\n          <execution>\n            <phase>package</phase>\n            <goals>\n              <goal>shade</goal>\n            </goals>\n            <configuration>\n              <!--<relocations>-->\n                <!--<relocation>-->\n                <!--<pattern>org.apache.commons.csv</pattern>-->\n                <!--<shadedPattern>shaded.org.apache.commons.csv</shadedPattern>-->\n                <!--</relocation>-->\n              <!--</relocations>-->\n            </configuration>\n          </execution>\n        </executions>\n      </plugin>\n    </plugins>\n  </build>\n</project>\n\n```\n\n","dateUpdated":"2019-07-12T15:28:43+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{"dse.version":""},"forms":{"dse.version":{"name":"dse.version","defaultValue":"","hidden":false,"$$hashKey":"object:509"}}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>cell 7 (ignore the dse.version text box above. I don&rsquo;t know why the markdown is generating it.)</p>\n<p>STEP 6 OF 6 FOR ILLUSTRATION</p>\n<p><strong><em>Now kill the process in cell 4 (Problem Illustration Step 3 - event generator) by hitting the &ldquo;pause&rdquo; icon</em></strong></p>\n<p><em>Below is reference information.</em></p>\n<p>Here is the source code for the standalone version of the consumer:</p>\n<pre><code>package com.datastax.kellett;\n\nimport org.apache.log4j.Level;\nimport org.apache.log4j.Logger;\nimport org.apache.spark.sql.Dataset;\nimport org.apache.spark.sql.Row;\nimport org.apache.spark.sql.SparkSession;\nimport org.apache.spark.sql.streaming.OutputMode;\nimport org.apache.spark.sql.streaming.StreamingQuery;\nimport org.apache.spark.sql.streaming.StreamingQueryException;\n\npublic class Lab2LinuxStandalone {\n\n    public static void main(String[] args) throws StreamingQueryException {\n\n        Logger.getLogger(&quot;org.apache&quot;).setLevel(Level.WARN);\n        Logger.getLogger(&quot;org.apache.spark.storage&quot;).setLevel(Level.ERROR);\n\n        // Connect to Spark\n        SparkSession session = SparkSession.builder()\n                .master(&quot;local[*]&quot;)\n                .appName(&quot;Spark Structured Streaming Lab2 Linux Version&quot;) // any name you like --- displays in Spark Management UI\n          //      .enableHiveSupport()    // Enables connection to a Hive metastore, support for hive SerDes and Hive user-defined functions\n                .getOrCreate();\n\n        // Read available incoming events into a data frame\n        Dataset&lt;Row&gt; df = session.readStream()\n                //       .format(&quot;kafka&quot;) // This is supposed to work, but apparently the short-name reference resolves incorrectly\n                .format(&quot;org.apache.spark.sql.kafka010.KafkaSourceProvider&quot;)\n                .option(&quot;kafka.bootstrap.servers&quot;, &quot;localhost:9092&quot;)\n                .option(&quot;subscribe&quot;, &quot;rating_modifiers&quot;)\n                .load();\n\n        // Register a table with Spark - this will hold the incoming events from Kafka\n        df.createOrReplaceTempView(&quot;rating_modifiers_incoming&quot;);\n\n        // key, value, timestamp are the columns available to me\n        // Note that you must cast value to a string in order to get readable results\n        Dataset&lt;Row&gt; results = session.sql(\n                &quot;select cast(value as string) as event_detail from rating_modifiers_incoming&quot;);\n\n        // Create a sink\n        // See https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#output-sinks for all sink types\n        StreamingQuery query = results.writeStream().format(&quot;console&quot;)\n                .outputMode(OutputMode.Append()) // values are Complete, Update, and Append - use Append for non-aggregation queries\n                .option(&quot;truncate&quot;, false)\n                .option(&quot;numRows&quot;, 30)\n                .start();\n\n        // Rinse and repeat\n        query.awaitTermination();\n    }\n}\n\n</code></pre>\n<p>Here is the .pom for the standalone version of the consumer:</p>\n<pre><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;\n&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;\n\txsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;\n\t&lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;\n\t&lt;url&gt;http://maven.apache.org&lt;/url&gt;\n\t\n    &lt;!-- above is all boilerplate\n        \n\t\tNOTE that I generated my base pom and project directory structure with the following maven command (run from \n\t\tthe directory ABOVE where you want the project to be created... e.g. /maven):\n        mvn archetype:generate -DgroupId=com.datastax.kellett -DartifactId=datastax-spark-streaming -DarchetypeArtifactId=maven-archetype-quickstart -DinteractiveMode=false\n\n        The junit dependency was created automatically for me, but I pasted in the other dependencies (if needed)\n        and the &lt;repositories&gt; section (if needed) and the &lt;properties&gt; section (if needed)\n\t\tand the &lt;build&gt; section all BEFORE importing the project into IntelliJ.  \n\t\tI imported by pointing to the datastax-spark-sql-v2 directory, and imported as Maven project.\n    --&gt;\t\n\t\n\t&lt;groupId&gt;com.datastax.kellett&lt;/groupId&gt;\n\t&lt;artifactId&gt;datastax-spark-streaming&lt;/artifactId&gt;\n\t&lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt;\n\t&lt;packaging&gt;jar&lt;/packaging&gt;\n\t&lt;name&gt;datastax-spark-streaming&lt;/name&gt;\n\n\t&lt;properties&gt; &lt;!-- Not currently used, but we keep them as doc and for possible future use --&gt;\n\t\t&lt;java.version&gt;1.8&lt;/java.version&gt;\n\t\t&lt;dse.version&gt;6.0.7&lt;/dse.version&gt;\n        &lt;scala.version&gt;2.11.8&lt;/scala.version&gt;\n\t&lt;/properties&gt;\n\n    &lt;dependencies&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;junit&lt;/groupId&gt;\n            &lt;artifactId&gt;junit&lt;/artifactId&gt;\n            &lt;version&gt;3.8.1&lt;/version&gt;\n            &lt;scope&gt;test&lt;/scope&gt;\n        &lt;/dependency&gt;\n\n&lt;!-- if below version (2.3.2) doesn&#39;t work, try 2.4.0 --&gt;\n        &lt;dependency&gt; &lt;!-- added 7/2/19 --&gt;\n            &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;\n            &lt;artifactId&gt;spark-core_2.11&lt;/artifactId&gt; &lt;!-- 2.11 is Scala version used to build Spark --&gt;\n            &lt;version&gt;2.3.2&lt;/version&gt; \t\t\t\t&lt;!-- this number refers to the Spark version --&gt;\n        &lt;/dependency&gt;  &lt;!--see https://spark.apache.org/downloads.html to get latest info for both of the above --&gt;\n\n        &lt;dependency&gt;\n            &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;\n            &lt;artifactId&gt;spark-sql_2.11&lt;/artifactId&gt; &lt;!-- 2.11 is Scala version used to build Spark --&gt;\n            &lt;version&gt;2.3.2&lt;/version&gt;  \t\t\t\t&lt;!-- this number refers to the Spark version --&gt;\n        &lt;/dependency&gt;  &lt;!--see https://spark.apache.org/downloads.html to get latest info for both of the above --&gt;\n\t\t\n\t\t&lt;dependency&gt; &lt;!-- This is for Spark Streaming (Dstream only???) --&gt;\n\t\t\t&lt;groupId&gt;org.apache.spark&lt;/groupId&gt;\n\t\t\t&lt;artifactId&gt;spark-streaming_2.11&lt;/artifactId&gt; &lt;!-- 2.11 is Scala version used to build Spark --&gt;\n\t\t\t&lt;version&gt;2.3.2&lt;/version&gt;  \t\t\t\t&lt;!-- this number refers to the Spark version --&gt;\n\t\t&lt;/dependency&gt;  &lt;!--see https://spark.apache.org/downloads.html to get latest info for both of the above --&gt;\n\n\t\t&lt;dependency&gt; &lt;!-- This is for Spark-Kafka integration --&gt;\n\t\t\t&lt;groupId&gt;org.apache.spark&lt;/groupId&gt;\n\t\t\t&lt;artifactId&gt;spark-streaming-kafka-0-10_2.11&lt;/artifactId&gt; &lt;!-- 2.11 is Scala version used to build Spark --&gt;\n\t\t\t&lt;version&gt;2.3.2&lt;/version&gt;  \t\t\t\t&lt;!-- this number refers to the Spark version --&gt;\n\t\t&lt;/dependency&gt;   &lt;!--see https://spark.apache.org/docs/2.3.2/streaming-kafka-integration.html to get latest info for the above --&gt;\n\n\t\t&lt;dependency&gt; &lt;!-- This is for Spark-Kafka STRUCTURED STREAMING --&gt;\n\t\t\t&lt;groupId&gt;org.apache.spark&lt;/groupId&gt;\n\t\t\t&lt;artifactId&gt;spark-sql-kafka-0-10_2.11&lt;/artifactId&gt; &lt;!-- 2.11 is Scala version used to build Spark --&gt;\n\t\t\t&lt;version&gt;2.3.2&lt;/version&gt;  \t\t\t\t&lt;!-- this number refers to the Spark version --&gt;\n\t\t&lt;/dependency&gt;\n\n        &lt;dependency&gt; &lt;!-- added 7/2/19 --&gt;\n            &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;\n            &lt;artifactId&gt;hadoop-hdfs&lt;/artifactId&gt;\n            &lt;version&gt;2.2.0&lt;/version&gt;\n        &lt;/dependency&gt;\n\t\t\n\t&lt;/dependencies&gt;\n\n    &lt;build&gt;\n        &lt;finalName&gt;datastax-spark-streaming&lt;/finalName&gt;\n        &lt;plugins&gt;\n            &lt;plugin&gt;\n                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;\n                &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt;\n                &lt;executions&gt;\n                    &lt;execution&gt;\n                        &lt;id&gt;create-my-bundle&lt;/id&gt;\n                        &lt;phase&gt;package&lt;/phase&gt;\n                        &lt;goals&gt;\n                            &lt;goal&gt;single&lt;/goal&gt;\n                        &lt;/goals&gt;\n                        &lt;configuration&gt;\n\n                            &lt;archive&gt;\n                                &lt;manifest&gt;\n                                    &lt;mainClass&gt;\n                                        com.datastax.kellett.Lab2Windows\n                                    &lt;/mainClass&gt;\n                                &lt;/manifest&gt;\n                            &lt;/archive&gt;\n\n                            &lt;descriptorRefs&gt;\n                                &lt;descriptorRef&gt;jar-with-dependencies&lt;/descriptorRef&gt;\n                            &lt;/descriptorRefs&gt;\n                        &lt;/configuration&gt;\n                    &lt;/execution&gt;\n                &lt;/executions&gt;\n            &lt;/plugin&gt;\n        &lt;/plugins&gt;\n    &lt;/build&gt;\n\n    &lt;repositories&gt;\n        &lt;repository&gt;\n            &lt;id&gt;DataStax-Repo&lt;/id&gt;\n            &lt;url&gt;https://datastax.artifactoryonline.com/datastax/public-repos/&lt;/url&gt;\n        &lt;/repository&gt;\n    &lt;/repositories&gt;\n\n&lt;/project&gt;\n\n</code></pre>\n<p>Here is the source code for the DSE version of the consumer:</p>\n<pre><code>package com.datastax.kellett;\n\nimport org.apache.log4j.Level;\nimport org.apache.log4j.Logger;\nimport org.apache.spark.sql.Dataset;\nimport org.apache.spark.sql.Row;\nimport org.apache.spark.sql.SparkSession;\nimport org.apache.spark.sql.streaming.OutputMode;\nimport org.apache.spark.sql.streaming.StreamingQuery;\nimport org.apache.spark.sql.streaming.StreamingQueryException;\n\npublic class Lab2theRightWay {\n\n    public static void main(String[] args) throws StreamingQueryException {\n\n        Logger.getLogger(&quot;org.apache&quot;).setLevel(Level.WARN);\n        Logger.getLogger(&quot;org.apache.spark.storage&quot;).setLevel(Level.ERROR);\n\n        // Connect to Spark\n        System.out.println(&quot;Lab2theRightWay test 001 - initial&quot;);\n        SparkSession session = SparkSession.builder()\n                .master(&quot;local[*]&quot;)\n                .appName(&quot;Spark Structured Streaming Lab2theRightWay&quot;) // any name you like --- displays in Spark Management UI\n                //      .enableHiveSupport()    // Enables connection to a Hive metastore, support for hive SerDes and Hive user-defined functions\n                .getOrCreate();\n\n        // Read available incoming events into a data frame\n        Dataset&lt;Row&gt; df = session.readStream()\n                //       .format(&quot;kafka&quot;) // This is supposed to work, but apparently the short-name reference resolves incorrectly\n                .format(&quot;org.apache.spark.sql.kafka010.KafkaSourceProvider&quot;)\n                .option(&quot;kafka.bootstrap.servers&quot;, &quot;localhost:9092&quot;)\n                .option(&quot;subscribe&quot;, &quot;rating_modifiers&quot;)\n                .load();\n\n        // Register a table with Spark - this will hold the incoming events from Kafka\n        df.createOrReplaceTempView(&quot;rating_modifiers_incoming&quot;);\n\n        // key, value, timestamp are the columns available to me\n        // Note that you must cast value to a string in order to get readable results\n      Dataset&lt;Row&gt; results = session.sql(\n              &quot;select cast(value as string) as event_detail from rating_modifiers_incoming&quot;);\n\n        // Create a sink\n        // See https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#output-sinks for all sink types\n        StreamingQuery query = results.writeStream().format(&quot;console&quot;)\n                .outputMode(OutputMode.Append()) // values are Complete, Update, and Append - use Append for non-aggregation queries\n                .option(&quot;truncate&quot;, false)\n                .option(&quot;numRows&quot;, 30)\n                .start();\n\n        // Rinse and repeat\n        query.awaitTermination();\n    }\n}\n\n</code></pre>\n<p>Here is the .pom for the DSE version of the consumer:</p>\n<pre><code>&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;\n  xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;\n  &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;\n\n  &lt;groupId&gt;com.datastax.kellett&lt;/groupId&gt;\n  &lt;artifactId&gt;streaming-spitzer-compliant&lt;/artifactId&gt;\n  &lt;version&gt;0.1&lt;/version&gt;\n  &lt;packaging&gt;jar&lt;/packaging&gt;\n  &lt;name&gt;streaming-spitzer-compliant&lt;/name&gt;\n\n  &lt;properties&gt;\n    &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;\n    &lt;dse.version&gt;6.0.7&lt;/dse.version&gt;\n  &lt;/properties&gt;\n\n  &lt;dependencies&gt;\n\n    &lt;dependency&gt;\n      &lt;groupId&gt;junit&lt;/groupId&gt;\n      &lt;artifactId&gt;junit&lt;/artifactId&gt;\n      &lt;version&gt;3.8.1&lt;/version&gt;\n      &lt;scope&gt;test&lt;/scope&gt;\n    &lt;/dependency&gt;\n\n    &lt;dependency&gt;\n      &lt;groupId&gt;com.datastax.dse&lt;/groupId&gt;\n      &lt;artifactId&gt;dse-spark-dependencies&lt;/artifactId&gt;\n      &lt;version&gt;&lt;/version&gt;\n      &lt;scope&gt;provided&lt;/scope&gt;\n    &lt;/dependency&gt;\n    &lt;!-- Your dependencies, &#39;provided&#39; are not included in jar --&gt;\n    &lt;!--&lt;dependency&gt;--&gt;\n      &lt;!--&lt;groupId&gt;org.apache.commons&lt;/groupId&gt;--&gt;\n      &lt;!--&lt;artifactId&gt;commons-math3&lt;/artifactId&gt;--&gt;\n      &lt;!--&lt;version&gt;3.6.1&lt;/version&gt;--&gt;\n    &lt;!--&lt;/dependency&gt;--&gt;\n    &lt;!--&lt;dependency&gt;--&gt;\n      &lt;!--&lt;groupId&gt;org.apache.commons&lt;/groupId&gt;--&gt;\n      &lt;!--&lt;artifactId&gt;commons-csv&lt;/artifactId&gt;--&gt;\n      &lt;!--&lt;version&gt;1.0&lt;/version&gt;--&gt;\n    &lt;!--&lt;/dependency&gt;--&gt;\n  &lt;/dependencies&gt;\n\n  &lt;repositories&gt;\n    &lt;repository&gt;\n      &lt;id&gt;DataStax-Repo&lt;/id&gt;\n      &lt;url&gt;https://repo.datastax.com/public-repos/&lt;/url&gt;\n    &lt;/repository&gt;\n  &lt;/repositories&gt;\n\n  &lt;build&gt;\n    &lt;plugins&gt;\n      &lt;plugin&gt;\n        &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;\n        &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt;\n        &lt;version&gt;3.5.1&lt;/version&gt;\n        &lt;configuration&gt;\n          &lt;source&gt;1.8&lt;/source&gt;\n          &lt;target&gt;1.8&lt;/target&gt;\n        &lt;/configuration&gt;\n      &lt;/plugin&gt;\n      &lt;plugin&gt;\n        &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;\n        &lt;artifactId&gt;maven-shade-plugin&lt;/artifactId&gt;\n        &lt;version&gt;2.4.3&lt;/version&gt;\n        &lt;executions&gt;\n          &lt;execution&gt;\n            &lt;phase&gt;package&lt;/phase&gt;\n            &lt;goals&gt;\n              &lt;goal&gt;shade&lt;/goal&gt;\n            &lt;/goals&gt;\n            &lt;configuration&gt;\n              &lt;!--&lt;relocations&gt;--&gt;\n                &lt;!--&lt;relocation&gt;--&gt;\n                &lt;!--&lt;pattern&gt;org.apache.commons.csv&lt;/pattern&gt;--&gt;\n                &lt;!--&lt;shadedPattern&gt;shaded.org.apache.commons.csv&lt;/shadedPattern&gt;--&gt;\n                &lt;!--&lt;/relocation&gt;--&gt;\n              &lt;!--&lt;/relocations&gt;--&gt;\n            &lt;/configuration&gt;\n          &lt;/execution&gt;\n        &lt;/executions&gt;\n      &lt;/plugin&gt;\n    &lt;/plugins&gt;\n  &lt;/build&gt;\n&lt;/project&gt;\n\n</code></pre>\n</div>"}]},"apps":[],"jobName":"paragraph_1562945323814_1089062292","id":"20190708-125604_411065673","dateCreated":"2019-07-12T15:28:43+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:300"},{"text":"%cassandra\n\n// cell 8 - FUTURE STUFF - NOT PART OF ILLUSTRATION\n\n// DDL\n\n// start from a clean slate\n\ndrop keyspace if exists streaming_workshop;\n\n// create the keyspace for the workshop\n\ncreate keyspace if not exists streaming_workshop \n    WITH replication = {'class': 'SimpleStrategy', 'replication_factor' : 3};\n\n// create all tables\n\ncreate table if not exists streaming_workshop.ratings_modifiers_all (\n    instrument_symbol           text,\n    ethical_category            text,\n    event_sequence_number       int,\n    source_url                  text,\n    source_weight               int,\n    source_sentiment            int,\n    primary key ((instrument_symbol, ethical_category), event_sequence_number)\n)\nwith clustering order by (event_sequence_number desc);\n\ncreate table if not exists streaming_workshop.ratings_modifiers_good_news (\n    instrument_symbol           text,\n    ethical_category            text,\n    event_sequence_number       int,\n    source_url                  text,\n    source_weight               int,\n    source_sentiment            int,\n    primary key ((instrument_symbol, ethical_category), event_sequence_number)\n)\nwith clustering order by (event_sequence_number desc);\n\ncreate table if not exists streaming_workshop.ratings_modifiers_bad_news (\n    instrument_symbol           text,\n    ethical_category            text,\n    event_sequence_number       int,\n    source_url                  text,\n    source_weight               int,\n    source_sentiment            int,\n    primary key ((instrument_symbol, ethical_category), event_sequence_number)\n)\nwith clustering order by (event_sequence_number desc);\n\ncreate table if not exists streaming_workshop.ratings_modifiers_aggregated_complete (\n    instrument_symbol           text,\n    ethical_category            text,\n    total_events                int,\n    weighted_sentiment_change   int,\n    primary key ((instrument_symbol), ethical_category)\n)\nwith clustering order by (ethical_category asc);\n\ncreate table if not exists streaming_workshop.ratings_modifiers_aggregated_windowed (\n    instrument_symbol           text,\n    ethical_category            text,\n    event_time_window           text,\n    total_events                int,\n    weighted_sentiment_change   int,\n    primary key ((instrument_symbol), ethical_category, event_time_window)\n)\nwith clustering order by (ethical_category asc, event_time_window desc);","dateUpdated":"2019-07-12T15:28:43+0000","config":{"editorSetting":{"language":"text","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/undefined","editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1562945323815_1088677543","id":"20190630-193659_1079039557","dateCreated":"2019-07-12T15:28:43+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:301"},{"text":"%cassandra\n","dateUpdated":"2019-07-12T15:28:43+0000","config":{"colWidth":12,"editorMode":"ace/mode/undefined","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1562945323816_1086753798","id":"20190703-194622_1480262872","dateCreated":"2019-07-12T15:28:43+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:302"}],"name":"bkplay","id":"2EHXRZF7W","angularObjects":{"2EF8853AD:shared_process":[],"2EFXNPACM:shared_process":[],"2EF7Q6AED:shared_process":[],"2EJAG6A4U:shared_process":[],"2EHKF5KUA:shared_process":[],"2EFHSUV44:shared_process":[],"2EG4GNZWV:shared_process":[],"2EHKTMGSF:shared_process":[],"2EJMCDV5Y:shared_process":[],"2EFF7VYYC:shared_process":[],"2EF8QCG7W:shared_process":[],"2EGVJREAU:shared_process":[],"2EJ5HC2KW:shared_process":[],"2EFZMRXZX:shared_process":[],"2EG16FQF1:shared_process":[],"2EHKBGES5:shared_process":[],"2EF366R9G:shared_process":[],"2EHDGAD3A:shared_process":[],"2EJJ9CN21:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}