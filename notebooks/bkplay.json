{"paragraphs":[{"text":"%sh\n\n# cell 1\n\n# STEP 1 OF 6 FOR PROBLEM ILLUSTRATION - just prove Kafka is running\n\n# make sure Kafka is running by listing available topics\n\ncd /tmp/datastax-spark-streaming-workshop/kafka_2.12-2.2.0/bin\n\n./kafka-topics.sh --list --bootstrap-server localhost:9092","user":"anonymous","dateUpdated":"2019-07-08T19:50:31+0000","config":{"colWidth":12,"editorMode":"ace/mode/sh","results":{},"enabled":true,"editorSetting":{"language":"sh","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1562528755509_503858856","id":"20190630-151028_871499823","dateCreated":"2019-07-07T19:45:55+0000","dateStarted":"2019-07-08T19:50:31+0000","dateFinished":"2019-07-08T19:50:33+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:898"},{"text":"%sh\n\n# cell 2\n\n# THIS IS NOT A STEP FOR PROBLEM ILLUSTRATION (it's just a utility in case I need to prove I can consume kafka events)\n\n# This utility consumes events (which proves we are generating them)\n\ncd /tmp/datastax-spark-streaming-workshop/kafka_2.12-2.2.0/bin\n\n./kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic rating_modifiers ### --from-beginning ","dateUpdated":"2019-07-07T19:45:55+0000","config":{"tableHide":false,"editorSetting":{"language":"sh","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/sh","results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1562528755523_487699402","id":"20190630-151549_1055871517","dateCreated":"2019-07-07T19:45:55+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:899"},{"text":"%sh\n\n# cell 3\n\n# STEP 2 OF 6 FOR PROBLEM ILLUSTRATION - start a *working* version of my event consumer... does NOT use DSE, uses .master(\"local[*]\")\n\n# This is BK's standalone Spark Streaming Event Consumer (does not use dse spark-submit... in fact, does not use spark-submit at all)\n\ncd /tmp/datastax-spark-streaming-workshop/executables\n\njava -cp datastax-spark-streaming-jar-with-dependencies.jar com.datastax.kellett.Lab2LinuxStandalone","user":"anonymous","dateUpdated":"2019-07-08T19:50:43+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"sh","editOnDblClick":false},"editorMode":"ace/mode/sh"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1562530410482_-958901515","id":"20190707-201330_849802427","dateCreated":"2019-07-07T20:13:30+0000","dateStarted":"2019-07-08T19:50:43+0000","dateFinished":"2019-07-08T19:51:04+0000","status":"ABORT","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:900"},{"text":"%sh\n\n# cell 4\n\n# STEP 3 OF 6 FOR PROBLEM ILLUSTRATION - begin generating events\n\n# This program generates events used for the workshop\n\ncd /tmp/datastax-spark-streaming-workshop\n\njava -jar executables/kafka-event-generator-1.0-SNAPSHOT-jar-with-dependencies.jar\n\n","user":"anonymous","dateUpdated":"2019-07-08T19:50:55+0000","config":{"tableHide":false,"editorSetting":{"language":"sh","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/sh","results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1562528755526_486545155","id":"20190630-152714_2036339272","dateCreated":"2019-07-07T19:45:55+0000","dateStarted":"2019-07-08T19:50:55+0000","dateFinished":"2019-07-08T19:51:55+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:901"},{"text":"%md\n\ncell 5 \n\nSTEP 4 OF 6 FOR PROBLEM ILLUSTRATION\n\nAt this point, we should see that the standalone version of the event consumer is working (look at output in cell 3).  This proves that:\n- my code is good\n- my build is good in a non-DSE environment\n\n*__Once you see that things are working, kill the process in cell 3 (Problem Illustration Step 2 - standalone event consumer) by hitting the \"pause\" icon.__*\n","user":"anonymous","dateUpdated":"2019-07-09T13:33:28+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1562590063914_689011144","id":"20190708-124743_240066369","dateCreated":"2019-07-08T12:47:43+0000","dateStarted":"2019-07-09T13:33:28+0000","dateFinished":"2019-07-09T13:33:28+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:902","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>cell 5 </p>\n<p>STEP 4 OF 6 FOR PROBLEM ILLUSTRATION</p>\n<p>At this point, we should see that the standalone version of the event consumer is working (look at output in cell 3). This proves that:<br/>- my code is good<br/>- my build is good in a non-DSE environment</p>\n<p><em><strong>Once you see that things are working, kill the process in cell 3 (Problem Illustration Step 2 - standalone event consumer) by hitting the &ldquo;pause&rdquo; icon.</strong></em></p>\n</div>"}]}},{"text":"%sh\n\n# cell 6\n\n# STEP 5 OF 6 FOR PROBLEM ILLUSTRATION - start the *non-working* version of event consumer in the DSE environment to see the exception that is thrown\n\n# This is BK's Spark Streaming Event Consumer using dse spark-submit\n# THIS FAILS\n\ncd /tmp/datastax-spark-streaming-workshop/executables\n\ndse spark-submit --class com.datastax.kellett.Lab2 --total-executor-cores 2 --executor-memory 1g  datastax-spark-streaming-dse-jar-with-dependencies.jar","user":"anonymous","dateUpdated":"2019-07-08T19:58:31+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"sh","editOnDblClick":false},"editorMode":"ace/mode/sh"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1562531028025_-1699452847","id":"20190707-202348_889041450","dateCreated":"2019-07-07T20:23:48+0000","dateStarted":"2019-07-08T19:58:31+0000","dateFinished":"2019-07-08T19:58:51+0000","status":"ERROR","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:903"},{"text":"%md\n\ncell 7 (ignore the dse.version text box above.  I don't know why the markdown is generating it.)\n\nSTEP 6 OF 6 FOR PROBLEM ILLUSTRATION\n\nFirst note the exception (no such method).  \n\n**_Now kill the process in cell 4 (Problem Illustration Step 3 - event generator) by hitting the \"pause\" icon_**\n\n_Below is reference information._\n\nHere is the source code for the working version of the consumer:\n\n```\npackage com.datastax.kellett;\n\nimport org.apache.log4j.Level;\nimport org.apache.log4j.Logger;\nimport org.apache.spark.sql.Dataset;\nimport org.apache.spark.sql.Row;\nimport org.apache.spark.sql.SparkSession;\nimport org.apache.spark.sql.streaming.OutputMode;\nimport org.apache.spark.sql.streaming.StreamingQuery;\nimport org.apache.spark.sql.streaming.StreamingQueryException;\n\npublic class Lab2LinuxStandalone {\n\n    public static void main(String[] args) throws StreamingQueryException {\n\n        Logger.getLogger(\"org.apache\").setLevel(Level.WARN);\n        Logger.getLogger(\"org.apache.spark.storage\").setLevel(Level.ERROR);\n\n        // Connect to Spark\n        SparkSession session = SparkSession.builder()\n                .master(\"local[*]\")\n                .appName(\"Spark Structured Streaming Lab2 Linux Version\") // any name you like --- displays in Spark Management UI\n          //      .enableHiveSupport()    // Enables connection to a Hive metastore, support for hive SerDes and Hive user-defined functions\n                .getOrCreate();\n\n        // Read available incoming events into a data frame\n        Dataset<Row> df = session.readStream()\n                //       .format(\"kafka\") // This is supposed to work, but apparently the short-name reference resolves incorrectly\n                .format(\"org.apache.spark.sql.kafka010.KafkaSourceProvider\")\n                .option(\"kafka.bootstrap.servers\", \"localhost:9092\")\n                .option(\"subscribe\", \"rating_modifiers\")\n                .load();\n\n        // Register a table with Spark - this will hold the incoming events from Kafka\n        df.createOrReplaceTempView(\"rating_modifiers_incoming\");\n\n        // key, value, timestamp are the columns available to me\n        // Note that you must cast value to a string in order to get readable results\n        Dataset<Row> results = session.sql(\n                \"select cast(value as string) as event_detail from rating_modifiers_incoming\");\n\n        // Create a sink\n        // See https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#output-sinks for all sink types\n        StreamingQuery query = results.writeStream().format(\"console\")\n                .outputMode(OutputMode.Append()) // values are Complete, Update, and Append - use Append for non-aggregation queries\n                .option(\"truncate\", false)\n                .option(\"numRows\", 30)\n                .start();\n\n        // Rinse and repeat\n        query.awaitTermination();\n    }\n}\n\n```\n\nHere is the .pom for the working version of the consumer:\n\n```\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n\txsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\">\n\t<modelVersion>4.0.0</modelVersion>\n\t<url>http://maven.apache.org</url>\n\t\n    <!-- above is all boilerplate\n        \n\t\tNOTE that I generated my base pom and project directory structure with the following maven command (run from \n\t\tthe directory ABOVE where you want the project to be created... e.g. /maven):\n        mvn archetype:generate -DgroupId=com.datastax.kellett -DartifactId=datastax-spark-streaming -DarchetypeArtifactId=maven-archetype-quickstart -DinteractiveMode=false\n\n        The junit dependency was created automatically for me, but I pasted in the other dependencies (if needed)\n        and the <repositories> section (if needed) and the <properties> section (if needed)\n\t\tand the <build> section all BEFORE importing the project into IntelliJ.  \n\t\tI imported by pointing to the datastax-spark-sql-v2 directory, and imported as Maven project.\n    -->\t\n\t\n\t<groupId>com.datastax.kellett</groupId>\n\t<artifactId>datastax-spark-streaming</artifactId>\n\t<version>0.0.1-SNAPSHOT</version>\n\t<packaging>jar</packaging>\n\t<name>datastax-spark-streaming</name>\n\n\t<properties> <!-- Not currently used, but we keep them as doc and for possible future use -->\n\t\t<java.version>1.8</java.version>\n\t\t<dse.version>6.0.7</dse.version>\n        <scala.version>2.11.8</scala.version>\n\t</properties>\n\n    <dependencies>\n        <dependency>\n            <groupId>junit</groupId>\n            <artifactId>junit</artifactId>\n            <version>3.8.1</version>\n            <scope>test</scope>\n        </dependency>\n\n<!-- if below version (2.3.2) doesn't work, try 2.4.0 -->\n        <dependency> <!-- added 7/2/19 -->\n            <groupId>org.apache.spark</groupId>\n            <artifactId>spark-core_2.11</artifactId> <!-- 2.11 is Scala version used to build Spark -->\n            <version>2.3.2</version> \t\t\t\t<!-- this number refers to the Spark version -->\n        </dependency>  <!--see https://spark.apache.org/downloads.html to get latest info for both of the above -->\n\n        <dependency>\n            <groupId>org.apache.spark</groupId>\n            <artifactId>spark-sql_2.11</artifactId> <!-- 2.11 is Scala version used to build Spark -->\n            <version>2.3.2</version>  \t\t\t\t<!-- this number refers to the Spark version -->\n        </dependency>  <!--see https://spark.apache.org/downloads.html to get latest info for both of the above -->\n\t\t\n\t\t<dependency> <!-- This is for Spark Streaming (Dstream only???) -->\n\t\t\t<groupId>org.apache.spark</groupId>\n\t\t\t<artifactId>spark-streaming_2.11</artifactId> <!-- 2.11 is Scala version used to build Spark -->\n\t\t\t<version>2.3.2</version>  \t\t\t\t<!-- this number refers to the Spark version -->\n\t\t</dependency>  <!--see https://spark.apache.org/downloads.html to get latest info for both of the above -->\n\n\t\t<dependency> <!-- This is for Spark-Kafka integration -->\n\t\t\t<groupId>org.apache.spark</groupId>\n\t\t\t<artifactId>spark-streaming-kafka-0-10_2.11</artifactId> <!-- 2.11 is Scala version used to build Spark -->\n\t\t\t<version>2.3.2</version>  \t\t\t\t<!-- this number refers to the Spark version -->\n\t\t</dependency>   <!--see https://spark.apache.org/docs/2.3.2/streaming-kafka-integration.html to get latest info for the above -->\n\n\t\t<dependency> <!-- This is for Spark-Kafka STRUCTURED STREAMING -->\n\t\t\t<groupId>org.apache.spark</groupId>\n\t\t\t<artifactId>spark-sql-kafka-0-10_2.11</artifactId> <!-- 2.11 is Scala version used to build Spark -->\n\t\t\t<version>2.3.2</version>  \t\t\t\t<!-- this number refers to the Spark version -->\n\t\t</dependency>\n\n        <dependency> <!-- added 7/2/19 -->\n            <groupId>org.apache.hadoop</groupId>\n            <artifactId>hadoop-hdfs</artifactId>\n            <version>2.2.0</version>\n        </dependency>\n\t\t\n\t</dependencies>\n\n    <build>\n        <finalName>datastax-spark-streaming</finalName>\n        <plugins>\n            <plugin>\n                <groupId>org.apache.maven.plugins</groupId>\n                <artifactId>maven-assembly-plugin</artifactId>\n                <executions>\n                    <execution>\n                        <id>create-my-bundle</id>\n                        <phase>package</phase>\n                        <goals>\n                            <goal>single</goal>\n                        </goals>\n                        <configuration>\n\n                            <archive>\n                                <manifest>\n                                    <mainClass>\n                                        com.datastax.kellett.Lab2Windows\n                                    </mainClass>\n                                </manifest>\n                            </archive>\n\n                            <descriptorRefs>\n                                <descriptorRef>jar-with-dependencies</descriptorRef>\n                            </descriptorRefs>\n                        </configuration>\n                    </execution>\n                </executions>\n            </plugin>\n        </plugins>\n    </build>\n\n    <repositories>\n        <repository>\n            <id>DataStax-Repo</id>\n            <url>https://datastax.artifactoryonline.com/datastax/public-repos/</url>\n        </repository>\n    </repositories>\n\n</project>\n\n```\n\nHere is the source code for the non-working version of the consumer:\n\n```\npackage com.datastax.kellett;\n\nimport org.apache.spark.sql.Dataset;\nimport org.apache.spark.sql.Row;\nimport org.apache.spark.sql.SparkSession;\nimport org.apache.spark.sql.streaming.OutputMode;\nimport org.apache.spark.sql.streaming.StreamingQuery;\nimport org.apache.spark.sql.streaming.StreamingQueryException;\n\npublic class Lab2 {\n\n    public static void main(String[] args) throws StreamingQueryException {\n\n        // Connect to Spark\n        System.out.println(\"Version: 7/9/19 01 with kitchen-sink pom\");\n        System.out.println(\"About to create session...\");\n        SparkSession session = SparkSession.builder()\n                .appName(\"Spark Structured Streaming Lab2\") // any name you like --- displays in Spark Management UI\n                //       .enableHiveSupport()    // Enables connection to a Hive metastore, support for hive SerDes and Hive user-defined functions\n                .getOrCreate();\n\n        // Read available incoming events into a data frame\n        System.out.println(\"About to read Kafka event stream...\");\n        Dataset<Row> df = session.readStream()\n                //.format(\"kafka\")\n                .format(\"org.apache.spark.sql.kafka010.KafkaSourceProvider\")\n                .option(\"kafka.bootstrap.servers\", \"localhost:9092\")\n                .option(\"subscribe\", \"rating_modifiers\")\n                .load();\n\n        // Register a table with Spark - this will hold the incoming events from Kafka\n        System.out.println(\"About to create temp view...\");\n        df.createOrReplaceTempView(\"rating_modifiers_incoming\");\n\n        // key, value, timestamp are the columns available to me\n        // Note that you must cast value to a string in order to get readable results\n        System.out.println(\"About to create results dataframe\");\n        Dataset<Row> results = session.sql(\n                \"select cast(value as string) as event_detail from rating_modifiers_incoming\");\n\n        // Create a sink\n        // See https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#output-sinks for all sink types\n        System.out.println(\"About to write results to console...\");\n        StreamingQuery query = results.writeStream().format(\"console\")\n                .outputMode(OutputMode.Append()) // values are Complete, Update, and Append - use Append for non-aggregation queries\n                .option(\"truncate\", false)\n                .option(\"numRows\", 30)\n                .start();\n\n        // Rinse and repeat\n        System.out.println(\"About to enter awaitTermination...\");\n        query.awaitTermination();\n    }\n}\n\n```\n\nHere is the .pom for the non-working version of the consumer:\n\n```\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n         xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\">\n    <modelVersion>4.0.0</modelVersion>\n    <url>http://maven.apache.org</url>\n\n    <!-- above is all boilerplate\n        \n\t\tNOTE that I generated my base pom and project directory structure with the following maven command (run from \n\t\tthe directory ABOVE where you want the project to be created... e.g. /maven):\n        mvn archetype:generate -DgroupId=com.datastax.kellett -DartifactId=datastax-spark-sql-v2 -DarchetypeArtifactId=maven-archetype-quickstart -DinteractiveMode=false\n\n        The junit dependency was created automatically for me, but I pasted in the other dependencies (if needed)\n        and the <repositories> section (if needed) and the <properties> section (if needed)\n\t\tand the <build> section all BEFORE importing the project into IntelliJ.\n\t\tI imported by pointing to the datastax-spark-sql-v2 directory, and imported as Maven project.\n    -->\n\n    <groupId>com.datastax.kellett</groupId>\n    <artifactId>datastax-spark-streaming-dse</artifactId>\n    <version>1.0</version>\n    <packaging>jar</packaging>\n    <name>datastax-spark-streaming-dse</name>\n\n    <properties> <!-- Not currently used, but we keep them as doc and for possible future use -->\n        <java.version>1.8</java.version>\n        <dse.version>6.0.7</dse.version>\n        <scala.version>2.11.8</scala.version>\n    </properties>\n\n    <dependencies>\n        <dependency>\n            <groupId>junit</groupId>\n            <artifactId>junit</artifactId>\n            <version>3.8.1</version>\n            <scope>test</scope>\n        </dependency>\n\n        <dependency> <!-- added 7/2/19 -->\n            <groupId>org.apache.spark</groupId>\n            <artifactId>spark-core_2.11</artifactId> <!-- 2.11 is Scala version used to build Spark -->\n            <version>2.3.2</version> \t\t\t\t<!-- this number refers to the Spark version -->\n        </dependency>  <!--see https://spark.apache.org/downloads.html to get latest info for both of the above -->\n\n        <dependency>\n            <groupId>org.apache.spark</groupId>\n            <artifactId>spark-sql_2.11</artifactId> <!-- 2.11 is Scala version used to build Spark -->\n            <version>2.3.2</version>  \t\t\t\t<!-- this number refers to the Spark version -->\n        </dependency>  <!--see https://spark.apache.org/downloads.html to get latest info for both of the above -->\n\n        <dependency> <!-- This is for Spark Streaming (Dstream only???) -->\n            <groupId>org.apache.spark</groupId>\n            <artifactId>spark-streaming_2.11</artifactId> <!-- 2.11 is Scala version used to build Spark -->\n            <version>2.3.2</version>  \t\t\t\t<!-- this number refers to the Spark version -->\n        </dependency>  <!--see https://spark.apache.org/downloads.html to get latest info for both of the above -->\n\n        <dependency> <!-- This is for Spark-Kafka integration -->\n            <groupId>org.apache.spark</groupId>\n            <artifactId>spark-streaming-kafka-0-10_2.11</artifactId> <!-- 2.11 is Scala version used to build Spark -->\n            <version>2.3.2</version>  \t\t\t\t<!-- this number refers to the Spark version -->\n        </dependency>   <!--see https://spark.apache.org/docs/2.3.2/streaming-kafka-integration.html to get latest info for the above -->\n\n        <dependency> <!-- This is for Spark-Kafka STRUCTURED STREAMING -->\n            <groupId>org.apache.spark</groupId>\n            <artifactId>spark-sql-kafka-0-10_2.11</artifactId> <!-- 2.11 is Scala version used to build Spark -->\n            <version>2.3.2</version>  \t\t\t\t<!-- this number refers to the Spark version -->\n        </dependency>\n\n        <dependency> <!-- added 7/2/19 -->\n            <groupId>org.apache.hadoop</groupId>\n            <artifactId>hadoop-hdfs</artifactId>\n            <version>2.2.0</version>\n        </dependency>\n\n        <dependency>\n            <groupId>com.datastax.dse</groupId>\n            <artifactId>dse-spark-dependencies</artifactId>\n            <version>${dse.version}</version>\n            <scope>provided</scope>\n        </dependency>\n\n    </dependencies>\n\n\n    <build>\n        <finalName>datastax-spark-streaming-dse</finalName>\n        <plugins>\n            <plugin>\n                <groupId>org.apache.maven.plugins</groupId>\n                <artifactId>maven-assembly-plugin</artifactId>\n                <executions>\n                    <execution>\n                        <id>create-my-bundle</id>\n                        <phase>package</phase>\n                        <goals>\n                            <goal>single</goal>\n                        </goals>\n                        <configuration>\n                            <descriptorRefs>\n                                <descriptorRef>jar-with-dependencies</descriptorRef>\n                            </descriptorRefs>\n                        </configuration>\n                    </execution>\n                </executions>\n            </plugin>\n        </plugins>\n    </build>\n\n    <repositories>\n        <repository>\n            <id>DataStax-Repo</id>\n            <url>https://datastax.artifactoryonline.com/datastax/public-repos/</url>\n        </repository>\n\n        <repository>\n            <id>DataStax-Repo-too</id>\n            <url>https://repo.datastax.com/public-repos/</url>\n        </repository>\n\n    </repositories>\n\n</project>\n\n```\n\n","user":"anonymous","dateUpdated":"2019-07-09T13:32:41+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{"dse.version":""},"forms":{"dse.version":{"name":"dse.version","defaultValue":"","hidden":false,"$$hashKey":"object:1598"}}},"apps":[],"jobName":"paragraph_1562590564588_-524733718","id":"20190708-125604_411065673","dateCreated":"2019-07-08T12:56:04+0000","dateStarted":"2019-07-09T13:32:41+0000","dateFinished":"2019-07-09T13:32:41+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:904","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>cell 7 (ignore the dse.version text box above. I don&rsquo;t know why the markdown is generating it.)</p>\n<p>STEP 6 OF 6 FOR PROBLEM ILLUSTRATION</p>\n<p>First note the exception (no such method). </p>\n<p><strong><em>Now kill the process in cell 4 (Problem Illustration Step 3 - event generator) by hitting the &ldquo;pause&rdquo; icon</em></strong></p>\n<p><em>Below is reference information.</em></p>\n<p>Here is the source code for the working version of the consumer:</p>\n<pre><code>package com.datastax.kellett;\n\nimport org.apache.log4j.Level;\nimport org.apache.log4j.Logger;\nimport org.apache.spark.sql.Dataset;\nimport org.apache.spark.sql.Row;\nimport org.apache.spark.sql.SparkSession;\nimport org.apache.spark.sql.streaming.OutputMode;\nimport org.apache.spark.sql.streaming.StreamingQuery;\nimport org.apache.spark.sql.streaming.StreamingQueryException;\n\npublic class Lab2LinuxStandalone {\n\n    public static void main(String[] args) throws StreamingQueryException {\n\n        Logger.getLogger(&quot;org.apache&quot;).setLevel(Level.WARN);\n        Logger.getLogger(&quot;org.apache.spark.storage&quot;).setLevel(Level.ERROR);\n\n        // Connect to Spark\n        SparkSession session = SparkSession.builder()\n                .master(&quot;local[*]&quot;)\n                .appName(&quot;Spark Structured Streaming Lab2 Linux Version&quot;) // any name you like --- displays in Spark Management UI\n          //      .enableHiveSupport()    // Enables connection to a Hive metastore, support for hive SerDes and Hive user-defined functions\n                .getOrCreate();\n\n        // Read available incoming events into a data frame\n        Dataset&lt;Row&gt; df = session.readStream()\n                //       .format(&quot;kafka&quot;) // This is supposed to work, but apparently the short-name reference resolves incorrectly\n                .format(&quot;org.apache.spark.sql.kafka010.KafkaSourceProvider&quot;)\n                .option(&quot;kafka.bootstrap.servers&quot;, &quot;localhost:9092&quot;)\n                .option(&quot;subscribe&quot;, &quot;rating_modifiers&quot;)\n                .load();\n\n        // Register a table with Spark - this will hold the incoming events from Kafka\n        df.createOrReplaceTempView(&quot;rating_modifiers_incoming&quot;);\n\n        // key, value, timestamp are the columns available to me\n        // Note that you must cast value to a string in order to get readable results\n        Dataset&lt;Row&gt; results = session.sql(\n                &quot;select cast(value as string) as event_detail from rating_modifiers_incoming&quot;);\n\n        // Create a sink\n        // See https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#output-sinks for all sink types\n        StreamingQuery query = results.writeStream().format(&quot;console&quot;)\n                .outputMode(OutputMode.Append()) // values are Complete, Update, and Append - use Append for non-aggregation queries\n                .option(&quot;truncate&quot;, false)\n                .option(&quot;numRows&quot;, 30)\n                .start();\n\n        // Rinse and repeat\n        query.awaitTermination();\n    }\n}\n\n</code></pre>\n<p>Here is the .pom for the working version of the consumer:</p>\n<pre><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;\n&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;\n\txsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;\n\t&lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;\n\t&lt;url&gt;http://maven.apache.org&lt;/url&gt;\n\t\n    &lt;!-- above is all boilerplate\n        \n\t\tNOTE that I generated my base pom and project directory structure with the following maven command (run from \n\t\tthe directory ABOVE where you want the project to be created... e.g. /maven):\n        mvn archetype:generate -DgroupId=com.datastax.kellett -DartifactId=datastax-spark-streaming -DarchetypeArtifactId=maven-archetype-quickstart -DinteractiveMode=false\n\n        The junit dependency was created automatically for me, but I pasted in the other dependencies (if needed)\n        and the &lt;repositories&gt; section (if needed) and the &lt;properties&gt; section (if needed)\n\t\tand the &lt;build&gt; section all BEFORE importing the project into IntelliJ.  \n\t\tI imported by pointing to the datastax-spark-sql-v2 directory, and imported as Maven project.\n    --&gt;\t\n\t\n\t&lt;groupId&gt;com.datastax.kellett&lt;/groupId&gt;\n\t&lt;artifactId&gt;datastax-spark-streaming&lt;/artifactId&gt;\n\t&lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt;\n\t&lt;packaging&gt;jar&lt;/packaging&gt;\n\t&lt;name&gt;datastax-spark-streaming&lt;/name&gt;\n\n\t&lt;properties&gt; &lt;!-- Not currently used, but we keep them as doc and for possible future use --&gt;\n\t\t&lt;java.version&gt;1.8&lt;/java.version&gt;\n\t\t&lt;dse.version&gt;6.0.7&lt;/dse.version&gt;\n        &lt;scala.version&gt;2.11.8&lt;/scala.version&gt;\n\t&lt;/properties&gt;\n\n    &lt;dependencies&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;junit&lt;/groupId&gt;\n            &lt;artifactId&gt;junit&lt;/artifactId&gt;\n            &lt;version&gt;3.8.1&lt;/version&gt;\n            &lt;scope&gt;test&lt;/scope&gt;\n        &lt;/dependency&gt;\n\n&lt;!-- if below version (2.3.2) doesn&#39;t work, try 2.4.0 --&gt;\n        &lt;dependency&gt; &lt;!-- added 7/2/19 --&gt;\n            &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;\n            &lt;artifactId&gt;spark-core_2.11&lt;/artifactId&gt; &lt;!-- 2.11 is Scala version used to build Spark --&gt;\n            &lt;version&gt;2.3.2&lt;/version&gt; \t\t\t\t&lt;!-- this number refers to the Spark version --&gt;\n        &lt;/dependency&gt;  &lt;!--see https://spark.apache.org/downloads.html to get latest info for both of the above --&gt;\n\n        &lt;dependency&gt;\n            &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;\n            &lt;artifactId&gt;spark-sql_2.11&lt;/artifactId&gt; &lt;!-- 2.11 is Scala version used to build Spark --&gt;\n            &lt;version&gt;2.3.2&lt;/version&gt;  \t\t\t\t&lt;!-- this number refers to the Spark version --&gt;\n        &lt;/dependency&gt;  &lt;!--see https://spark.apache.org/downloads.html to get latest info for both of the above --&gt;\n\t\t\n\t\t&lt;dependency&gt; &lt;!-- This is for Spark Streaming (Dstream only???) --&gt;\n\t\t\t&lt;groupId&gt;org.apache.spark&lt;/groupId&gt;\n\t\t\t&lt;artifactId&gt;spark-streaming_2.11&lt;/artifactId&gt; &lt;!-- 2.11 is Scala version used to build Spark --&gt;\n\t\t\t&lt;version&gt;2.3.2&lt;/version&gt;  \t\t\t\t&lt;!-- this number refers to the Spark version --&gt;\n\t\t&lt;/dependency&gt;  &lt;!--see https://spark.apache.org/downloads.html to get latest info for both of the above --&gt;\n\n\t\t&lt;dependency&gt; &lt;!-- This is for Spark-Kafka integration --&gt;\n\t\t\t&lt;groupId&gt;org.apache.spark&lt;/groupId&gt;\n\t\t\t&lt;artifactId&gt;spark-streaming-kafka-0-10_2.11&lt;/artifactId&gt; &lt;!-- 2.11 is Scala version used to build Spark --&gt;\n\t\t\t&lt;version&gt;2.3.2&lt;/version&gt;  \t\t\t\t&lt;!-- this number refers to the Spark version --&gt;\n\t\t&lt;/dependency&gt;   &lt;!--see https://spark.apache.org/docs/2.3.2/streaming-kafka-integration.html to get latest info for the above --&gt;\n\n\t\t&lt;dependency&gt; &lt;!-- This is for Spark-Kafka STRUCTURED STREAMING --&gt;\n\t\t\t&lt;groupId&gt;org.apache.spark&lt;/groupId&gt;\n\t\t\t&lt;artifactId&gt;spark-sql-kafka-0-10_2.11&lt;/artifactId&gt; &lt;!-- 2.11 is Scala version used to build Spark --&gt;\n\t\t\t&lt;version&gt;2.3.2&lt;/version&gt;  \t\t\t\t&lt;!-- this number refers to the Spark version --&gt;\n\t\t&lt;/dependency&gt;\n\n        &lt;dependency&gt; &lt;!-- added 7/2/19 --&gt;\n            &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;\n            &lt;artifactId&gt;hadoop-hdfs&lt;/artifactId&gt;\n            &lt;version&gt;2.2.0&lt;/version&gt;\n        &lt;/dependency&gt;\n\t\t\n\t&lt;/dependencies&gt;\n\n    &lt;build&gt;\n        &lt;finalName&gt;datastax-spark-streaming&lt;/finalName&gt;\n        &lt;plugins&gt;\n            &lt;plugin&gt;\n                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;\n                &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt;\n                &lt;executions&gt;\n                    &lt;execution&gt;\n                        &lt;id&gt;create-my-bundle&lt;/id&gt;\n                        &lt;phase&gt;package&lt;/phase&gt;\n                        &lt;goals&gt;\n                            &lt;goal&gt;single&lt;/goal&gt;\n                        &lt;/goals&gt;\n                        &lt;configuration&gt;\n\n                            &lt;archive&gt;\n                                &lt;manifest&gt;\n                                    &lt;mainClass&gt;\n                                        com.datastax.kellett.Lab2Windows\n                                    &lt;/mainClass&gt;\n                                &lt;/manifest&gt;\n                            &lt;/archive&gt;\n\n                            &lt;descriptorRefs&gt;\n                                &lt;descriptorRef&gt;jar-with-dependencies&lt;/descriptorRef&gt;\n                            &lt;/descriptorRefs&gt;\n                        &lt;/configuration&gt;\n                    &lt;/execution&gt;\n                &lt;/executions&gt;\n            &lt;/plugin&gt;\n        &lt;/plugins&gt;\n    &lt;/build&gt;\n\n    &lt;repositories&gt;\n        &lt;repository&gt;\n            &lt;id&gt;DataStax-Repo&lt;/id&gt;\n            &lt;url&gt;https://datastax.artifactoryonline.com/datastax/public-repos/&lt;/url&gt;\n        &lt;/repository&gt;\n    &lt;/repositories&gt;\n\n&lt;/project&gt;\n\n</code></pre>\n<p>Here is the source code for the non-working version of the consumer:</p>\n<pre><code>package com.datastax.kellett;\n\nimport org.apache.spark.sql.Dataset;\nimport org.apache.spark.sql.Row;\nimport org.apache.spark.sql.SparkSession;\nimport org.apache.spark.sql.streaming.OutputMode;\nimport org.apache.spark.sql.streaming.StreamingQuery;\nimport org.apache.spark.sql.streaming.StreamingQueryException;\n\npublic class Lab2 {\n\n    public static void main(String[] args) throws StreamingQueryException {\n\n        // Connect to Spark\n        System.out.println(&quot;Version: 7/9/19 01 with kitchen-sink pom&quot;);\n        System.out.println(&quot;About to create session...&quot;);\n        SparkSession session = SparkSession.builder()\n                .appName(&quot;Spark Structured Streaming Lab2&quot;) // any name you like --- displays in Spark Management UI\n                //       .enableHiveSupport()    // Enables connection to a Hive metastore, support for hive SerDes and Hive user-defined functions\n                .getOrCreate();\n\n        // Read available incoming events into a data frame\n        System.out.println(&quot;About to read Kafka event stream...&quot;);\n        Dataset&lt;Row&gt; df = session.readStream()\n                //.format(&quot;kafka&quot;)\n                .format(&quot;org.apache.spark.sql.kafka010.KafkaSourceProvider&quot;)\n                .option(&quot;kafka.bootstrap.servers&quot;, &quot;localhost:9092&quot;)\n                .option(&quot;subscribe&quot;, &quot;rating_modifiers&quot;)\n                .load();\n\n        // Register a table with Spark - this will hold the incoming events from Kafka\n        System.out.println(&quot;About to create temp view...&quot;);\n        df.createOrReplaceTempView(&quot;rating_modifiers_incoming&quot;);\n\n        // key, value, timestamp are the columns available to me\n        // Note that you must cast value to a string in order to get readable results\n        System.out.println(&quot;About to create results dataframe&quot;);\n        Dataset&lt;Row&gt; results = session.sql(\n                &quot;select cast(value as string) as event_detail from rating_modifiers_incoming&quot;);\n\n        // Create a sink\n        // See https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#output-sinks for all sink types\n        System.out.println(&quot;About to write results to console...&quot;);\n        StreamingQuery query = results.writeStream().format(&quot;console&quot;)\n                .outputMode(OutputMode.Append()) // values are Complete, Update, and Append - use Append for non-aggregation queries\n                .option(&quot;truncate&quot;, false)\n                .option(&quot;numRows&quot;, 30)\n                .start();\n\n        // Rinse and repeat\n        System.out.println(&quot;About to enter awaitTermination...&quot;);\n        query.awaitTermination();\n    }\n}\n\n</code></pre>\n<p>Here is the .pom for the non-working version of the consumer:</p>\n<pre><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;\n&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;\n         xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;\n    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;\n    &lt;url&gt;http://maven.apache.org&lt;/url&gt;\n\n    &lt;!-- above is all boilerplate\n        \n\t\tNOTE that I generated my base pom and project directory structure with the following maven command (run from \n\t\tthe directory ABOVE where you want the project to be created... e.g. /maven):\n        mvn archetype:generate -DgroupId=com.datastax.kellett -DartifactId=datastax-spark-sql-v2 -DarchetypeArtifactId=maven-archetype-quickstart -DinteractiveMode=false\n\n        The junit dependency was created automatically for me, but I pasted in the other dependencies (if needed)\n        and the &lt;repositories&gt; section (if needed) and the &lt;properties&gt; section (if needed)\n\t\tand the &lt;build&gt; section all BEFORE importing the project into IntelliJ.\n\t\tI imported by pointing to the datastax-spark-sql-v2 directory, and imported as Maven project.\n    --&gt;\n\n    &lt;groupId&gt;com.datastax.kellett&lt;/groupId&gt;\n    &lt;artifactId&gt;datastax-spark-streaming-dse&lt;/artifactId&gt;\n    &lt;version&gt;1.0&lt;/version&gt;\n    &lt;packaging&gt;jar&lt;/packaging&gt;\n    &lt;name&gt;datastax-spark-streaming-dse&lt;/name&gt;\n\n    &lt;properties&gt; &lt;!-- Not currently used, but we keep them as doc and for possible future use --&gt;\n        &lt;java.version&gt;1.8&lt;/java.version&gt;\n        &lt;dse.version&gt;6.0.7&lt;/dse.version&gt;\n        &lt;scala.version&gt;2.11.8&lt;/scala.version&gt;\n    &lt;/properties&gt;\n\n    &lt;dependencies&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;junit&lt;/groupId&gt;\n            &lt;artifactId&gt;junit&lt;/artifactId&gt;\n            &lt;version&gt;3.8.1&lt;/version&gt;\n            &lt;scope&gt;test&lt;/scope&gt;\n        &lt;/dependency&gt;\n\n        &lt;dependency&gt; &lt;!-- added 7/2/19 --&gt;\n            &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;\n            &lt;artifactId&gt;spark-core_2.11&lt;/artifactId&gt; &lt;!-- 2.11 is Scala version used to build Spark --&gt;\n            &lt;version&gt;2.3.2&lt;/version&gt; \t\t\t\t&lt;!-- this number refers to the Spark version --&gt;\n        &lt;/dependency&gt;  &lt;!--see https://spark.apache.org/downloads.html to get latest info for both of the above --&gt;\n\n        &lt;dependency&gt;\n            &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;\n            &lt;artifactId&gt;spark-sql_2.11&lt;/artifactId&gt; &lt;!-- 2.11 is Scala version used to build Spark --&gt;\n            &lt;version&gt;2.3.2&lt;/version&gt;  \t\t\t\t&lt;!-- this number refers to the Spark version --&gt;\n        &lt;/dependency&gt;  &lt;!--see https://spark.apache.org/downloads.html to get latest info for both of the above --&gt;\n\n        &lt;dependency&gt; &lt;!-- This is for Spark Streaming (Dstream only???) --&gt;\n            &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;\n            &lt;artifactId&gt;spark-streaming_2.11&lt;/artifactId&gt; &lt;!-- 2.11 is Scala version used to build Spark --&gt;\n            &lt;version&gt;2.3.2&lt;/version&gt;  \t\t\t\t&lt;!-- this number refers to the Spark version --&gt;\n        &lt;/dependency&gt;  &lt;!--see https://spark.apache.org/downloads.html to get latest info for both of the above --&gt;\n\n        &lt;dependency&gt; &lt;!-- This is for Spark-Kafka integration --&gt;\n            &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;\n            &lt;artifactId&gt;spark-streaming-kafka-0-10_2.11&lt;/artifactId&gt; &lt;!-- 2.11 is Scala version used to build Spark --&gt;\n            &lt;version&gt;2.3.2&lt;/version&gt;  \t\t\t\t&lt;!-- this number refers to the Spark version --&gt;\n        &lt;/dependency&gt;   &lt;!--see https://spark.apache.org/docs/2.3.2/streaming-kafka-integration.html to get latest info for the above --&gt;\n\n        &lt;dependency&gt; &lt;!-- This is for Spark-Kafka STRUCTURED STREAMING --&gt;\n            &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;\n            &lt;artifactId&gt;spark-sql-kafka-0-10_2.11&lt;/artifactId&gt; &lt;!-- 2.11 is Scala version used to build Spark --&gt;\n            &lt;version&gt;2.3.2&lt;/version&gt;  \t\t\t\t&lt;!-- this number refers to the Spark version --&gt;\n        &lt;/dependency&gt;\n\n        &lt;dependency&gt; &lt;!-- added 7/2/19 --&gt;\n            &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;\n            &lt;artifactId&gt;hadoop-hdfs&lt;/artifactId&gt;\n            &lt;version&gt;2.2.0&lt;/version&gt;\n        &lt;/dependency&gt;\n\n        &lt;dependency&gt;\n            &lt;groupId&gt;com.datastax.dse&lt;/groupId&gt;\n            &lt;artifactId&gt;dse-spark-dependencies&lt;/artifactId&gt;\n            &lt;version&gt;&lt;/version&gt;\n            &lt;scope&gt;provided&lt;/scope&gt;\n        &lt;/dependency&gt;\n\n    &lt;/dependencies&gt;\n\n\n    &lt;build&gt;\n        &lt;finalName&gt;datastax-spark-streaming-dse&lt;/finalName&gt;\n        &lt;plugins&gt;\n            &lt;plugin&gt;\n                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;\n                &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt;\n                &lt;executions&gt;\n                    &lt;execution&gt;\n                        &lt;id&gt;create-my-bundle&lt;/id&gt;\n                        &lt;phase&gt;package&lt;/phase&gt;\n                        &lt;goals&gt;\n                            &lt;goal&gt;single&lt;/goal&gt;\n                        &lt;/goals&gt;\n                        &lt;configuration&gt;\n                            &lt;descriptorRefs&gt;\n                                &lt;descriptorRef&gt;jar-with-dependencies&lt;/descriptorRef&gt;\n                            &lt;/descriptorRefs&gt;\n                        &lt;/configuration&gt;\n                    &lt;/execution&gt;\n                &lt;/executions&gt;\n            &lt;/plugin&gt;\n        &lt;/plugins&gt;\n    &lt;/build&gt;\n\n    &lt;repositories&gt;\n        &lt;repository&gt;\n            &lt;id&gt;DataStax-Repo&lt;/id&gt;\n            &lt;url&gt;https://datastax.artifactoryonline.com/datastax/public-repos/&lt;/url&gt;\n        &lt;/repository&gt;\n\n        &lt;repository&gt;\n            &lt;id&gt;DataStax-Repo-too&lt;/id&gt;\n            &lt;url&gt;https://repo.datastax.com/public-repos/&lt;/url&gt;\n        &lt;/repository&gt;\n\n    &lt;/repositories&gt;\n\n&lt;/project&gt;\n\n</code></pre>\n</div>"}]}},{"text":"%cassandra\n\n// cell 8 - FUTURE STUFF - NOT PART OF PROBLEM ILLUSTRATION\n\n// DDL\n\n// start from a clean slate\n\ndrop keyspace if exists streaming_workshop;\n\n// create the keyspace for the workshop\n\ncreate keyspace if not exists streaming_workshop \n    WITH replication = {'class': 'SimpleStrategy', 'replication_factor' : 3};\n\n// create all tables\n\ncreate table if not exists streaming_workshop.ratings_modifiers_all (\n    instrument_symbol           text,\n    ethical_category            text,\n    event_sequence_number       int,\n    source_url                  text,\n    source_weight               int,\n    source_sentiment            int,\n    primary key ((instrument_symbol, ethical_category), event_sequence_number)\n)\nwith clustering order by (event_sequence_number desc);\n\ncreate table if not exists streaming_workshop.ratings_modifiers_heavyweight (\n    instrument_symbol           text,\n    ethical_category            text,\n    event_sequence_number       int,\n    source_url                  text,\n    source_weight               int,\n    source_sentiment            int,\n    primary key ((instrument_symbol, ethical_category), event_sequence_number)\n)\nwith clustering order by (event_sequence_number desc);\n\ncreate table if not exists streaming_workshop.ratings_modifiers_aggregated_complete (\n    instrument_symbol           text,\n    ethical_category            text,\n    total_events                int,\n    weighted_sentiment_change   int,\n    primary key ((instrument_symbol), ethical_category)\n)\nwith clustering order by (ethical_category asc);\n\ncreate table if not exists streaming_workshop.ratings_modifiers_aggregated_windowed (\n    instrument_symbol           text,\n    ethical_category            text,\n    event_time_window           text,\n    total_events                int,\n    weighted_sentiment_change   int,\n    primary key ((instrument_symbol), ethical_category, event_time_window)\n)\nwith clustering order by (ethical_category asc, event_time_window desc);","dateUpdated":"2019-07-07T19:45:55+0000","config":{"editorSetting":{"language":"text","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/undefined","editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1562528755533_482312917","id":"20190630-193659_1079039557","dateCreated":"2019-07-07T19:45:55+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:905"},{"text":"%cassandra\n","dateUpdated":"2019-07-07T19:45:55+0000","config":{"colWidth":12,"editorMode":"ace/mode/undefined","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1562528755537_493085887","id":"20190703-194622_1480262872","dateCreated":"2019-07-07T19:45:55+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:906"}],"name":"bkplay","id":"2EESGDE5Y","angularObjects":{"2EHETX6SC:shared_process":[],"2EE9UK628:shared_process":[],"2EEACY9PR:shared_process":[],"2EFVV2FNS:shared_process":[],"2EEJBPS92:shared_process":[],"2EJ1746NE:shared_process":[],"2EJ339CUC:shared_process":[],"2EED6EHAE:shared_process":[],"2EGAVSJA6:shared_process":[],"2EFFD8ETH:shared_process":[],"2EHD31FXC:shared_process":[],"2EHAP1Q44:shared_process":[],"2EHN7WY4G:shared_process":[],"2EF54WKX2:shared_process":[],"2EE8CCYHT:shared_process":[],"2EFKAU3B6:shared_process":[],"2EHW9HHZE:shared_process":[],"2EGF9247E:shared_process":[],"2EFRYUTZJ:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}