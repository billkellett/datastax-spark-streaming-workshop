{"paragraphs":[{"text":"%md\n\ncell 1\n\n# **Lab 1 - Streaming Detail-Level Data into Cassandra Tables**\n\nRightVest is an investment firm that enables buyers to select investments based on their personal Ethical Investing priorities.  The firm provides a number of Ethical Investing categories (e.g., Protects Environment, Humane to Workers, etc.), and enables individual investors to prioritize or de-prioritize each category for their own investment portfolios.  \n\nRightVest must provide ratings for all investment instruments.  Each instrument is rated on a scale of 1 to 5 for each Ethical Investing category.\n\nOf course, ratings must change over time as new information is available for each investment instrument.  In order to keep ratings up-to-date, RightVest has built a software platform called Etho-Tron.  Etho-Tron continually scans many sources of information, and uses AI to perform sentiment analysis on the information.  A \"rating modifier\" is than calculated, which is used to update the rating of each investment.\n\nEtho-Tron logs its rating modifiers by publishing events to Kafka.  Our mission today is to receive this streaming information from Kafka, and persist it in a Cassandra table.  Each Kafka event provides the following data:\n\n- Investment Ticker Symbol\n- Event Sequence Number\n- Ethical Category (Protects Environment, Humane to workers, etc.)\n- Information Source URL\n- Information Source Credibility Weight (an integer from 1 to 10)\n- Information Source Sentiment Score (an integer from -100 to 100)\n\nLet's see how we can use DSE Analytics to capture this stream of incoming data and persist it into Cassandra tables.\n\n### Our Lab Environment\n\nOur Lab Environment includes the following:\n\n- DataStax Enterprise running on a three-node cluster\n- A running Kafka instance, configured with a topic named \"rating_modifiers\"\n- A simple standalone Java program that reads a .csv file and uses its data to publish events to Kafka at the rate of two events per second\n- A DataStax Spark program that consumes these events and writes them to a Cassandra table","dateUpdated":"2019-07-10T20:02:19+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>cell 1</p>\n<h1><strong>Lab 1 - Streaming Detail-Level Data into Cassandra Tables</strong></h1>\n<p>RightVest is an investment firm that enables buyers to select investments based on their personal Ethical Investing priorities. The firm provides a number of Ethical Investing categories (e.g., Protects Environment, Humane to Workers, etc.), and enables individual investors to prioritize or de-prioritize each category for their own investment portfolios. </p>\n<p>RightVest must provide ratings for all investment instruments. Each instrument is rated on a scale of 1 to 5 for each Ethical Investing category.</p>\n<p>Of course, ratings must change over time as new information is available for each investment instrument. In order to keep ratings up-to-date, RightVest has built a software platform called Etho-Tron. Etho-Tron continually scans many sources of information, and uses AI to perform sentiment analysis on the information. A &ldquo;rating modifier&rdquo; is than calculated, which is used to update the rating of each investment.</p>\n<p>Etho-Tron logs its rating modifiers by publishing events to Kafka. Our mission today is to receive this streaming information from Kafka, and persist it in a Cassandra table. Each Kafka event provides the following data:</p>\n<ul>\n  <li>Investment Ticker Symbol</li>\n  <li>Event Sequence Number</li>\n  <li>Ethical Category (Protects Environment, Humane to workers, etc.)</li>\n  <li>Information Source URL</li>\n  <li>Information Source Credibility Weight (an integer from 1 to 10)</li>\n  <li>Information Source Sentiment Score (an integer from -100 to 100)</li>\n</ul>\n<p>Let&rsquo;s see how we can use DSE Analytics to capture this stream of incoming data and persist it into Cassandra tables.</p>\n<h3>Our Lab Environment</h3>\n<p>Our Lab Environment includes the following:</p>\n<ul>\n  <li>DataStax Enterprise running on a three-node cluster</li>\n  <li>A running Kafka instance, configured with a topic named &ldquo;rating_modifiers&rdquo;</li>\n  <li>A simple standalone Java program that reads a .csv file and uses its data to publish events to Kafka at the rate of two events per second</li>\n  <li>A DataStax Spark program that consumes these events and writes them to a Cassandra table</li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1562788939056_-1527469458","id":"20190710-160628_1108150746","dateCreated":"2019-07-10T20:02:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:2070"},{"text":"%sh\n\n# cell 2\n\n# First, let's make sure Kafka is running\n\n# This utility asks Kafka to list its available topics.  We should see the topic \"rating_modifiers\"\n\ncd /tmp/datastax-spark-streaming-workshop/kafka_2.12-2.2.0/bin\n\n./kafka-topics.sh --list --bootstrap-server localhost:9092","dateUpdated":"2019-07-10T20:57:41+0000","config":{"colWidth":12,"editorMode":"ace/mode/sh","results":{},"enabled":true,"editorSetting":{"language":"sh","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1562788939057_-1527854207","id":"20190710-161109_1919623156","dateCreated":"2019-07-10T20:02:19+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2071","user":"anonymous","dateFinished":"2019-07-10T20:57:43+0000","dateStarted":"2019-07-10T20:57:41+0000"},{"text":"%cassandra\n\n// cell 3\n\n// Now let's examine our target tables on Cassandra\n\ndescribe keyspace streaming_workshop;\n","dateUpdated":"2019-07-10T20:57:53+0000","config":{"colWidth":12,"editorMode":"ace/mode/undefined","results":{},"enabled":true,"editorSetting":{"language":"text","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1562788939058_-1526699960","id":"20190710-170813_341225013","dateCreated":"2019-07-10T20:02:19+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2072","user":"anonymous","dateFinished":"2019-07-10T20:57:53+0000","dateStarted":"2019-07-10T20:57:53+0000"},{"text":"%cassandra\nselect * from streaming_workshop.ratings_modifiers_all;","user":"anonymous","dateUpdated":"2019-07-10T20:58:07+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1562789164802_657441280","id":"20190710-200604_1139360496","dateCreated":"2019-07-10T20:06:04+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:2380","dateFinished":"2019-07-10T20:58:07+0000","dateStarted":"2019-07-10T20:58:07+0000","errorMessage":""},{"text":"%md\n\nTo start the Spark job that listens for Kafka events and writes them to Cassandra, execute the following in a terminal window.\nNote that this will not work if you try to run from a shell in this notebook... Zeppelin will have a memory error.\n\n```\ncd /tmp/datastax-spark-streaming-workshop/executables\n\ndse spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.2.3,org.apache.spark:spark-streaming-kafka-0-10_2.11:2.2.3 --class com.datastax.kellett.Lab1 datastax-spark-streaming-v2-0.1.jar\n```","user":"anonymous","dateUpdated":"2019-07-10T20:21:11+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"text","editOnDblClick":false},"editorMode":"ace/mode/text","editorHide":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1562789390190_-1626134863","id":"20190710-200950_2008260297","dateCreated":"2019-07-10T20:09:50+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:2466","dateFinished":"2019-07-10T20:21:08+0000","dateStarted":"2019-07-10T20:21:08+0000","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>To start the Spark job that listens for Kafka events and writes them to Cassandra, execute the following in a terminal window.<br/>Note that this will not work if you try to run from a shell in this notebook&hellip; Zeppelin will have a memory error.</p>\n<pre><code>cd /tmp/datastax-spark-streaming-workshop/executables\n\ndse spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.2.3,org.apache.spark:spark-streaming-kafka-0-10_2.11:2.2.3 --class com.datastax.kellett.Lab1 datastax-spark-streaming-v2-0.1.jar\n</code></pre>\n</div>"}]}},{"text":"%md\n","user":"anonymous","dateUpdated":"2019-07-10T20:20:10+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1562790010592_-1693760369","id":"20190710-202010_1573903280","dateCreated":"2019-07-10T20:20:10+0000","status":"READY","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:2576"}],"name":"Lab_1_-_Streaming_Detail_Data","id":"2EGZSGN9S","angularObjects":{"2EJ3UKRMR:shared_process":[],"2EF8R44MD:shared_process":[],"2EJ2DSHXZ:shared_process":[],"2EHZ7UTGV:shared_process":[],"2EGHSMNVJ:shared_process":[],"2EHTKM78A:shared_process":[],"2EGVHCDH4:shared_process":[],"2EFVNG8RH:shared_process":[],"2EFVQ3KPK:shared_process":[],"2EJ2AXFV5:shared_process":[],"2EGYWGAKV:shared_process":[],"2EEZCYPTH:shared_process":[],"2EET3EG6E:shared_process":[],"2EFJM6KW2:shared_process":[],"2EG1N9MMH:shared_process":[],"2EEW11KMF:shared_process":[],"2EFM4CZ94:shared_process":[],"2EF1NV642:shared_process":[],"2EHYQC4CP:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}