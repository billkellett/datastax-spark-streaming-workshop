{"paragraphs":[{"text":"%md\n\ncell 1\n\n# **Lab 1 - Streaming Detail-Level Data into Cassandra Tables**\n\nRightVest is an investment firm that enables buyers to select investments based on their personal Ethical Investing priorities.  The firm provides a number of Ethical Investing categories (e.g., Protects Environment, Humane to Workers, etc.), and enables individual investors to prioritize or de-prioritize each category for their own investment portfolios.  \n\nRightVest must provide ratings for all investment instruments.  Each instrument is rated on a scale of 1 to 5 for each Ethical Investing category.\n\nOf course, ratings must change over time as new information is available for each investment instrument.  In order to keep ratings up-to-date, RightVest has built a software platform called Etho-Tron.  Etho-Tron continually scans many sources of information, and uses AI to perform sentiment analysis on the information.  A \"rating modifier\" is than calculated, which is used to update the rating of each investment.\n\nEtho-Tron logs its rating modifiers by publishing events to Kafka.  Our mission today is to receive this streaming information from Kafka, and persist it in a Cassandra table.  Each Kafka event provides the following data:\n\n- Investment Ticker Symbol\n- Event Sequence Number\n- Ethical Category (Protects Environment, Humane to workers, etc.)\n- Information Source URL\n- Information Source Credibility Weight (an integer from 1 to 10)\n- Information Source Sentiment Score (an integer from -100 to 100)\n\nLet's see how we can use DSE Analytics to capture this stream of incoming data and persist it into Cassandra tables.\n\n### Our Lab Environment\n\nOur Lab Environment includes the following:\n\n- DataStax Enterprise running on a three-node cluster\n- A running Kafka instance, configured with a topic named \"rating_modifiers\"\n- A simple standalone Java program that reads a .csv file and uses its data to publish events to Kafka at the rate of two events per second\n- A DataStax Spark program that consumes these events and writes them to a Cassandra table","user":"anonymous","dateUpdated":"2019-07-10T17:14:05+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>cell 1</p>\n<h1><strong>Lab 1 - Streaming Detail-Level Data into Cassandra Tables</strong></h1>\n<p>RightVest is an investment firm that enables buyers to select investments based on their personal Ethical Investing priorities. The firm provides a number of Ethical Investing categories (e.g., Protects Environment, Humane to Workers, etc.), and enables individual investors to prioritize or de-prioritize each category for their own investment portfolios. </p>\n<p>RightVest must provide ratings for all investment instruments. Each instrument is rated on a scale of 1 to 5 for each Ethical Investing category.</p>\n<p>Of course, ratings must change over time as new information is available for each investment instrument. In order to keep ratings up-to-date, RightVest has built a software platform called Etho-Tron. Etho-Tron continually scans many sources of information, and uses AI to perform sentiment analysis on the information. A &ldquo;rating modifier&rdquo; is than calculated, which is used to update the rating of each investment.</p>\n<p>Etho-Tron logs its rating modifiers by publishing events to Kafka. Our mission today is to receive this streaming information from Kafka, and persist it in a Cassandra table. Each Kafka event provides the following data:</p>\n<ul>\n  <li>Investment Ticker Symbol</li>\n  <li>Event Sequence Number</li>\n  <li>Ethical Category (Protects Environment, Humane to workers, etc.)</li>\n  <li>Information Source URL</li>\n  <li>Information Source Credibility Weight (an integer from 1 to 10)</li>\n  <li>Information Source Sentiment Score (an integer from -100 to 100)</li>\n</ul>\n<p>Let&rsquo;s see how we can use DSE Analytics to capture this stream of incoming data and persist it into Cassandra tables.</p>\n<h3>Our Lab Environment</h3>\n<p>Our Lab Environment includes the following:</p>\n<ul>\n  <li>DataStax Enterprise running on a three-node cluster</li>\n  <li>A running Kafka instance, configured with a topic named &ldquo;rating_modifiers&rdquo;</li>\n  <li>A simple standalone Java program that reads a .csv file and uses its data to publish events to Kafka at the rate of two events per second</li>\n  <li>A DataStax Spark program that consumes these events and writes them to a Cassandra table</li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1562774788742_-1686882129","id":"20190710-160628_1108150746","dateCreated":"2019-07-10T16:06:28+0000","dateStarted":"2019-07-10T17:14:05+0000","dateFinished":"2019-07-10T17:14:05+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:2662"},{"text":"%sh\n\n# cell 2\n\n# First, let's make sure Kafka is running\n\n# This utility asks Kafka to list its available topics.  We should see the topic \"rating_modifiers\"\n\ncd /tmp/datastax-spark-streaming-workshop/kafka_2.12-2.2.0/bin\n\n./kafka-topics.sh --list --bootstrap-server localhost:9092","user":"anonymous","dateUpdated":"2019-07-10T18:14:03+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"sh","editOnDblClick":false},"editorMode":"ace/mode/sh"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1562775069538_502184226","id":"20190710-161109_1919623156","dateCreated":"2019-07-10T16:11:09+0000","dateStarted":"2019-07-10T17:08:39+0000","dateFinished":"2019-07-10T17:08:41+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2663"},{"text":"%sh\n","user":"anonymous","dateUpdated":"2019-07-10T17:08:13+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1562778493652_968399725","id":"20190710-170813_341225013","dateCreated":"2019-07-10T17:08:13+0000","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:2664"}],"name":"Lab_1_-_Streaming_Detail_Data","id":"2EGXXTTB6","angularObjects":{"2EHBP6YFB:shared_process":[],"2EF3NKQNZ:shared_process":[],"2EG43Q3U4:shared_process":[],"2EFJCW7EF:shared_process":[],"2EFBM19FS:shared_process":[],"2EFU6K97U:shared_process":[],"2EHSG6XRT:shared_process":[],"2EFFZBY8D:shared_process":[],"2EHDTYMP2:shared_process":[],"2EFWYDC21:shared_process":[],"2EF3CK67Z:shared_process":[],"2EEUN1W16:shared_process":[],"2EHDEA7D7:shared_process":[],"2EFHBJBFM:shared_process":[],"2EEWU7AV9:shared_process":[],"2EFUZ8VBB:shared_process":[],"2EJBMH447:shared_process":[],"2EHJ9NV4A:shared_process":[],"2EF5J7Z8W:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}