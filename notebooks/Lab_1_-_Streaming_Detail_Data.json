{"paragraphs":[{"text":"%md\n\ncell 1 (ignore the dse.version text box above.  It's just something about the way Zeppelin processes markdown)\n\n# **Lab 1 - Streaming Detail-Level Data into Cassandra Tables**\n\nRightVest is an investment firm that enables buyers to select investments based on their personal Ethical Investing priorities.  The firm provides a number of Ethical Investing categories (e.g., Protects Environment, Humane to Workers, etc.), and enables individual investors to prioritize or de-prioritize each category for their own investment portfolios.  \n\nRightVest must provide ratings for all investment instruments.  Each instrument is rated on a scale of 1 to 5 for each Ethical Investing category.\n\nOf course, ratings must change over time as new information is available for each investment instrument.  In order to keep ratings up-to-date, RightVest has built a software platform called Etho-Tron.  Etho-Tron continually scans many sources of information, and uses AI to perform sentiment analysis on the information.  A \"rating modifier\" is than calculated, which is used to update the rating of each investment.\n\nEtho-Tron logs its rating modifiers by publishing events to Kafka.  Our mission today is to receive this streaming information from Kafka, and persist it in a Cassandra table.  Each Kafka event provides the following data:\n\n- Investment Ticker Symbol\n- Event Sequence Number\n- Ethical Category (Protects Environment, Humane to workers, etc.)\n- Information Source URL\n- Information Source Credibility Weight (an integer from 1 to 10)\n- Information Source Sentiment Score (an integer from -100 to 100)\n\nLet's see how we can use DSE Analytics to capture this stream of incoming data and persist it into Cassandra tables.\n\n### Our Lab Environment\n\nOur Lab Environment includes the following:\n\n- DataStax Enterprise running on a three-node cluster\n- A running Kafka instance, configured with a topic named \"rating_modifiers\"\n- A simple standalone Java program that reads a .csv file and uses its data to publish events to Kafka at the rate of two events per second\n- A DataStax Spark program that consumes these events and writes them to a Cassandra table\n\n### The Program\n\nLet's take a look at the code we need to consume Kafka events and write them to Cassandra.\n\n```\npackage com.datastax.kellett;\n\nimport org.apache.log4j.Level;\nimport org.apache.log4j.Logger;\nimport org.apache.spark.sql.Dataset;\nimport org.apache.spark.sql.Row;\nimport org.apache.spark.sql.SparkSession;\nimport org.apache.spark.sql.streaming.OutputMode;\nimport org.apache.spark.sql.streaming.StreamingQuery;\nimport org.apache.spark.sql.streaming.StreamingQueryException;\n\npublic class Lab1 {\n\n    public static void main(String[] args) throws StreamingQueryException {\n\n        Logger.getLogger(\"org.apache\").setLevel(Level.WARN);\n        Logger.getLogger(\"org.apache.spark.storage\").setLevel(Level.ERROR);\n\n        // Connect to Spark\n        System.out.println(\"DataStax Spark Streaming Workshop Lab 1\");\n        SparkSession session = SparkSession.builder()\n                //.master(\"local[*]\")  // use for debugging if necessary\n                .appName(\"Spark Structured Streaming Lab1\") // any name you like --- displays in Spark Management UI\n                .getOrCreate();\n\n        // Read available incoming events into a data frame\n        Dataset<Row> df = session.readStream()\n                .format(\"kafka\") // If any trouble resolving \"kafka\" try long name \"org.apache.spark.sql.kafka010.KafkaSourceProvider\"\n                .option(\"kafka.bootstrap.servers\", \"localhost:9092\") // point to kafka instance\n                .option(\"subscribe\", \"rating_modifiers\") // kafka topic we want to subscribe\n                .load();\n\n        // Register a table with Spark - this will hold the incoming events from Kafka\n        df.createOrReplaceTempView(\"rating_modifiers_incoming\");\n\n        // key, value, timestamp are the columns available to me\n        // Note that you must cast value to a string in order to get readable results\n        //\n        // This rather ungainly SQL statement is a way to parse a CSV string into individual columns.\n        // The innermost cast is necessary to make the kafka value field readable.\n        // The next-outer substring_index goes to the delimiter just past the target field, and takes everything\n        // to the left of it.\n        // The outermost substring_index takes the intermediate result and plucks out the rightmost field,\n        // which is my final target.\n        Dataset<Row> results = session.sql(\n                \"select    substring_index( cast(value as string), ',', 1)                     as instrument_symbol, \"\n                        + \"substring_index( substring_index( cast(value as string), ',', 2), ',', -1) as event_sequence_number, \"\n                        + \"substring_index( substring_index( cast(value as string), ',', 3), ',', -1) as ethical_category, \"\n                        + \"substring_index( substring_index( cast(value as string), ',', 4), ',', -1) as source_url, \"\n                        + \"substring_index( substring_index( cast(value as string), ',', 5), ',', -1) as source_weight, \"\n                        + \"substring_index( substring_index( cast(value as string), ',', 6), ',', -1) as source_sentiment \"\n                        + \"from rating_modifiers_incoming\");\n\n        // Create a sink\n        // See https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#output-sinks for standard sink types\n        // Here we use DSE's Cassandra sink\n        StreamingQuery query = results.writeStream().format(\"org.apache.spark.sql.cassandra\")\n                .outputMode(OutputMode.Append()) // values are Complete, Update, and Append - use Append for non-aggregation queries\n                .option(\"keyspace\", \"streaming_workshop\")\n                .option(\"table\", \"ratings_modifiers_all\")\n                .option(\"checkpointLocation\", \"dsefs://node0:5598/checkpoint/lab1/\") // enables query restart after a failure\n                .start();\n\n        // Writes to console... use for debugging if necessary\n//        StreamingQuery query = results.writeStream().format(\"console\")\n//                .outputMode(OutputMode.Append()) // values are Complete, Update, and Append - use Append for non-aggregation queries\n//                .option(\"truncate\", false)\n//                .option(\"numRows\", 30)\n//                .start();\n\n        // Rinse and repeat\n        query.awaitTermination();\n    }\n}\n```\n\nNow let's look at the Maven Build.\n\n```\n<project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n  xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\">\n  <modelVersion>4.0.0</modelVersion>\n  \n    <!-- above is all boilerplate\n        \n\t\tNOTE that I generated my base pom and project directory structure with the following maven command (run from \n\t\tthe directory ABOVE where you want the project to be created... e.g. /maven):\n        mvn archetype:generate -DgroupId=com.datastax.kellett -DartifactId=datastax-spark-streaming-v2 -DarchetypeArtifactId=maven-archetype-quickstart -DinteractiveMode=false\n\n    -->\t\n\n  <groupId>com.datastax.kellett</groupId>\n  <artifactId>datastax-spark-streaming-v2</artifactId>\n  <version>0.1</version>\n  <packaging>jar</packaging>\n  <name>datastax-spark-streaming-v2</name>\n\n  <properties>\n    <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>\n    <dse.version>6.0.7</dse.version>\n  </properties>\n\n  <dependencies>\n\n    <dependency>\n      <groupId>junit</groupId>\n      <artifactId>junit</artifactId>\n      <version>3.8.1</version>\n      <scope>test</scope>\n    </dependency>\n\n    <dependency>\n      <groupId>com.datastax.dse</groupId>\n      <artifactId>dse-spark-dependencies</artifactId>\n      <version>${dse.version}</version>\n      <scope>provided</scope>\n    </dependency>\n    \n  </dependencies>\n\n  <repositories>\n    <repository>\n      <id>DataStax-Repo</id>\n      <url>https://repo.datastax.com/public-repos/</url>\n    </repository>\n  </repositories>\n\n  <build>\n    <plugins>\n      <plugin>\n        <groupId>org.apache.maven.plugins</groupId>\n        <artifactId>maven-compiler-plugin</artifactId>\n        <version>3.5.1</version>\n        <configuration>\n          <source>1.8</source>\n          <target>1.8</target>\n        </configuration>\n      </plugin>\n      <plugin>\n        <groupId>org.apache.maven.plugins</groupId>\n        <artifactId>maven-shade-plugin</artifactId>\n        <version>2.4.3</version>\n        <executions>\n          <execution>\n            <phase>package</phase>\n            <goals>\n              <goal>shade</goal>\n            </goals>\n            <configuration>\n            </configuration>\n          </execution>\n        </executions>\n      </plugin>\n    </plugins>\n  </build>\n</project>\n```\n\n## **_OK, let's run it!_**","dateUpdated":"2019-07-11T15:52:22+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{"dse.version":""},"forms":{"dse.version":{"name":"dse.version","defaultValue":"","hidden":false,"$$hashKey":"object:2746"}}},"apps":[],"jobName":"paragraph_1562849697760_1730273562","id":"20190710-160628_1108150746","dateCreated":"2019-07-11T12:54:57+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:1481","user":"anonymous","dateFinished":"2019-07-11T15:52:22+0000","dateStarted":"2019-07-11T15:52:22+0000","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>cell 1 (ignore the dse.version text box above. It&rsquo;s just something about the way Zeppelin processes markdown)</p>\n<h1><strong>Lab 1 - Streaming Detail-Level Data into Cassandra Tables</strong></h1>\n<p>RightVest is an investment firm that enables buyers to select investments based on their personal Ethical Investing priorities. The firm provides a number of Ethical Investing categories (e.g., Protects Environment, Humane to Workers, etc.), and enables individual investors to prioritize or de-prioritize each category for their own investment portfolios. </p>\n<p>RightVest must provide ratings for all investment instruments. Each instrument is rated on a scale of 1 to 5 for each Ethical Investing category.</p>\n<p>Of course, ratings must change over time as new information is available for each investment instrument. In order to keep ratings up-to-date, RightVest has built a software platform called Etho-Tron. Etho-Tron continually scans many sources of information, and uses AI to perform sentiment analysis on the information. A &ldquo;rating modifier&rdquo; is than calculated, which is used to update the rating of each investment.</p>\n<p>Etho-Tron logs its rating modifiers by publishing events to Kafka. Our mission today is to receive this streaming information from Kafka, and persist it in a Cassandra table. Each Kafka event provides the following data:</p>\n<ul>\n  <li>Investment Ticker Symbol</li>\n  <li>Event Sequence Number</li>\n  <li>Ethical Category (Protects Environment, Humane to workers, etc.)</li>\n  <li>Information Source URL</li>\n  <li>Information Source Credibility Weight (an integer from 1 to 10)</li>\n  <li>Information Source Sentiment Score (an integer from -100 to 100)</li>\n</ul>\n<p>Let&rsquo;s see how we can use DSE Analytics to capture this stream of incoming data and persist it into Cassandra tables.</p>\n<h3>Our Lab Environment</h3>\n<p>Our Lab Environment includes the following:</p>\n<ul>\n  <li>DataStax Enterprise running on a three-node cluster</li>\n  <li>A running Kafka instance, configured with a topic named &ldquo;rating_modifiers&rdquo;</li>\n  <li>A simple standalone Java program that reads a .csv file and uses its data to publish events to Kafka at the rate of two events per second</li>\n  <li>A DataStax Spark program that consumes these events and writes them to a Cassandra table</li>\n</ul>\n<h3>The Program</h3>\n<p>Let&rsquo;s take a look at the code we need to consume Kafka events and write them to Cassandra.</p>\n<pre><code>package com.datastax.kellett;\n\nimport org.apache.log4j.Level;\nimport org.apache.log4j.Logger;\nimport org.apache.spark.sql.Dataset;\nimport org.apache.spark.sql.Row;\nimport org.apache.spark.sql.SparkSession;\nimport org.apache.spark.sql.streaming.OutputMode;\nimport org.apache.spark.sql.streaming.StreamingQuery;\nimport org.apache.spark.sql.streaming.StreamingQueryException;\n\npublic class Lab1 {\n\n    public static void main(String[] args) throws StreamingQueryException {\n\n        Logger.getLogger(&quot;org.apache&quot;).setLevel(Level.WARN);\n        Logger.getLogger(&quot;org.apache.spark.storage&quot;).setLevel(Level.ERROR);\n\n        // Connect to Spark\n        System.out.println(&quot;DataStax Spark Streaming Workshop Lab 1&quot;);\n        SparkSession session = SparkSession.builder()\n                //.master(&quot;local[*]&quot;)  // use for debugging if necessary\n                .appName(&quot;Spark Structured Streaming Lab1&quot;) // any name you like --- displays in Spark Management UI\n                .getOrCreate();\n\n        // Read available incoming events into a data frame\n        Dataset&lt;Row&gt; df = session.readStream()\n                .format(&quot;kafka&quot;) // If any trouble resolving &quot;kafka&quot; try long name &quot;org.apache.spark.sql.kafka010.KafkaSourceProvider&quot;\n                .option(&quot;kafka.bootstrap.servers&quot;, &quot;localhost:9092&quot;) // point to kafka instance\n                .option(&quot;subscribe&quot;, &quot;rating_modifiers&quot;) // kafka topic we want to subscribe\n                .load();\n\n        // Register a table with Spark - this will hold the incoming events from Kafka\n        df.createOrReplaceTempView(&quot;rating_modifiers_incoming&quot;);\n\n        // key, value, timestamp are the columns available to me\n        // Note that you must cast value to a string in order to get readable results\n        //\n        // This rather ungainly SQL statement is a way to parse a CSV string into individual columns.\n        // The innermost cast is necessary to make the kafka value field readable.\n        // The next-outer substring_index goes to the delimiter just past the target field, and takes everything\n        // to the left of it.\n        // The outermost substring_index takes the intermediate result and plucks out the rightmost field,\n        // which is my final target.\n        Dataset&lt;Row&gt; results = session.sql(\n                &quot;select    substring_index( cast(value as string), &#39;,&#39;, 1)                     as instrument_symbol, &quot;\n                        + &quot;substring_index( substring_index( cast(value as string), &#39;,&#39;, 2), &#39;,&#39;, -1) as event_sequence_number, &quot;\n                        + &quot;substring_index( substring_index( cast(value as string), &#39;,&#39;, 3), &#39;,&#39;, -1) as ethical_category, &quot;\n                        + &quot;substring_index( substring_index( cast(value as string), &#39;,&#39;, 4), &#39;,&#39;, -1) as source_url, &quot;\n                        + &quot;substring_index( substring_index( cast(value as string), &#39;,&#39;, 5), &#39;,&#39;, -1) as source_weight, &quot;\n                        + &quot;substring_index( substring_index( cast(value as string), &#39;,&#39;, 6), &#39;,&#39;, -1) as source_sentiment &quot;\n                        + &quot;from rating_modifiers_incoming&quot;);\n\n        // Create a sink\n        // See https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#output-sinks for standard sink types\n        // Here we use DSE&#39;s Cassandra sink\n        StreamingQuery query = results.writeStream().format(&quot;org.apache.spark.sql.cassandra&quot;)\n                .outputMode(OutputMode.Append()) // values are Complete, Update, and Append - use Append for non-aggregation queries\n                .option(&quot;keyspace&quot;, &quot;streaming_workshop&quot;)\n                .option(&quot;table&quot;, &quot;ratings_modifiers_all&quot;)\n                .option(&quot;checkpointLocation&quot;, &quot;dsefs://node0:5598/checkpoint/lab1/&quot;) // enables query restart after a failure\n                .start();\n\n        // Writes to console... use for debugging if necessary\n//        StreamingQuery query = results.writeStream().format(&quot;console&quot;)\n//                .outputMode(OutputMode.Append()) // values are Complete, Update, and Append - use Append for non-aggregation queries\n//                .option(&quot;truncate&quot;, false)\n//                .option(&quot;numRows&quot;, 30)\n//                .start();\n\n        // Rinse and repeat\n        query.awaitTermination();\n    }\n}\n</code></pre>\n<p>Now let&rsquo;s look at the Maven Build.</p>\n<pre><code>&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;\n  xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;\n  &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;\n  \n    &lt;!-- above is all boilerplate\n        \n\t\tNOTE that I generated my base pom and project directory structure with the following maven command (run from \n\t\tthe directory ABOVE where you want the project to be created... e.g. /maven):\n        mvn archetype:generate -DgroupId=com.datastax.kellett -DartifactId=datastax-spark-streaming-v2 -DarchetypeArtifactId=maven-archetype-quickstart -DinteractiveMode=false\n\n    --&gt;\t\n\n  &lt;groupId&gt;com.datastax.kellett&lt;/groupId&gt;\n  &lt;artifactId&gt;datastax-spark-streaming-v2&lt;/artifactId&gt;\n  &lt;version&gt;0.1&lt;/version&gt;\n  &lt;packaging&gt;jar&lt;/packaging&gt;\n  &lt;name&gt;datastax-spark-streaming-v2&lt;/name&gt;\n\n  &lt;properties&gt;\n    &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;\n    &lt;dse.version&gt;6.0.7&lt;/dse.version&gt;\n  &lt;/properties&gt;\n\n  &lt;dependencies&gt;\n\n    &lt;dependency&gt;\n      &lt;groupId&gt;junit&lt;/groupId&gt;\n      &lt;artifactId&gt;junit&lt;/artifactId&gt;\n      &lt;version&gt;3.8.1&lt;/version&gt;\n      &lt;scope&gt;test&lt;/scope&gt;\n    &lt;/dependency&gt;\n\n    &lt;dependency&gt;\n      &lt;groupId&gt;com.datastax.dse&lt;/groupId&gt;\n      &lt;artifactId&gt;dse-spark-dependencies&lt;/artifactId&gt;\n      &lt;version&gt;&lt;/version&gt;\n      &lt;scope&gt;provided&lt;/scope&gt;\n    &lt;/dependency&gt;\n    \n  &lt;/dependencies&gt;\n\n  &lt;repositories&gt;\n    &lt;repository&gt;\n      &lt;id&gt;DataStax-Repo&lt;/id&gt;\n      &lt;url&gt;https://repo.datastax.com/public-repos/&lt;/url&gt;\n    &lt;/repository&gt;\n  &lt;/repositories&gt;\n\n  &lt;build&gt;\n    &lt;plugins&gt;\n      &lt;plugin&gt;\n        &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;\n        &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt;\n        &lt;version&gt;3.5.1&lt;/version&gt;\n        &lt;configuration&gt;\n          &lt;source&gt;1.8&lt;/source&gt;\n          &lt;target&gt;1.8&lt;/target&gt;\n        &lt;/configuration&gt;\n      &lt;/plugin&gt;\n      &lt;plugin&gt;\n        &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;\n        &lt;artifactId&gt;maven-shade-plugin&lt;/artifactId&gt;\n        &lt;version&gt;2.4.3&lt;/version&gt;\n        &lt;executions&gt;\n          &lt;execution&gt;\n            &lt;phase&gt;package&lt;/phase&gt;\n            &lt;goals&gt;\n              &lt;goal&gt;shade&lt;/goal&gt;\n            &lt;/goals&gt;\n            &lt;configuration&gt;\n            &lt;/configuration&gt;\n          &lt;/execution&gt;\n        &lt;/executions&gt;\n      &lt;/plugin&gt;\n    &lt;/plugins&gt;\n  &lt;/build&gt;\n&lt;/project&gt;\n</code></pre>\n<h2><strong><em>OK, let&rsquo;s run it!</em></strong></h2>\n</div>"}]}},{"text":"%sh\n\n# cell 2\n\n# First, let's make sure Kafka is running\n\n# This utility asks Kafka to list its available topics.  We should see the topic \"rating_modifiers\"\n\ncd /tmp/datastax-spark-streaming-workshop/kafka_2.12-2.2.0/bin\n\n./kafka-topics.sh --list --bootstrap-server localhost:9092","user":"anonymous","dateUpdated":"2019-07-11T15:41:47+0000","config":{"colWidth":12,"editorMode":"ace/mode/sh","results":{},"enabled":true,"editorSetting":{"language":"sh","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1562849697761_1729888813","id":"20190710-161109_1919623156","dateCreated":"2019-07-11T12:54:57+0000","dateStarted":"2019-07-11T15:41:47+0000","dateFinished":"2019-07-11T15:41:49+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1482"},{"text":"%cassandra\n\n// cell 3\n\n// Now let's examine our target tables on Cassandra\n\ndescribe keyspace streaming_workshop;\n","user":"anonymous","dateUpdated":"2019-07-11T15:41:54+0000","config":{"colWidth":12,"editorMode":"ace/mode/undefined","results":{},"enabled":true,"editorSetting":{"language":"text","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1562849697762_1731043059","id":"20190710-170813_341225013","dateCreated":"2019-07-11T12:54:57+0000","dateStarted":"2019-07-11T15:41:54+0000","dateFinished":"2019-07-11T15:41:54+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1483"},{"text":"%cassandra\n\n// cell 4\n\n// Make sure our target table is empty\n\ntruncate table streaming_workshop.ratings_modifiers_all;","user":"anonymous","dateUpdated":"2019-07-11T15:42:03+0000","config":{"colWidth":12,"editorMode":"ace/mode/undefined","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1562849697762_1731043059","id":"20190710-200604_1139360496","dateCreated":"2019-07-11T12:54:57+0000","dateStarted":"2019-07-11T15:42:03+0000","dateFinished":"2019-07-11T15:42:03+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1484"},{"text":"%sh\n\n# cell 5\n\n# Make sure DSE Analytics is running\n\ndse client-tool spark leader-address","user":"anonymous","dateUpdated":"2019-07-11T15:42:12+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"sh","editOnDblClick":false},"editorMode":"ace/mode/sh"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1562850981648_591827805","id":"20190711-131621_1003199963","dateCreated":"2019-07-11T13:16:21+0000","dateStarted":"2019-07-11T15:42:12+0000","dateFinished":"2019-07-11T15:42:18+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1485"},{"text":"%md\n\ncell 6\n\n### Run the program that consumes Kafka events and writes them to Cassandra\n\nTo start the Spark job that listens for Kafka events and writes them to Cassandra, execute the following in a terminal window.\nNote that this will not work if you try to run from a shell in this notebook... Zeppelin will have a memory error.\n\n```\ncd /tmp/datastax-spark-streaming-workshop/executables\n\ndse spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.2.3,org.apache.spark:spark-streaming-kafka-0-10_2.11:2.2.3 --class com.datastax.kellett.Lab1 datastax-spark-streaming-v2-0.1.jar\n```","user":"anonymous","dateUpdated":"2019-07-11T13:44:50+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1562849697763_1730658311","id":"20190710-200950_2008260297","dateCreated":"2019-07-11T12:54:57+0000","dateStarted":"2019-07-11T13:44:50+0000","dateFinished":"2019-07-11T13:44:50+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1486","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>cell 6</p>\n<h3>Run the program that consumes Kafka events and writes them to Cassandra</h3>\n<p>To start the Spark job that listens for Kafka events and writes them to Cassandra, execute the following in a terminal window.<br/>Note that this will not work if you try to run from a shell in this notebook&hellip; Zeppelin will have a memory error.</p>\n<pre><code>cd /tmp/datastax-spark-streaming-workshop/executables\n\ndse spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.2.3,org.apache.spark:spark-streaming-kafka-0-10_2.11:2.2.3 --class com.datastax.kellett.Lab1 datastax-spark-streaming-v2-0.1.jar\n</code></pre>\n</div>"}]}},{"text":"%md\n\ncell 7\n\n### Run the program that generates Kafka events\n\nStart this program in a **separate** terminal window.\nNote that this will not work if you try to run from a shell in this notebook... Zeppelin will have a memory error.\n\nThis standalone Java program reads a file of simulated event data, and publishes to Kafka at the rate of 2 events per second.\n\n```\ncd /tmp/datastax-spark-streaming-workshop\n\njava -jar executables/kafka-event-generator-1.0-SNAPSHOT-jar-with-dependencies.jar\n```\n\n#### Halt the event generator\n\nWatch the console window that is running the event generator.  After approximately 100 events have been generated, halt the event generator with **CTRL-C**.\n\n#### Halt the event consumer\n\nIn the console window that is running the event consumer, halt the program with **CTRL-C**.","user":"anonymous","dateUpdated":"2019-07-11T13:53:16+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1562851624851_674897462","id":"20190711-132704_1461625321","dateCreated":"2019-07-11T13:27:04+0000","dateStarted":"2019-07-11T13:53:16+0000","dateFinished":"2019-07-11T13:53:16+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1487","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>cell 7</p>\n<h3>Run the program that generates Kafka events</h3>\n<p>Start this program in a <strong>separate</strong> terminal window.<br/>Note that this will not work if you try to run from a shell in this notebook&hellip; Zeppelin will have a memory error.</p>\n<p>This standalone Java program reads a file of simulated event data, and publishes to Kafka at the rate of 2 events per second.</p>\n<pre><code>cd /tmp/datastax-spark-streaming-workshop\n\njava -jar executables/kafka-event-generator-1.0-SNAPSHOT-jar-with-dependencies.jar\n</code></pre>\n<h4>Halt the event generator</h4>\n<p>Watch the console window that is running the event generator. After approximately 100 events have been generated, halt the event generator with <strong>CTRL-C</strong>.</p>\n<h4>Halt the event consumer</h4>\n<p>In the console window that is running the event consumer, halt the program with <strong>CTRL-C</strong>.</p>\n</div>"}]}},{"text":"%cassandra\n\n// cell 8\n\n// Now read from the Cassandra table to verify that events have been persisted to Cassandra.\n\nselect * from streaming_workshop.ratings_modifiers_all;\n","user":"anonymous","dateUpdated":"2019-07-11T15:44:40+0000","config":{"colWidth":12,"editorMode":"ace/mode/undefined","results":{},"enabled":true,"editorSetting":{"language":"text","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1562849697763_1730658311","id":"20190710-202010_1573903280","dateCreated":"2019-07-11T12:54:57+0000","dateStarted":"2019-07-11T15:44:40+0000","dateFinished":"2019-07-11T15:44:40+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1488"},{"text":"%cassandra\n\n// cell 9\n\n// Verify the count of records written to Cassandra\n\nselect count(*) from streaming_workshop.ratings_modifiers_all;","user":"anonymous","dateUpdated":"2019-07-11T15:44:56+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/undefined"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1562852006994_2031626064","id":"20190711-133326_184074770","dateCreated":"2019-07-11T13:33:26+0000","dateStarted":"2019-07-11T15:44:56+0000","dateFinished":"2019-07-11T15:44:56+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1489"},{"text":"%md\n\ncell 10\n\n### Summary... what just happened?\n\nIn this lab, we showed how to consume a stream of events and write them to a Cassandra table.  We had a simple 1-to-1 relationship between incoming events and Cassandra rows. This is a very useful pattern when we need to store detail-level data that comes in on a stream.  In future labs, we'll build on this simple foundation.\n","user":"anonymous","dateUpdated":"2019-07-11T15:56:58+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1562852484209_64665972","id":"20190711-134124_2125497515","dateCreated":"2019-07-11T13:41:24+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1490","dateFinished":"2019-07-11T15:56:58+0000","dateStarted":"2019-07-11T15:56:58+0000","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>cell 10</p>\n<h3>Summary&hellip; what just happened?</h3>\n<p>In this lab, we showed how to consume a stream of events and write them to a Cassandra table. We had a simple 1-to-1 relationship between incoming events and Cassandra rows. This is a very useful pattern when we need to store detail-level data that comes in on a stream. In future labs, we&rsquo;ll build on this simple foundation.</p>\n</div>"}]}},{"text":"%md\n","user":"anonymous","dateUpdated":"2019-07-11T15:56:58+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1562860618275_1512104940","id":"20190711-155658_1924356506","dateCreated":"2019-07-11T15:56:58+0000","status":"READY","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:2756"}],"name":"Lab_1_-_Streaming_Detail_Data","id":"2EFUC19KG","angularObjects":{"2EG2XZT14:shared_process":[],"2EF5HK1NN:shared_process":[],"2EH2Y3BQC:shared_process":[],"2EHXPHEV7:shared_process":[],"2EJG983Z5:shared_process":[],"2EFWH5S7W:shared_process":[],"2EH4R59NM:shared_process":[],"2EF7MGMWC:shared_process":[],"2EJ7FRTEZ:shared_process":[],"2EGAFFCTU:shared_process":[],"2EFYEYMNR:shared_process":[],"2EEN1D11F:shared_process":[],"2EHGZ5EW7:shared_process":[],"2EHVH5KGP:shared_process":[],"2EG942HA4:shared_process":[],"2EG772NG6:shared_process":[],"2EJ1K7GF1:shared_process":[],"2EGFX31HN:shared_process":[],"2EEVFMYAH:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}