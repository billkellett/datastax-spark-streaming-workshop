{"paragraphs":[{"text":"%md\n\ncell 1 \n\n# **Lab 2 - Streaming Aggregate-Level Data into Cassandra Tables**\n\nIn Lab 1 we consumed Kafka events and persisted them as individual rows in a Cassandra table.  This is useful because it provides the lowest level of detail, and is the most flexible form for any kind of analsyis.\n\nHowever, we might also want to compute aggregated results from the incoming data stream, and persist the aggregate data in Cassandra.  This is very useful if, for example, we want to enable real-time dashboards.\n\nIn this lab, we'll examine some techniques for producing and persisting aggregate data.\n\n### Example 1 - a running aggregation from \"the beginning of time\"\n\nTo get started, let's try a simple program that provides a \"continuous\" aggregation that sums up all activity since we first began capturing data.\n\nLet's take a look at the code.  The code is very similar to Lab 1, so we'll only examine the important differences.\n\nAs you might expect, the biggest difference is in the SQL itself.  We use a Group By clause to create the aggregation.\n\nEach Group consists of one ethical category for one instrument symbol.  Within each group, we aggregate:\n\n- the total number of events received\n- the \"weighted sentiment change\"\n\nWhat is \"weighted sentiment change?\" \n\nFirst, we have an input field named source_weight.  This represents our measure of the credibility of a data source, and is a number from 1 to 10. For example, the New York Times might have a source_weight of 10, while a less-reputable source might have a rating of 2.  \n\nNext, we have an input field named source_sentiment.  This is a number between -100 and 100, and it is a sentiment analysis score produced by our AI when it \"reads\" the information provided by the source.\n\nTo get a weighted rating for each individual event, we multiply the above two numbers.  To get the aggregate, we sum up all of the weighted scores from all events in the group.\n\nHere is the SQL.  As you can see, we use the same parsing method as in Lab 1.\n\n```\n        // key, value, timestamp are the columns available to me\n        // Note that you must cast value to a string in order to get readable results\n        //\n        // This rather ungainly SQL statement is a way to parse a CSV string into individual columns.\n        // The innermost cast is necessary to make the kafka value field readable.\n        // The next-outer substring_index goes to the delimiter just past the target field, and takes everything\n        // to the left of it.\n        // The outermost substring_index takes the intermediate result and plucks out the rightmost field,\n        // which is my final target.\n        // We also add cast() when necessary to get integers needed for math operations\n        Dataset<Row> results = session.sql(\n                \"select substring_index( cast(value as string), ',', 1) as instrument_symbol, \"\n                        + \"substring_index( substring_index( cast(value as string), ',', 3), ',', -1) as ethical_category, \"\n                        + \"sum(1) as total_events, \"\n                        + \"sum( \"\n                            + \"cast( substring_index( substring_index( cast(value as string), ',', 5), ',', -1) as int ) \"  // source_weight\n                            + \" * \"\n                            + \"cast( substring_index( substring_index( cast(value as string), ',', 6), ',', -1) as int ) \" // source_sentiment\n                        + \" ) as weighted_sentiment_change \"\n                        + \"from rating_modifiers_incoming \"\n                        + \"group by instrument_symbol, ethical_category\" );\n```\n\nThe only other meaningful change from Lab 1 is in the writeStream() call:\n\n```\n        StreamingQuery query = results.writeStream().format(\"org.apache.spark.sql.cassandra\")\n                .outputMode(OutputMode.Update()) // values are Complete, Update, and Append - use Complete or Update for aggregation queries\n                .option(\"keyspace\", \"streaming_workshop\")\n                .option(\"table\", \"ratings_modifiers_aggregated_complete\")\n                .option(\"checkpointLocation\", \"dsefs://node0:5598/checkpoint/lab2a/\") // enables query restart after a failure\n                .start();\n```\n\nNotice that our **OutpuMode** is now **Update()** instead of the **Append()** we used in Lab 1.  \n\nSpark Structured Streaming supports three Output Modes.  To understand the differences between them, first consider that the overall concept of streaming data in Spark Structured Streaming is an \"infinite table.\"  Think of the data stream as a database table that is constantly being updated.  That's why we can run SQL against it.  Now let's consider the three output modes:\n\n- **Append:** our conceptual model is that of an \"infinite table,\" but of course we can't actually keep an infinite amount of data in Spark memory.  Therefore, when we are dealing with detail-level data, Append is the only viable option.  This option means that a query for streaming data will give us only the rows that were appended since the last time we received data.\n- **Complete:** this mode can be used with aggregation queries, since they typically provide a manageable row count.  A running aggregation is continually computed as we receive new data from the stream.  The result set returns all rows, even if they have not changed, which is why this mode is called \"complete.\"\n- **Update:** this mode is similar to Complete, except that the result set does not include rows that have not changed since the last time we queried.\n\nSince we want to persist our aggregated results to Cassandra, Update mode is a perfect choice that fits well with Cassandra's upsert behavior.  Whenever we encounter a new row, it will be inserted into the Cassandra table.  If we encounter an updated count for an existing row, that row will be updated.  If no update has occurred, we simply leave the existing Cassandra row alone.\n\nFinally, a note on our test data.  Since we want to show easy-to-see aggregations, we have limited the test data to 10 different instrument symbols and 5 different ethical categories.  Therefore, no matter how many individual events we consume, our aggregated table will never have more than 50 rows.  We'll simply see the event counts go up in these 50 rows as more events are processed.\n\n## **_OK, let's run it!_**","dateUpdated":"2019-07-15T21:06:22+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{"dse.version":""},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>cell 1 </p>\n<h1><strong>Lab 2 - Streaming Aggregate-Level Data into Cassandra Tables</strong></h1>\n<p>In Lab 1 we consumed Kafka events and persisted them as individual rows in a Cassandra table. This is useful because it provides the lowest level of detail, and is the most flexible form for any kind of analsyis.</p>\n<p>However, we might also want to compute aggregated results from the incoming data stream, and persist the aggregate data in Cassandra. This is very useful if, for example, we want to enable real-time dashboards.</p>\n<p>In this lab, we&rsquo;ll examine some techniques for producing and persisting aggregate data.</p>\n<h3>Example 1 - a running aggregation from &ldquo;the beginning of time&rdquo;</h3>\n<p>To get started, let&rsquo;s try a simple program that provides a &ldquo;continuous&rdquo; aggregation that sums up all activity since we first began capturing data.</p>\n<p>Let&rsquo;s take a look at the code. The code is very similar to Lab 1, so we&rsquo;ll only examine the important differences.</p>\n<p>As you might expect, the biggest difference is in the SQL itself. We use a Group By clause to create the aggregation.</p>\n<p>Each Group consists of one ethical category for one instrument symbol. Within each group, we aggregate:</p>\n<ul>\n  <li>the total number of events received</li>\n  <li>the &ldquo;weighted sentiment change&rdquo;</li>\n</ul>\n<p>What is &ldquo;weighted sentiment change?&rdquo; </p>\n<p>First, we have an input field named source_weight. This represents our measure of the credibility of a data source, and is a number from 1 to 10. For example, the New York Times might have a source_weight of 10, while a less-reputable source might have a rating of 2. </p>\n<p>Next, we have an input field named source_sentiment. This is a number between -100 and 100, and it is a sentiment analysis score produced by our AI when it &ldquo;reads&rdquo; the information provided by the source.</p>\n<p>To get a weighted rating for each individual event, we multiply the above two numbers. To get the aggregate, we sum up all of the weighted scores from all events in the group.</p>\n<p>Here is the SQL. As you can see, we use the same parsing method as in Lab 1.</p>\n<pre><code>        // key, value, timestamp are the columns available to me\n        // Note that you must cast value to a string in order to get readable results\n        //\n        // This rather ungainly SQL statement is a way to parse a CSV string into individual columns.\n        // The innermost cast is necessary to make the kafka value field readable.\n        // The next-outer substring_index goes to the delimiter just past the target field, and takes everything\n        // to the left of it.\n        // The outermost substring_index takes the intermediate result and plucks out the rightmost field,\n        // which is my final target.\n        // We also add cast() when necessary to get integers needed for math operations\n        Dataset&lt;Row&gt; results = session.sql(\n                &quot;select substring_index( cast(value as string), &#39;,&#39;, 1) as instrument_symbol, &quot;\n                        + &quot;substring_index( substring_index( cast(value as string), &#39;,&#39;, 3), &#39;,&#39;, -1) as ethical_category, &quot;\n                        + &quot;sum(1) as total_events, &quot;\n                        + &quot;sum( &quot;\n                            + &quot;cast( substring_index( substring_index( cast(value as string), &#39;,&#39;, 5), &#39;,&#39;, -1) as int ) &quot;  // source_weight\n                            + &quot; * &quot;\n                            + &quot;cast( substring_index( substring_index( cast(value as string), &#39;,&#39;, 6), &#39;,&#39;, -1) as int ) &quot; // source_sentiment\n                        + &quot; ) as weighted_sentiment_change &quot;\n                        + &quot;from rating_modifiers_incoming &quot;\n                        + &quot;group by instrument_symbol, ethical_category&quot; );\n</code></pre>\n<p>The only other meaningful change from Lab 1 is in the writeStream() call:</p>\n<pre><code>        StreamingQuery query = results.writeStream().format(&quot;org.apache.spark.sql.cassandra&quot;)\n                .outputMode(OutputMode.Update()) // values are Complete, Update, and Append - use Complete or Update for aggregation queries\n                .option(&quot;keyspace&quot;, &quot;streaming_workshop&quot;)\n                .option(&quot;table&quot;, &quot;ratings_modifiers_aggregated_complete&quot;)\n                .option(&quot;checkpointLocation&quot;, &quot;dsefs://node0:5598/checkpoint/lab2a/&quot;) // enables query restart after a failure\n                .start();\n</code></pre>\n<p>Notice that our <strong>OutpuMode</strong> is now <strong>Update()</strong> instead of the <strong>Append()</strong> we used in Lab 1. </p>\n<p>Spark Structured Streaming supports three Output Modes. To understand the differences between them, first consider that the overall concept of streaming data in Spark Structured Streaming is an &ldquo;infinite table.&rdquo; Think of the data stream as a database table that is constantly being updated. That&rsquo;s why we can run SQL against it. Now let&rsquo;s consider the three output modes:</p>\n<ul>\n  <li><strong>Append:</strong> our conceptual model is that of an &ldquo;infinite table,&rdquo; but of course we can&rsquo;t actually keep an infinite amount of data in Spark memory. Therefore, when we are dealing with detail-level data, Append is the only viable option. This option means that a query for streaming data will give us only the rows that were appended since the last time we received data.</li>\n  <li><strong>Complete:</strong> this mode can be used with aggregation queries, since they typically provide a manageable row count. A running aggregation is continually computed as we receive new data from the stream. The result set returns all rows, even if they have not changed, which is why this mode is called &ldquo;complete.&rdquo;</li>\n  <li><strong>Update:</strong> this mode is similar to Complete, except that the result set does not include rows that have not changed since the last time we queried.</li>\n</ul>\n<p>Since we want to persist our aggregated results to Cassandra, Update mode is a perfect choice that fits well with Cassandra&rsquo;s upsert behavior. Whenever we encounter a new row, it will be inserted into the Cassandra table. If we encounter an updated count for an existing row, that row will be updated. If no update has occurred, we simply leave the existing Cassandra row alone.</p>\n<p>Finally, a note on our test data. Since we want to show easy-to-see aggregations, we have limited the test data to 10 different instrument symbols and 5 different ethical categories. Therefore, no matter how many individual events we consume, our aggregated table will never have more than 50 rows. We&rsquo;ll simply see the event counts go up in these 50 rows as more events are processed.</p>\n<h2><strong><em>OK, let&rsquo;s run it!</em></strong></h2>\n</div>"}]},"apps":[],"jobName":"paragraph_1563223669187_-272252763","id":"20190710-160628_1108150746","dateCreated":"2019-07-15T20:47:49+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:1493","user":"anonymous","dateFinished":"2019-07-15T21:06:24+0000","dateStarted":"2019-07-15T21:06:22+0000"},{"text":"%sh\n\n# cell 2\n\n# First, let's make sure Kafka is running\n\n# This utility asks Kafka to list its available topics.  We should see the topic \"rating_modifiers\"\n\ncd /tmp/datastax-spark-streaming-workshop/kafka_2.12-2.2.0/bin\n\n./kafka-topics.sh --list --bootstrap-server localhost:9092","dateUpdated":"2019-07-15T20:47:49+0000","config":{"colWidth":12,"editorMode":"ace/mode/sh","results":{},"enabled":true,"editorSetting":{"language":"sh","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1563223669189_-274561256","id":"20190710-161109_1919623156","dateCreated":"2019-07-15T20:47:49+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1494"},{"text":"%cassandra\n\n// cell 3\n\n// Now let's examine our target tables on Cassandra\n// ratings_modifiers_aggregated_all_time is the table of interest\n\ndescribe keyspace streaming_workshop;\n","dateUpdated":"2019-07-15T20:47:49+0000","config":{"colWidth":12,"editorMode":"ace/mode/undefined","results":{},"enabled":true,"editorSetting":{"language":"text","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1563223669190_-273407010","id":"20190710-170813_341225013","dateCreated":"2019-07-15T20:47:49+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1495"},{"text":"%cassandra\n\n// cell 4\n\n// Make sure our target table is empty\n\ntruncate table streaming_workshop.ratings_modifiers_aggregated_all_time;","dateUpdated":"2019-07-15T20:47:49+0000","config":{"colWidth":12,"editorMode":"ace/mode/undefined","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1563223669190_-273407010","id":"20190710-200604_1139360496","dateCreated":"2019-07-15T20:47:49+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1496"},{"text":"%sh\n\n# cell 5\n\n# Make sure DSE Analytics is running\n\ndse client-tool spark leader-address","dateUpdated":"2019-07-15T20:47:49+0000","config":{"colWidth":12,"editorMode":"ace/mode/sh","results":{},"enabled":true,"editorSetting":{"language":"sh","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1563223669191_-273791759","id":"20190711-131621_1003199963","dateCreated":"2019-07-15T20:47:49+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1497"},{"text":"%md\n\ncell 6\n\n### Run the program that consumes Kafka events, aggregates, and writes to Cassandra\n\nTo start the Spark job that listens for Kafka events, aggregates them, and writes them to Cassandra, execute the following in a terminal window.\nNote that this will not work if you try to run from a shell in this notebook... Zeppelin will have a memory error.\n\n```\ncd /tmp/datastax-spark-streaming-workshop/executables\n\ndse spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.2.3,org.apache.spark:spark-streaming-kafka-0-10_2.11:2.2.3 --class com.datastax.kellett.Lab2a datastax-spark-streaming-v2-0.1.jar\n```","dateUpdated":"2019-07-15T20:47:49+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>cell 6</p>\n<h3>Run the program that consumes Kafka events, aggregates, and writes to Cassandra</h3>\n<p>To start the Spark job that listens for Kafka events, aggregates them, and writes them to Cassandra, execute the following in a terminal window.<br/>Note that this will not work if you try to run from a shell in this notebook&hellip; Zeppelin will have a memory error.</p>\n<pre><code>cd /tmp/datastax-spark-streaming-workshop/executables\n\ndse spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.2.3,org.apache.spark:spark-streaming-kafka-0-10_2.11:2.2.3 --class com.datastax.kellett.Lab2a datastax-spark-streaming-v2-0.1.jar\n</code></pre>\n</div>"}]},"apps":[],"jobName":"paragraph_1563223669191_-273791759","id":"20190710-200950_2008260297","dateCreated":"2019-07-15T20:47:49+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1498"},{"text":"%md\n\ncell 7\n\n### Run the program that generates Kafka events\n\nStart this program in a **separate** terminal window.\nNote that this will not work if you try to run from a shell in this notebook... Zeppelin will have a memory error.\n\nThis standalone Java program reads a file of simulated event data, and publishes to Kafka at the rate of 2 events per second.\n\n```\ncd /tmp/datastax-spark-streaming-workshop\n\njava -jar executables/kafka-event-generator-1.0-SNAPSHOT-jar-with-dependencies.jar\n```\n\n#### Halt the event generator\n\nWatch the console window that is running the event generator.  After a few hundred events have been generated, halt the event generator with **CTRL-C**.\n\n#### Halt the event consumer\n\nIn the console window that is running the event consumer, halt the program with **CTRL-C**.","dateUpdated":"2019-07-15T20:47:49+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>cell 7</p>\n<h3>Run the program that generates Kafka events</h3>\n<p>Start this program in a <strong>separate</strong> terminal window.<br/>Note that this will not work if you try to run from a shell in this notebook&hellip; Zeppelin will have a memory error.</p>\n<p>This standalone Java program reads a file of simulated event data, and publishes to Kafka at the rate of 2 events per second.</p>\n<pre><code>cd /tmp/datastax-spark-streaming-workshop\n\njava -jar executables/kafka-event-generator-1.0-SNAPSHOT-jar-with-dependencies.jar\n</code></pre>\n<h4>Halt the event generator</h4>\n<p>Watch the console window that is running the event generator. After a few hundred events have been generated, halt the event generator with <strong>CTRL-C</strong>.</p>\n<h4>Halt the event consumer</h4>\n<p>In the console window that is running the event consumer, halt the program with <strong>CTRL-C</strong>.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1563223669192_-275715503","id":"20190711-132704_1461625321","dateCreated":"2019-07-15T20:47:49+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1499"},{"text":"%cassandra\n\n// cell 8\n\n// Now read from the Cassandra table to verify that events have been persisted to Cassandra.\n\nselect * from streaming_workshop.ratings_modifiers_aggregated_all_time where instrument_symbol = 'AXSM';\n","dateUpdated":"2019-07-15T20:47:49+0000","config":{"colWidth":12,"editorMode":"ace/mode/undefined","results":{},"enabled":true,"editorSetting":{"language":"text","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1563223669193_-276100252","id":"20190710-202010_1573903280","dateCreated":"2019-07-15T20:47:49+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1500"},{"text":"%cassandra\n\n// cell 9\n\n// Verify the count of records written to Cassandra\n\nselect count(*) from streaming_workshop.ratings_modifiers_aggregated_all_time;","dateUpdated":"2019-07-15T20:47:49+0000","config":{"colWidth":12,"editorMode":"ace/mode/undefined","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1563223669193_-276100252","id":"20190711-133326_184074770","dateCreated":"2019-07-15T20:47:49+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1501"},{"text":"%md\n\ncell 10\n\n### Aggregation using a sliding time window\n\nThe job we ran above gives us aggregate results since the beginning of the event stream.  This is interesting, but what we really want is to see the trends over specific windows of time.  \n\nNext, we'll use Spark Structured Streaming to compute the aggregation over a sliding time window.  This will help RightVest to produce alerts in response to short-term trends.\n\nLet's look at the relevant changes in our code.\n\n```\n        // In this version, we also group by the function window().  This gives us a time window aggregation,\n        // rather than a \"from-the-beginning-of-time\" aggregation.\n        // Note that you can use window as a column in your select statement.\n        // The window column would be useful as part of the primary keyif you want to persist many snapshots\n        // in a single table.\n        // However, we only want to continually update the latest data, so we persist the window as an ordinary column.\n        Dataset<Row> results = session.sql(\n                \"select window as last_update_window, \"\n                        + \"substring_index( cast(value as string), ',', 1) as instrument_symbol, \"\n                        + \"substring_index( substring_index( cast(value as string), ',', 3), ',', -1) as ethical_category, \"\n                        + \"sum(1) as total_events, \"\n                        + \"sum( \"\n                        + \"cast( substring_index( substring_index( cast(value as string), ',', 5), ',', -1) as int ) \"  // source_weight\n                        + \" * \"\n                        + \"cast( substring_index( substring_index( cast(value as string), ',', 6), ',', -1) as int ) \" // source_sentiment\n                        + \" ) as weighted_sentiment_change \"\n                        + \"from rating_modifiers_incoming \"\n                        + \"group by window( timestamp, '2 minutes', '1 minute' ), instrument_symbol, ethical_category\" );\n```\n\nFirst, notice the Group By clause on the last line above.  Window is a function provided by Spark. \n\n- The first parameter names the data we want to use as our basis for windowing.  For almost all use cases, \"timestamp\" is the appropriate value.  As we have seen, timestamp is one of the data fields provided by Kafka.  Timestamp refers to the time the event was received by Kafka.  \n- The second parameter is the time window boundary we want to use for aggregation.  You can specify seconds, minutes, hours, etc, enclosed in single quotes.  \n- The third parameter is optional.  It specifies a time boundary for a \"sliding window\" within the overall window.  If used, the overall window must be an integer multiple of the sliding window.\n\nOur values here (2 minutes and 1 minute) probably aren't useful for most use cases.  However, they will help us see the results in the short time frame of this workshop.\n\nNext, notice that we are also using window as a column in the projection.  This column is also provided by Spark.  It is a text column that shows the starting datetime and ending datetime of the window period.\n\nWe are persisting this new window column in Cassandra, but it is **not** defined as part of the primary key.  Therefore, we will still end up with a maximum table size of 50 rows, but the window column (aliased here as last_update_window) will show us the last time any particular row was updated.  If your use case requires you to persist many time-window snapshots, simply make the window column part of your primary key.  You might also want to use Cassandra TTL to expire the snapshots after a certain amount of time.\n\nThe only other important change is that we are writing to a different Cassandra table, as shown below:\n\n```\n        // Create a sink\n        // See https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#output-sinks for standard sink types\n        // Here we use DSE's Cassandra sink\n        StreamingQuery query = results.writeStream().format(\"org.apache.spark.sql.cassandra\")\n                .outputMode(OutputMode.Update()) // values are Complete, Update, and Append - use Complete or Update for aggregation queries\n                .option(\"keyspace\", \"streaming_workshop\")\n                .option(\"table\", \"ratings_modifiers_aggregated_windowed\")\n                .option(\"checkpointLocation\", \"dsefs://node0:5598/checkpoint/lab2b/\") // enables query restart after a failure\n                .start();\n```\n\nNote that we are still using Update as our output mode.\n\n## **_OK, Let's run it!_**","dateUpdated":"2019-07-15T20:47:49+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>cell 10</p>\n<h3>Aggregation using a sliding time window</h3>\n<p>The job we ran above gives us aggregate results since the beginning of the event stream. This is interesting, but what we really want is to see the trends over specific windows of time. </p>\n<p>Next, we&rsquo;ll use Spark Structured Streaming to compute the aggregation over a sliding time window. This will help RightVest to produce alerts in response to short-term trends.</p>\n<p>Let&rsquo;s look at the relevant changes in our code.</p>\n<pre><code>        // In this version, we also group by the function window().  This gives us a time window aggregation,\n        // rather than a &quot;from-the-beginning-of-time&quot; aggregation.\n        // Note that you can use window as a column in your select statement.\n        // The window column would be useful as part of the primary keyif you want to persist many snapshots\n        // in a single table.\n        // However, we only want to continually update the latest data, so we persist the window as an ordinary column.\n        Dataset&lt;Row&gt; results = session.sql(\n                &quot;select window as last_update_window, &quot;\n                        + &quot;substring_index( cast(value as string), &#39;,&#39;, 1) as instrument_symbol, &quot;\n                        + &quot;substring_index( substring_index( cast(value as string), &#39;,&#39;, 3), &#39;,&#39;, -1) as ethical_category, &quot;\n                        + &quot;sum(1) as total_events, &quot;\n                        + &quot;sum( &quot;\n                        + &quot;cast( substring_index( substring_index( cast(value as string), &#39;,&#39;, 5), &#39;,&#39;, -1) as int ) &quot;  // source_weight\n                        + &quot; * &quot;\n                        + &quot;cast( substring_index( substring_index( cast(value as string), &#39;,&#39;, 6), &#39;,&#39;, -1) as int ) &quot; // source_sentiment\n                        + &quot; ) as weighted_sentiment_change &quot;\n                        + &quot;from rating_modifiers_incoming &quot;\n                        + &quot;group by window( timestamp, &#39;2 minutes&#39;, &#39;1 minute&#39; ), instrument_symbol, ethical_category&quot; );\n</code></pre>\n<p>First, notice the Group By clause on the last line above. Window is a function provided by Spark. </p>\n<ul>\n  <li>The first parameter names the data we want to use as our basis for windowing. For almost all use cases, &ldquo;timestamp&rdquo; is the appropriate value. As we have seen, timestamp is one of the data fields provided by Kafka. Timestamp refers to the time the event was received by Kafka.</li>\n  <li>The second parameter is the time window boundary we want to use for aggregation. You can specify seconds, minutes, hours, etc, enclosed in single quotes.</li>\n  <li>The third parameter is optional. It specifies a time boundary for a &ldquo;sliding window&rdquo; within the overall window. If used, the overall window must be an integer multiple of the sliding window.</li>\n</ul>\n<p>Our values here (2 minutes and 1 minute) probably aren&rsquo;t useful for most use cases. However, they will help us see the results in the short time frame of this workshop.</p>\n<p>Next, notice that we are also using window as a column in the projection. This column is also provided by Spark. It is a text column that shows the starting datetime and ending datetime of the window period.</p>\n<p>We are persisting this new window column in Cassandra, but it is <strong>not</strong> defined as part of the primary key. Therefore, we will still end up with a maximum table size of 50 rows, but the window column (aliased here as last_update_window) will show us the last time any particular row was updated. If your use case requires you to persist many time-window snapshots, simply make the window column part of your primary key. You might also want to use Cassandra TTL to expire the snapshots after a certain amount of time.</p>\n<p>The only other important change is that we are writing to a different Cassandra table, as shown below:</p>\n<pre><code>        // Create a sink\n        // See https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#output-sinks for standard sink types\n        // Here we use DSE&#39;s Cassandra sink\n        StreamingQuery query = results.writeStream().format(&quot;org.apache.spark.sql.cassandra&quot;)\n                .outputMode(OutputMode.Update()) // values are Complete, Update, and Append - use Complete or Update for aggregation queries\n                .option(&quot;keyspace&quot;, &quot;streaming_workshop&quot;)\n                .option(&quot;table&quot;, &quot;ratings_modifiers_aggregated_windowed&quot;)\n                .option(&quot;checkpointLocation&quot;, &quot;dsefs://node0:5598/checkpoint/lab2b/&quot;) // enables query restart after a failure\n                .start();\n</code></pre>\n<p>Note that we are still using Update as our output mode.</p>\n<h2><strong><em>OK, Let&rsquo;s run it!</em></strong></h2>\n</div>"}]},"apps":[],"jobName":"paragraph_1563223669194_-274946005","id":"20190712-173923_1227926811","dateCreated":"2019-07-15T20:47:49+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1502"},{"text":"%sh\n\n# cell 11\n\n# First, let's make sure Kafka is running\n\n# This utility asks Kafka to list its available topics.  We should see the topic \"rating_modifiers\"\n\ncd /tmp/datastax-spark-streaming-workshop/kafka_2.12-2.2.0/bin\n\n./kafka-topics.sh --list --bootstrap-server localhost:9092","dateUpdated":"2019-07-15T20:47:49+0000","config":{"colWidth":12,"editorMode":"ace/mode/sh","results":{},"enabled":true,"editorSetting":{"language":"sh","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1563223669195_-275330754","id":"20190712-190941_674263316","dateCreated":"2019-07-15T20:47:49+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1503"},{"text":"%cassandra\n\n// cell 12\n\n// Now let's examine our target tables on Cassandra\n// ratings_modifiers_aggregated_windowed is the table of interest\n\ndescribe keyspace streaming_workshop;","dateUpdated":"2019-07-15T20:47:49+0000","config":{"colWidth":12,"editorMode":"ace/mode/undefined","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1563223669196_-277254499","id":"20190712-191026_1914920714","dateCreated":"2019-07-15T20:47:49+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1504"},{"text":"%cassandra\n\n// cell 13\n\n// Make sure our target table is empty\n\ntruncate table streaming_workshop.ratings_modifiers_aggregated_windowed;","dateUpdated":"2019-07-15T20:47:49+0000","config":{"colWidth":12,"editorMode":"ace/mode/undefined","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1563223669196_-277254499","id":"20190712-191117_1672119775","dateCreated":"2019-07-15T20:47:49+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1505"},{"text":"%sh\n\n# cell 14\n\n# Make sure DSE Analytics is running\n\ndse client-tool spark leader-address","dateUpdated":"2019-07-15T20:47:49+0000","config":{"colWidth":12,"editorMode":"ace/mode/sh","results":{},"enabled":true,"editorSetting":{"language":"sh","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1563223669197_-277639248","id":"20190712-191256_1133965658","dateCreated":"2019-07-15T20:47:49+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1506"},{"text":"%md\n\ncell 15\n\n### Run the program that consumes Kafka events, aggregates over time windows, and writes to Cassandra\n\nTo start the Spark job that listens for Kafka events, aggregates them over time windows, and writes them to Cassandra, execute the following in a terminal window.\nNote that this will not work if you try to run from a shell in this notebook... Zeppelin will have a memory error.\n\n```\ncd /tmp/datastax-spark-streaming-workshop/executables\n\ndse spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.2.3,org.apache.spark:spark-streaming-kafka-0-10_2.11:2.2.3 --class com.datastax.kellett.Lab2b datastax-spark-streaming-v2-0.1.jar\n```","dateUpdated":"2019-07-15T20:47:49+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>cell 15</p>\n<h3>Run the program that consumes Kafka events, aggregates over time windows, and writes to Cassandra</h3>\n<p>To start the Spark job that listens for Kafka events, aggregates them over time windows, and writes them to Cassandra, execute the following in a terminal window.<br/>Note that this will not work if you try to run from a shell in this notebook&hellip; Zeppelin will have a memory error.</p>\n<pre><code>cd /tmp/datastax-spark-streaming-workshop/executables\n\ndse spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.2.3,org.apache.spark:spark-streaming-kafka-0-10_2.11:2.2.3 --class com.datastax.kellett.Lab2b datastax-spark-streaming-v2-0.1.jar\n</code></pre>\n</div>"}]},"apps":[],"jobName":"paragraph_1563223669197_-277639248","id":"20190712-191330_1990858642","dateCreated":"2019-07-15T20:47:49+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1507"},{"text":"%md\n\ncell 16\n\n### Run the program that generates Kafka events\n\nStart this program in a **separate** terminal window.\nNote that this will not work if you try to run from a shell in this notebook... Zeppelin will have a memory error.\n\nThis standalone Java program reads a file of simulated event data, and publishes to Kafka at the rate of 2 events per second.\n\n```\ncd /tmp/datastax-spark-streaming-workshop\n\njava -jar executables/kafka-event-generator-1.0-SNAPSHOT-jar-with-dependencies.jar\n```\n\nLet the generator job run until it produces about a thousand events.  That way, we'll get to see a few different time windows in the output.\n\n#### Halt the event generator\n\nWatch the console window that is running the event generator.  After a few hundred events have been generated, halt the event generator with **CTRL-C**.\n\n#### Halt the event consumer\n\nIn the console window that is running the event consumer, halt the program with **CTRL-C**.","dateUpdated":"2019-07-15T20:47:49+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>cell 16</p>\n<h3>Run the program that generates Kafka events</h3>\n<p>Start this program in a <strong>separate</strong> terminal window.<br/>Note that this will not work if you try to run from a shell in this notebook&hellip; Zeppelin will have a memory error.</p>\n<p>This standalone Java program reads a file of simulated event data, and publishes to Kafka at the rate of 2 events per second.</p>\n<pre><code>cd /tmp/datastax-spark-streaming-workshop\n\njava -jar executables/kafka-event-generator-1.0-SNAPSHOT-jar-with-dependencies.jar\n</code></pre>\n<p>Let the generator job run until it produces about a thousand events. That way, we&rsquo;ll get to see a few different time windows in the output.</p>\n<h4>Halt the event generator</h4>\n<p>Watch the console window that is running the event generator. After a few hundred events have been generated, halt the event generator with <strong>CTRL-C</strong>.</p>\n<h4>Halt the event consumer</h4>\n<p>In the console window that is running the event consumer, halt the program with <strong>CTRL-C</strong>.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1563223669198_-276485001","id":"20190712-191554_1563993020","dateCreated":"2019-07-15T20:47:49+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1508"},{"text":"%cassandra\n\n// cell 17\n\n// Now read from the Cassandra table to verify that events have been persisted to Cassandra.\n\n// Examine the output.  How many time windows do you see?  Do you see how the window slides?\n\nselect * from streaming_workshop.ratings_modifiers_aggregated_windowed where instrument_symbol = 'AXSM';\n","dateUpdated":"2019-07-15T20:47:49+0000","config":{"colWidth":12,"editorMode":"ace/mode/undefined","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1563223669199_-276869750","id":"20190712-191806_673264477","dateCreated":"2019-07-15T20:47:49+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1509"},{"text":"%cassandra\n\n// cell 9\n\n// Verify the count of records written to Cassandra\n\nselect count(*) from streaming_workshop.ratings_modifiers_aggregated_windowed;","dateUpdated":"2019-07-15T20:47:49+0000","config":{"colWidth":12,"editorMode":"ace/mode/undefined","results":{},"enabled":true,"editorSetting":{"language":"text","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1563223669199_-276869750","id":"20190712-180013_2083774352","dateCreated":"2019-07-15T20:47:49+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1510"},{"text":"%md\n\ncell 19\n\n### Summary... what just happened?\n\nIn this lab, we showed how to aggregate data from an incoming stream, and persist the aggregate data to Cassandra.  We then saw how to create aggregations over sliding time windows.  This is a very useful approach for building dashboards or other tools that are concerned with trends in the data.\n","dateUpdated":"2019-07-15T20:47:49+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>cell 19</p>\n<h3>Summary&hellip; what just happened?</h3>\n<p>In this lab, we showed how to aggregate data from an incoming stream, and persist the aggregate data to Cassandra. We then saw how to create aggregations over sliding time windows. This is a very useful approach for building dashboards or other tools that are concerned with trends in the data.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1563223669200_-266481529","id":"20190711-134124_2125497515","dateCreated":"2019-07-15T20:47:49+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1511"},{"text":"%md\n","dateUpdated":"2019-07-15T20:47:49+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","results":{},"enabled":true,"editorSetting":{"language":"markdown","editOnDblClick":true}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1563223669201_-266866278","id":"20190711-155658_1924356506","dateCreated":"2019-07-15T20:47:49+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1512"}],"name":"Lab_2_-_Streaming_Aggregations","id":"2EHQ8WH64","angularObjects":{"2EFGUS7TG:shared_process":[],"2EHRERAAW:shared_process":[],"2EFJ4JGE1:shared_process":[],"2EH8JURNG:shared_process":[],"2EF91ZBM5:shared_process":[],"2EFZTY7TU:shared_process":[],"2EFU9WAN9:shared_process":[],"2EHJWFTCQ:shared_process":[],"2EG9H25MP:shared_process":[],"2EHEP3CGQ:shared_process":[],"2EGP2S67K:shared_process":[],"2EH1XSJBU:shared_process":[],"2EHU6YHXF:shared_process":[],"2EGXFS5NG:shared_process":[],"2EHY48G2S:shared_process":[],"2EJUT5SWH:shared_process":[],"2EHGW69DX:shared_process":[],"2EF6MHWWH:shared_process":[],"2EGKQ7ZXB:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}