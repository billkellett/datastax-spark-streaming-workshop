{"paragraphs":[{"text":"%md\n\ncell 1 \n\n# **Lab 2 - Streaming Aggregate-Level Data into Cassandra Tables**\n\nIn Lab 1 we consumed Kafka events and persisted them as individual rows in a Cassandra table.  This is useful because it provides the lowest level of detail, and is the most flexible form for any kind of analsyis.\n\nHowever, we might also want to compute aggregated results from the incoming data stream, and the aggregate data in Cassandra.  This is very useful if, for example, we want to enable real-time dashboards.\n\nIn this lab, we'll examine some techniques for producing and persisting aggregate data.\n\n### Example 1 - a running aggregation from \"the beginning of time\"\n\nTo get started, let's try a simple program that provides a \"continuous\" aggregation that sums up all activity since we first began capturing data.\n\nLet's take a look at the code.  The code is very similar to Lab 1, so we'll only examine the important differences.\n\nAs you might expect, the biggest difference is in the SQL itself.  We use a Group By clause to create the aggregation.\n\nEach Group consists of one ethical category for one instrument symbol.  Within each group, we aggregate:\n\n- the total number of events received\n- the \"weighted sentiment change\"\n\nWhat is \"weighted sentiment change?\" \n\nFirst, we have an input field named source_weight.  This represents our measure of the credibility of a data source, and is a number from 1 to 10. For example, the New York Times might have a source_weight of 10, while a less-reputable source might have a rating of 2.  \n\nNext, we have an input field named source_sentiment.  This is a number between -100 and 100, and it is a sentiment analysis score produced by our AI when it \"reads\" the information provided by the source.\n\nTo get a weighted rating for each individual event, we multiply the above two numbers.  To get the aggregate, we sum up all of the weighted scores from all events in the group.\n\nHere is the SQL.  As you can see, we use the same parsing method as in Lab 1.\n\n```\n        // key, value, timestamp are the columns available to me\n        // Note that you must cast value to a string in order to get readable results\n        //\n        // This rather ungainly SQL statement is a way to parse a CSV string into individual columns.\n        // The innermost cast is necessary to make the kafka value field readable.\n        // The next-outer substring_index goes to the delimiter just past the target field, and takes everything\n        // to the left of it.\n        // The outermost substring_index takes the intermediate result and plucks out the rightmost field,\n        // which is my final target.\n        // We also add cast() when necessary to get integers needed for math operations\n        Dataset<Row> results = session.sql(\n                \"select substring_index( cast(value as string), ',', 1) as instrument_symbol, \"\n                        + \"substring_index( substring_index( cast(value as string), ',', 3), ',', -1) as ethical_category, \"\n                        + \"sum(1) as total_events, \"\n                        + \"sum( \"\n                            + \"cast( substring_index( substring_index( cast(value as string), ',', 5), ',', -1) as int ) \"  // source_weight\n                            + \" * \"\n                            + \"cast( substring_index( substring_index( cast(value as string), ',', 6), ',', -1) as int ) \" // source_sentiment\n                        + \" ) as weighted_sentiment_change \"\n                        + \"from rating_modifiers_incoming \"\n                        + \"group by instrument_symbol, ethical_category\" );\n```\n\nThe only other meaningful change from Lab 1 is in the writeStream() call:\n\n```\n        StreamingQuery query = results.writeStream().format(\"org.apache.spark.sql.cassandra\")\n                .outputMode(OutputMode.Update()) // values are Complete, Update, and Append - use Complete or Update for aggregation queries\n                .option(\"keyspace\", \"streaming_workshop\")\n                .option(\"table\", \"ratings_modifiers_aggregated_complete\")\n                .option(\"checkpointLocation\", \"dsefs://node0:5598/checkpoint/lab2a/\") // enables query restart after a failure\n                .start();\n```\n\nNotice that our **OutpuMode** is now **Update()** instead of the **Append()** we used in Lab 1.  \n\nSpark Structured Streaming supports three Output Modes.  To understand the differences between them, first consider that the overall concept of streaming data in Spark Structured Streaming is an \"infinite table.\"  Think of the data stream as a database table that is constantly being updated.  That's why we can run SQL against it.  Now let's consider the three output modes:\n\n- **Append:** our conceptual model is that of an \"infinite table,\" but of course we can't actually keep an infinite amount of data in Spark memory.  Therefore, when we are dealing with detail-level data, Append is the only viable option.  This option means that a query for streaming data will give us only the rows that were appended since the last time we received data.\n- **Complete:** this mode can be used with aggregation queries, since they typically provide a manageable row count.  A running aggregation is continually computed as we receive new data from the stream.  The result set returns all rows, even if they have not changed, which is why this mode is called \"complete.\"\n- **Update:** this mode is similar to Complete, except that the result set does not include rows that have not changed since the last time we queried.\n\nSince we want to persist our aggregated results to Cassandra, Update mode is a perfect choice that fits well with Cassandra's upsert behavior.  Whenever we encounter a new row, it will be inserted into the Cassandra table.  If we encounter an updated count for an existing row, that row will be updated.  If no update has occurred, we simply leave the existing Cassandra row alone.\n\nFinally, a note on our test data.  Since we want to show easy-to-see aggregations, we have limited the test data to 10 different instrument symbols and 5 different ethical categories.  Therefore, no matter how many individual events we consume, our aggregated table will never have more than 50 rows.  We'll simply see the event counts go up in these 50 rows as more events are processed.\n\n## **_OK, let's run it!_**","dateUpdated":"2019-07-11T19:59:39+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{"dse.version":""},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>cell 1 </p>\n<h1><strong>Lab 2 - Streaming Aggregate-Level Data into Cassandra Tables</strong></h1>\n<p>In Lab 1 we consumed Kafka events and persisted them as individual rows in a Cassandra table. This is useful because it provides the lowest level of detail, and is the most flexible form for any kind of analsyis.</p>\n<p>However, we might also want to compute aggregated results from the incoming data stream, and the aggregate data in Cassandra. This is very useful if, for example, we want to enable real-time dashboards.</p>\n<p>In this lab, we&rsquo;ll examine some techniques for producing and persisting aggregate data.</p>\n<h3>Example 1 - a running aggregation from &ldquo;the beginning of time&rdquo;</h3>\n<p>To get started, let&rsquo;s try a simple program that provides a &ldquo;continuous&rdquo; aggregation that sums up all activity since we first began capturing data.</p>\n<p>Let&rsquo;s take a look at the code. The code is very similar to Lab 1, so we&rsquo;ll only examine the important differences.</p>\n<p>As you might expect, the biggest difference is in the SQL itself. We use a Group By clause to create the aggregation.</p>\n<p>Each Group consists of one ethical category for one instrument symbol. Within each group, we aggregate:</p>\n<ul>\n  <li>the total number of events received</li>\n  <li>the &ldquo;weighted sentiment change&rdquo;</li>\n</ul>\n<p>What is &ldquo;weighted sentiment change?&rdquo; </p>\n<p>First, we have an input field named source_weight. This represents our measure of the credibility of a data source, and is a number from 1 to 10. For example, the New York Times might have a source_weight of 10, while a less-reputable source might have a rating of 2. </p>\n<p>Next, we have an input field named source_sentiment. This is a number between -100 and 100, and it is a sentiment analysis score produced by our AI when it &ldquo;reads&rdquo; the information provided by the source.</p>\n<p>To get a weighted rating for each individual event, we multiply the above two numbers. To get the aggregate, we sum up all of the weighted scores from all events in the group.</p>\n<p>Here is the SQL. As you can see, we use the same parsing method as in Lab 1.</p>\n<pre><code>        // key, value, timestamp are the columns available to me\n        // Note that you must cast value to a string in order to get readable results\n        //\n        // This rather ungainly SQL statement is a way to parse a CSV string into individual columns.\n        // The innermost cast is necessary to make the kafka value field readable.\n        // The next-outer substring_index goes to the delimiter just past the target field, and takes everything\n        // to the left of it.\n        // The outermost substring_index takes the intermediate result and plucks out the rightmost field,\n        // which is my final target.\n        // We also add cast() when necessary to get integers needed for math operations\n        Dataset&lt;Row&gt; results = session.sql(\n                &quot;select substring_index( cast(value as string), &#39;,&#39;, 1) as instrument_symbol, &quot;\n                        + &quot;substring_index( substring_index( cast(value as string), &#39;,&#39;, 3), &#39;,&#39;, -1) as ethical_category, &quot;\n                        + &quot;sum(1) as total_events, &quot;\n                        + &quot;sum( &quot;\n                            + &quot;cast( substring_index( substring_index( cast(value as string), &#39;,&#39;, 5), &#39;,&#39;, -1) as int ) &quot;  // source_weight\n                            + &quot; * &quot;\n                            + &quot;cast( substring_index( substring_index( cast(value as string), &#39;,&#39;, 6), &#39;,&#39;, -1) as int ) &quot; // source_sentiment\n                        + &quot; ) as weighted_sentiment_change &quot;\n                        + &quot;from rating_modifiers_incoming &quot;\n                        + &quot;group by instrument_symbol, ethical_category&quot; );\n</code></pre>\n<p>The only other meaningful change from Lab 1 is in the writeStream() call:</p>\n<pre><code>        StreamingQuery query = results.writeStream().format(&quot;org.apache.spark.sql.cassandra&quot;)\n                .outputMode(OutputMode.Update()) // values are Complete, Update, and Append - use Complete or Update for aggregation queries\n                .option(&quot;keyspace&quot;, &quot;streaming_workshop&quot;)\n                .option(&quot;table&quot;, &quot;ratings_modifiers_aggregated_complete&quot;)\n                .option(&quot;checkpointLocation&quot;, &quot;dsefs://node0:5598/checkpoint/lab2a/&quot;) // enables query restart after a failure\n                .start();\n</code></pre>\n<p>Notice that our <strong>OutpuMode</strong> is now <strong>Update()</strong> instead of the <strong>Append()</strong> we used in Lab 1. </p>\n<p>Spark Structured Streaming supports three Output Modes. To understand the differences between them, first consider that the overall concept of streaming data in Spark Structured Streaming is an &ldquo;infinite table.&rdquo; Think of the data stream as a database table that is constantly being updated. That&rsquo;s why we can run SQL against it. Now let&rsquo;s consider the three output modes:</p>\n<ul>\n  <li><strong>Append:</strong> our conceptual model is that of an &ldquo;infinite table,&rdquo; but of course we can&rsquo;t actually keep an infinite amount of data in Spark memory. Therefore, when we are dealing with detail-level data, Append is the only viable option. This option means that a query for streaming data will give us only the rows that were appended since the last time we received data.</li>\n  <li><strong>Complete:</strong> this mode can be used with aggregation queries, since they typically provide a manageable row count. A running aggregation is continually computed as we receive new data from the stream. The result set returns all rows, even if they have not changed, which is why this mode is called &ldquo;complete.&rdquo;</li>\n  <li><strong>Update:</strong> this mode is similar to Complete, except that the result set does not include rows that have not changed since the last time we queried.</li>\n</ul>\n<p>Since we want to persist our aggregated results to Cassandra, Update mode is a perfect choice that fits well with Cassandra&rsquo;s upsert behavior. Whenever we encounter a new row, it will be inserted into the Cassandra table. If we encounter an updated count for an existing row, that row will be updated. If no update has occurred, we simply leave the existing Cassandra row alone.</p>\n<p>Finally, a note on our test data. Since we want to show easy-to-see aggregations, we have limited the test data to 10 different instrument symbols and 5 different ethical categories. Therefore, no matter how many individual events we consume, our aggregated table will never have more than 50 rows. We&rsquo;ll simply see the event counts go up in these 50 rows as more events are processed.</p>\n<h2><strong><em>OK, let&rsquo;s run it!</em></strong></h2>\n</div>"}]},"apps":[],"jobName":"paragraph_1562869252357_404610582","id":"20190710-160628_1108150746","dateCreated":"2019-07-11T18:20:52+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:4510","user":"anonymous","dateFinished":"2019-07-11T19:59:39+0000","dateStarted":"2019-07-11T19:59:39+0000"},{"text":"%sh\n\n# cell 2\n\n# First, let's make sure Kafka is running\n\n# This utility asks Kafka to list its available topics.  We should see the topic \"rating_modifiers\"\n\ncd /tmp/datastax-spark-streaming-workshop/kafka_2.12-2.2.0/bin\n\n./kafka-topics.sh --list --bootstrap-server localhost:9092","dateUpdated":"2019-07-11T20:00:03+0000","config":{"colWidth":12,"editorMode":"ace/mode/sh","results":{},"enabled":true,"editorSetting":{"language":"sh","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1562869252357_404610582","id":"20190710-161109_1919623156","dateCreated":"2019-07-11T18:20:52+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4511","user":"anonymous","dateFinished":"2019-07-11T20:00:04+0000","dateStarted":"2019-07-11T20:00:03+0000"},{"text":"%cassandra\n\n// cell 3\n\n// Now let's examine our target tables on Cassandra\n\ndescribe keyspace streaming_workshop;\n","dateUpdated":"2019-07-11T20:00:10+0000","config":{"colWidth":12,"editorMode":"ace/mode/undefined","results":{},"enabled":true,"editorSetting":{"language":"text","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1562869252358_405764829","id":"20190710-170813_341225013","dateCreated":"2019-07-11T18:20:52+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4512","user":"anonymous","dateFinished":"2019-07-11T20:00:10+0000","dateStarted":"2019-07-11T20:00:10+0000"},{"text":"%cassandra\n\n// cell 4\n\n// Make sure our target table is empty\n\ntruncate table streaming_workshop.ratings_modifiers_aggregated_complete;","dateUpdated":"2019-07-11T20:14:02+0000","config":{"colWidth":12,"editorMode":"ace/mode/undefined","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1562869252359_405380080","id":"20190710-200604_1139360496","dateCreated":"2019-07-11T18:20:52+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4513","user":"anonymous","dateFinished":"2019-07-11T20:14:02+0000","dateStarted":"2019-07-11T20:14:02+0000"},{"text":"%sh\n\n# cell 5\n\n# Make sure DSE Analytics is running\n\ndse client-tool spark leader-address","dateUpdated":"2019-07-11T20:00:32+0000","config":{"colWidth":12,"editorMode":"ace/mode/sh","results":{},"enabled":true,"editorSetting":{"language":"sh","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1562869252359_405380080","id":"20190711-131621_1003199963","dateCreated":"2019-07-11T18:20:52+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4514","user":"anonymous","dateFinished":"2019-07-11T20:00:39+0000","dateStarted":"2019-07-11T20:00:32+0000"},{"text":"%md\n\ncell 6\n\n### Run the program that consumes Kafka events, aggregates, and writes to Cassandra\n\nTo start the Spark job that listens for Kafka events, aggregates them, and writes them to Cassandra, execute the following in a terminal window.\nNote that this will not work if you try to run from a shell in this notebook... Zeppelin will have a memory error.\n\n```\ncd /tmp/datastax-spark-streaming-workshop/executables\n\ndse spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.2.3,org.apache.spark:spark-streaming-kafka-0-10_2.11:2.2.3 --class com.datastax.kellett.Lab2a datastax-spark-streaming-v2-0.1.jar\n```","dateUpdated":"2019-07-11T20:08:35+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>cell 6</p>\n<h3>Run the program that consumes Kafka events, aggregates, and writes to Cassandra</h3>\n<p>To start the Spark job that listens for Kafka events, aggregates them, and writes them to Cassandra, execute the following in a terminal window.<br/>Note that this will not work if you try to run from a shell in this notebook&hellip; Zeppelin will have a memory error.</p>\n<pre><code>cd /tmp/datastax-spark-streaming-workshop/executables\n\ndse spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.2.3,org.apache.spark:spark-streaming-kafka-0-10_2.11:2.2.3 --class com.datastax.kellett.Lab2a datastax-spark-streaming-v2-0.1.jar\n</code></pre>\n</div>"}]},"apps":[],"jobName":"paragraph_1562869252359_405380080","id":"20190710-200950_2008260297","dateCreated":"2019-07-11T18:20:52+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:4515","user":"anonymous","dateFinished":"2019-07-11T20:08:35+0000","dateStarted":"2019-07-11T20:08:35+0000"},{"text":"%md\n\ncell 7\n\n### Run the program that generates Kafka events\n\nStart this program in a **separate** terminal window.\nNote that this will not work if you try to run from a shell in this notebook... Zeppelin will have a memory error.\n\nThis standalone Java program reads a file of simulated event data, and publishes to Kafka at the rate of 2 events per second.\n\n```\ncd /tmp/datastax-spark-streaming-workshop\n\njava -jar executables/kafka-event-generator-1.0-SNAPSHOT-jar-with-dependencies.jar\n```\n\n#### Halt the event generator\n\nWatch the console window that is running the event generator.  After a few hundred events have been generated, halt the event generator with **CTRL-C**.\n\n#### Halt the event consumer\n\nIn the console window that is running the event consumer, halt the program with **CTRL-C**.","dateUpdated":"2019-07-11T20:21:34+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>cell 7</p>\n<h3>Run the program that generates Kafka events</h3>\n<p>Start this program in a <strong>separate</strong> terminal window.<br/>Note that this will not work if you try to run from a shell in this notebook&hellip; Zeppelin will have a memory error.</p>\n<p>This standalone Java program reads a file of simulated event data, and publishes to Kafka at the rate of 2 events per second.</p>\n<pre><code>cd /tmp/datastax-spark-streaming-workshop\n\njava -jar executables/kafka-event-generator-1.0-SNAPSHOT-jar-with-dependencies.jar\n</code></pre>\n<h4>Halt the event generator</h4>\n<p>Watch the console window that is running the event generator. After a few hundred events have been generated, halt the event generator with <strong>CTRL-C</strong>.</p>\n<h4>Halt the event consumer</h4>\n<p>In the console window that is running the event consumer, halt the program with <strong>CTRL-C</strong>.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1562869252360_403456336","id":"20190711-132704_1461625321","dateCreated":"2019-07-11T18:20:52+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:4516","user":"anonymous","dateFinished":"2019-07-11T20:21:34+0000","dateStarted":"2019-07-11T20:21:34+0000"},{"text":"%cassandra\n\n// cell 8\n\n// Now read from the Cassandra table to verify that events have been persisted to Cassandra.\n\nselect * from streaming_workshop.ratings_modifiers_aggregated_complete where instrument_symbol = 'AXSM';\n","dateUpdated":"2019-07-11T20:18:13+0000","config":{"colWidth":12,"editorMode":"ace/mode/undefined","results":{},"enabled":true,"editorSetting":{"language":"text","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1562869252360_403456336","id":"20190710-202010_1573903280","dateCreated":"2019-07-11T18:20:52+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4517","user":"anonymous","dateFinished":"2019-07-11T20:18:13+0000","dateStarted":"2019-07-11T20:18:13+0000"},{"text":"%cassandra\n\n// cell 9\n\n// Verify the count of records written to Cassandra\n\nselect count(*) from streaming_workshop.ratings_modifiers_aggregated_complete;","dateUpdated":"2019-07-11T20:18:26+0000","config":{"colWidth":12,"editorMode":"ace/mode/undefined","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1562869252360_403456336","id":"20190711-133326_184074770","dateCreated":"2019-07-11T18:20:52+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4518","user":"anonymous","dateFinished":"2019-07-11T20:18:26+0000","dateStarted":"2019-07-11T20:18:26+0000"},{"text":"%md\n\ncell 10\n\n### Summary... what just happened?\n\nIn this lab, we showed how to aggregate data from an incoming stream, and persist the aggregate data to Cassandra.  This is a very useful approach for building dashboards or other tools that are concerned with trends in the data.\n","dateUpdated":"2019-07-11T20:19:49+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>cell 10</p>\n<h3>Summary&hellip; what just happened?</h3>\n<p>In this lab, we showed how to aggregate data from an incoming stream, and persist the aggregate data to Cassandra. This is a very useful approach for building dashboards or other tools that are concerned with trends in the data.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1562869252361_403071587","id":"20190711-134124_2125497515","dateCreated":"2019-07-11T18:20:52+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:4519","user":"anonymous","dateFinished":"2019-07-11T20:19:50+0000","dateStarted":"2019-07-11T20:19:49+0000"},{"text":"%md\n","dateUpdated":"2019-07-11T18:20:52+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1562869252361_403071587","id":"20190711-155658_1924356506","dateCreated":"2019-07-11T18:20:52+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4520"}],"name":"Lab_2_-_Streaming_Aggregations","id":"2EEVJMASD","angularObjects":{"2EG2XZT14:shared_process":[],"2EF5HK1NN:shared_process":[],"2EH2Y3BQC:shared_process":[],"2EHXPHEV7:shared_process":[],"2EJG983Z5:shared_process":[],"2EFWH5S7W:shared_process":[],"2EH4R59NM:shared_process":[],"2EF7MGMWC:shared_process":[],"2EJ7FRTEZ:shared_process":[],"2EGAFFCTU:shared_process":[],"2EFYEYMNR:shared_process":[],"2EEN1D11F:shared_process":[],"2EHGZ5EW7:shared_process":[],"2EHVH5KGP:shared_process":[],"2EG942HA4:shared_process":[],"2EG772NG6:shared_process":[],"2EJ1K7GF1:shared_process":[],"2EGFX31HN:shared_process":[],"2EEVFMYAH:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}