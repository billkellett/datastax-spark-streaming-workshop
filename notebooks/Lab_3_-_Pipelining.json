{"paragraphs":[{"text":"%md\n\ncell 1 \n\n# **Lab 3 - Using Spark and Kafka to develop a Pipeline workflow**\n\nA single Kafka event stream might be useful to different applications in different contexts.  A powerful technique for enabling such use cases is Pipelining.  Pipelining means that we consume an event stream and then re-publish some or all of those events to different Kafka event streams.  Other applications then subscribe to these event streams and do whatever is needed to satisfy their own use cases.\n\nRightVest has a use case that can be satisfied by Pipelining.  If an incoming event has a weighted sentiment score of 400 or greater, we consider this very good news, and we want to pass it to an application that provides special alerting for exceptional good-news situations.  Similarly, if an incoming event has a weighted sentiment score of -400 or less, we want to pass it to an alerting application for exceptionally bad news.  We'll do this by publishing exceptional events to their own Kafka topics.  Other Spark programs will read these topics and process the exceptional events according to their own needs.\n\n### 1 - Creating the Pipeline\n\nTo get started, let's create a simple program that reads all incoming events, and writes exceptional events to the good_news or bad_news Kafka topics.\n\nExceptional events are those that have a weighted sentiment score greater than 400 (good news) or less than -400 (bad news).  To calculate the weighted score, we multiply an event's:\n\n- source credibility rating (an integer from 1-10)\nTIMES\n- sentiment score (an integer from -100 to 100)\n\nLet's look at the code for this \"piper\" application:\n\n```\npackage com.datastax.kellett;\n\nimport org.apache.log4j.Level;\nimport org.apache.log4j.Logger;\nimport org.apache.spark.sql.Dataset;\nimport org.apache.spark.sql.Row;\nimport org.apache.spark.sql.SparkSession;\nimport org.apache.spark.sql.streaming.StreamingQuery;\nimport org.apache.spark.sql.streaming.StreamingQueryException;\n\npublic class Lab3Piper {\n\n    public static void main(String[] args) throws StreamingQueryException {\n\n        Logger.getLogger(\"org.apache\").setLevel(Level.WARN);\n        Logger.getLogger(\"org.apache.spark.storage\").setLevel(Level.ERROR);\n\n        // Connect to Spark\n        System.out.println(\"DataStax Spark Streaming Workshop Lab 3 Piper\");\n        SparkSession session = SparkSession.builder()\n                //.master(\"local[*]\")  // use for debugging if necessary\n                .appName(\"Spark Structured Streaming Lab3Piper\") // any name you like --- displays in Spark Management UI\n                .getOrCreate();\n\n        // Read available incoming events into a data frame\n        Dataset<Row> df = session.readStream()\n                .format(\"kafka\") // If any trouble resolving \"kafka\" try long name \"org.apache.spark.sql.kafka010.KafkaSourceProvider\"\n                .option(\"kafka.bootstrap.servers\", \"node0:9092\") // point to kafka instance\n                .option(\"subscribe\", \"rating_modifiers\") // kafka topic we want to subscribe\n                .load();\n\n        // Register a table with Spark - this will hold the incoming events from Kafka\n        df.createOrReplaceTempView(\"rating_modifiers_incoming\");\n\n        // key, value, timestamp are the columns available to me\n        // Note that you must cast value to a string in order to get readable results\n        //\n        // This rather ungainly SQL statement is a way to parse a CSV string into individual columns.\n        // The innermost cast is necessary to make the kafka value field readable.\n        // The next-outer substring_index goes to the delimiter just past the target field, and takes everything\n        // to the left of it.\n        // The outermost substring_index takes the intermediate result and plucks out the rightmost field,\n        // which is my final target.\n        Dataset<Row> results = session.sql(\n                \"select    substring_index( cast(value as string), ',', 1)                     as instrument_symbol, \"\n                        + \"substring_index( substring_index( cast(value as string), ',', 2), ',', -1) as event_sequence_number, \"\n                        + \"substring_index( substring_index( cast(value as string), ',', 3), ',', -1) as ethical_category, \"\n                        + \"substring_index( substring_index( cast(value as string), ',', 4), ',', -1) as source_url, \"\n                        + \"substring_index( substring_index( cast(value as string), ',', 5), ',', -1) as source_weight, \"\n                        + \"substring_index( substring_index( cast(value as string), ',', 6), ',', -1) as source_sentiment \"\n                        + \"from rating_modifiers_incoming\");\n\n        results.createOrReplaceTempView(\"all_events\");\n        Dataset<Row> goodNews = session.sql(\n                \"select event_sequence_number as key, \"\n                        + \"concat_ws(',', instrument_symbol, event_sequence_number, ethical_category, source_url, source_weight, source_sentiment) as value \"\n                        + \"from all_events where source_weight * source_sentiment > 399\");\n\n        Dataset<Row> badNews = session.sql(\n                \"select event_sequence_number as key, \"\n                        + \"concat_ws(',', instrument_symbol, event_sequence_number, ethical_category, source_url, source_weight, source_sentiment) as value \"\n                        + \"from all_events where source_weight * source_sentiment < -399\");\n\n        // Create sinks\n        // See https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#output-sinks for standard sink types\n        // Here we use a Kafka sink\n        StreamingQuery goodNewsQuery = goodNews.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")\n                                                .writeStream()\n                                                .format(\"kafka\")\n                                                .option(\"kafka.bootstrap.servers\", \"node0:9092\")\n                                                .option(\"topic\", \"good_news\")\n                                                .option(\"checkpointLocation\", \"dsefs://node0:5598/checkpoint/lab3pipergoodnews/\") // enables query restart after a failure\n                                                .start();\n\n        StreamingQuery badNewsQuery = badNews.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")\n                .writeStream()\n                .format(\"kafka\")\n                .option(\"kafka.bootstrap.servers\", \"node0:9092\")\n                .option(\"topic\", \"bad_news\")\n                .option(\"checkpointLocation\", \"dsefs://node0:5598/checkpoint/lab3piperbadnews/\") // enables query restart after a failure\n                .start();\n\n        // Rinse and repeat\n        goodNewsQuery.awaitTermination();\n        badNewsQuery.awaitTermination();\n    }\n}\n\n```\n\nIn the code above, we parse the incoming event stream so we can deal with individual columns.  We then view this parsed stream as a table named all_events.  We query all_events for exceptional good-news and bad-news events, storing the results in Dataframes called goodNews and badNews.  \n\nTo put goodNews and badNews back into a Kafka-friendly format, we use the concat_ws (concatenate with separator) function, using a comma as the separator.  Then we can publish them to the appropriate Kafka topics.\n\n### 2 - Processing the Good News events\n\nThe code that listens for good_news events is very similar to the code we built for Lab 1.  It reads the event stream, parses each event to a set of columns, and writes to a Cassandra table.  Here's the code:\n\n```\npackage com.datastax.kellett;\n\nimport org.apache.log4j.Level;\nimport org.apache.log4j.Logger;\nimport org.apache.spark.sql.Dataset;\nimport org.apache.spark.sql.Row;\nimport org.apache.spark.sql.SparkSession;\nimport org.apache.spark.sql.streaming.OutputMode;\nimport org.apache.spark.sql.streaming.StreamingQuery;\nimport org.apache.spark.sql.streaming.StreamingQueryException;\n\npublic class Lab3GoodNews {\n\n    public static void main(String[] args) throws StreamingQueryException {\n\n        Logger.getLogger(\"org.apache\").setLevel(Level.WARN);\n        Logger.getLogger(\"org.apache.spark.storage\").setLevel(Level.ERROR);\n\n        // Connect to Spark\n        System.out.println(\"DataStax Spark Streaming Workshop Lab 3 Good News\");\n        SparkSession session = SparkSession.builder()\n                //.master(\"local[*]\")  // use for debugging if necessary\n                .appName(\"Spark Structured Streaming Lab3GoodNews\") // any name you like --- displays in Spark Management UI\n                .getOrCreate();\n\n        // Read available incoming events into a data frame\n        Dataset<Row> df = session.readStream()\n                .format(\"kafka\") // If any trouble resolving \"kafka\" try long name \"org.apache.spark.sql.kafka010.KafkaSourceProvider\"\n                .option(\"kafka.bootstrap.servers\", \"node0:9092\") // point to kafka instance\n                .option(\"subscribe\", \"good_news\") // kafka topic we want to subscribe\n                .load();\n\n        // Register a table with Spark - this will hold the incoming events from Kafka\n        df.createOrReplaceTempView(\"good_news_incoming\");\n\n        // key, value, timestamp are the columns available to me\n        // Note that you must cast value to a string in order to get readable results\n        //\n        // This rather ungainly SQL statement is a way to parse a CSV string into individual columns.\n        // The innermost cast is necessary to make the kafka value field readable.\n        // The next-outer substring_index goes to the delimiter just past the target field, and takes everything\n        // to the left of it.\n        // The outermost substring_index takes the intermediate result and plucks out the rightmost field,\n        // which is my final target.\n        Dataset<Row> results = session.sql(\n                \"select    substring_index( cast(value as string), ',', 1)                     as instrument_symbol, \"\n                        + \"substring_index( substring_index( cast(value as string), ',', 2), ',', -1) as event_sequence_number, \"\n                        + \"substring_index( substring_index( cast(value as string), ',', 3), ',', -1) as ethical_category, \"\n                        + \"substring_index( substring_index( cast(value as string), ',', 4), ',', -1) as source_url, \"\n                        + \"substring_index( substring_index( cast(value as string), ',', 5), ',', -1) as source_weight, \"\n                        + \"substring_index( substring_index( cast(value as string), ',', 6), ',', -1) as source_sentiment \"\n                        + \"from good_news_incoming\");\n\n        // Create a sink\n        // See https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#output-sinks for standard sink types\n        // Here we use DSE's Cassandra sink\n        StreamingQuery query = results.writeStream().format(\"org.apache.spark.sql.cassandra\")\n                .outputMode(OutputMode.Append()) // values are Complete, Update, and Append - use Append for non-aggregation queries\n                .option(\"keyspace\", \"streaming_workshop\")\n                .option(\"table\", \"ratings_modifiers_good_news\")\n                .option(\"checkpointLocation\", \"dsefs://node0:5598/checkpoint/lab3goodnews/\") // enables query restart after a failure\n                .start();\n\n        // Rinse and repeat\n        query.awaitTermination();\n    }\n}\n\n```\n\n### 3 - Processing the Bad News events\n\nThe Bad News code is almost identical to the Good News code, except it reads from a different Kafka topic, and writes to a different Cassandra table.\n\n```\npackage com.datastax.kellett;\n\nimport org.apache.log4j.Level;\nimport org.apache.log4j.Logger;\nimport org.apache.spark.sql.Dataset;\nimport org.apache.spark.sql.Row;\nimport org.apache.spark.sql.SparkSession;\nimport org.apache.spark.sql.streaming.OutputMode;\nimport org.apache.spark.sql.streaming.StreamingQuery;\nimport org.apache.spark.sql.streaming.StreamingQueryException;\n\npublic class Lab3BadNews {\n\n    public static void main(String[] args) throws StreamingQueryException {\n\n        Logger.getLogger(\"org.apache\").setLevel(Level.WARN);\n        Logger.getLogger(\"org.apache.spark.storage\").setLevel(Level.ERROR);\n\n        // Connect to Spark\n        System.out.println(\"DataStax Spark Streaming Workshop Lab 3 Bad News\");\n        SparkSession session = SparkSession.builder()\n                //.master(\"local[*]\")  // use for debugging if necessary\n                .appName(\"Spark Structured Streaming Lab3BadNews\") // any name you like --- displays in Spark Management UI\n                .getOrCreate();\n\n        // Read available incoming events into a data frame\n        Dataset<Row> df = session.readStream()\n                .format(\"kafka\") // If any trouble resolving \"kafka\" try long name \"org.apache.spark.sql.kafka010.KafkaSourceProvider\"\n                .option(\"kafka.bootstrap.servers\", \"node0:9092\") // point to kafka instance\n                .option(\"subscribe\", \"bad_news\") // kafka topic we want to subscribe\n                .load();\n\n        // Register a table with Spark - this will hold the incoming events from Kafka\n        df.createOrReplaceTempView(\"bad_news_incoming\");\n\n        // key, value, timestamp are the columns available to me\n        // Note that you must cast value to a string in order to get readable results\n        //\n        // This rather ungainly SQL statement is a way to parse a CSV string into individual columns.\n        // The innermost cast is necessary to make the kafka value field readable.\n        // The next-outer substring_index goes to the delimiter just past the target field, and takes everything\n        // to the left of it.\n        // The outermost substring_index takes the intermediate result and plucks out the rightmost field,\n        // which is my final target.\n        Dataset<Row> results = session.sql(\n                \"select    substring_index( cast(value as string), ',', 1)                     as instrument_symbol, \"\n                        + \"substring_index( substring_index( cast(value as string), ',', 2), ',', -1) as event_sequence_number, \"\n                        + \"substring_index( substring_index( cast(value as string), ',', 3), ',', -1) as ethical_category, \"\n                        + \"substring_index( substring_index( cast(value as string), ',', 4), ',', -1) as source_url, \"\n                        + \"substring_index( substring_index( cast(value as string), ',', 5), ',', -1) as source_weight, \"\n                        + \"substring_index( substring_index( cast(value as string), ',', 6), ',', -1) as source_sentiment \"\n                        + \"from bad_news_incoming\");\n\n        // Create a sink\n        // See https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#output-sinks for standard sink types\n        // Here we use DSE's Cassandra sink\n        StreamingQuery query = results.writeStream().format(\"org.apache.spark.sql.cassandra\")\n                .outputMode(OutputMode.Append()) // values are Complete, Update, and Append - use Append for non-aggregation queries\n                .option(\"keyspace\", \"streaming_workshop\")\n                .option(\"table\", \"ratings_modifiers_bad_news\")\n                .option(\"checkpointLocation\", \"dsefs://node0:5598/checkpoint/lab3badnews/\") // enables query restart after a failure\n                .start();\n\n        // Rinse and repeat\n        query.awaitTermination();\n    }\n}\n\n```\n\n## **_OK, let's run it!_**","user":"anonymous","dateUpdated":"2019-07-14T20:02:19+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{"dse.version":""},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>cell 1 </p>\n<h1><strong>Lab 3 - Using Spark and Kafka to develop a Pipeline workflow</strong></h1>\n<p>A single Kafka event stream might be useful to different applications in different contexts. A powerful technique for enabling such use cases is Pipelining. Pipelining means that we consume an event stream and then re-publish some or all of those events to different Kafka event streams. Other applications then subscribe to these event streams and do whatever is needed to satisfy their own use cases.</p>\n<p>RightVest has a use case that can be satisfied by Pipelining. If an incoming event has a weighted sentiment score of 400 or greater, we consider this very good news, and we want to pass it to an application that provides special alerting for exceptional good-news situations. Similarly, if an incoming event has a weighted sentiment score of -400 or less, we want to pass it to an alerting application for exceptionally bad news. We&rsquo;ll do this by publishing exceptional events to their own Kafka topics. Other Spark programs will read these topics and process the exceptional events according to their own needs.</p>\n<h3>1 - Creating the Pipeline</h3>\n<p>To get started, let&rsquo;s create a simple program that reads all incoming events, and writes exceptional events to the good_news or bad_news Kafka topics.</p>\n<p>Exceptional events are those that have a weighted sentiment score greater than 400 (good news) or less than -400 (bad news). To calculate the weighted score, we multiply an event&rsquo;s:</p>\n<ul>\n  <li>source credibility rating (an integer from 1-10)<br/>TIMES</li>\n  <li>sentiment score (an integer from -100 to 100)</li>\n</ul>\n<p>Let&rsquo;s look at the code for this &ldquo;piper&rdquo; application:</p>\n<pre><code>package com.datastax.kellett;\n\nimport org.apache.log4j.Level;\nimport org.apache.log4j.Logger;\nimport org.apache.spark.sql.Dataset;\nimport org.apache.spark.sql.Row;\nimport org.apache.spark.sql.SparkSession;\nimport org.apache.spark.sql.streaming.StreamingQuery;\nimport org.apache.spark.sql.streaming.StreamingQueryException;\n\npublic class Lab3Piper {\n\n    public static void main(String[] args) throws StreamingQueryException {\n\n        Logger.getLogger(&quot;org.apache&quot;).setLevel(Level.WARN);\n        Logger.getLogger(&quot;org.apache.spark.storage&quot;).setLevel(Level.ERROR);\n\n        // Connect to Spark\n        System.out.println(&quot;DataStax Spark Streaming Workshop Lab 3 Piper&quot;);\n        SparkSession session = SparkSession.builder()\n                //.master(&quot;local[*]&quot;)  // use for debugging if necessary\n                .appName(&quot;Spark Structured Streaming Lab3Piper&quot;) // any name you like --- displays in Spark Management UI\n                .getOrCreate();\n\n        // Read available incoming events into a data frame\n        Dataset&lt;Row&gt; df = session.readStream()\n                .format(&quot;kafka&quot;) // If any trouble resolving &quot;kafka&quot; try long name &quot;org.apache.spark.sql.kafka010.KafkaSourceProvider&quot;\n                .option(&quot;kafka.bootstrap.servers&quot;, &quot;node0:9092&quot;) // point to kafka instance\n                .option(&quot;subscribe&quot;, &quot;rating_modifiers&quot;) // kafka topic we want to subscribe\n                .load();\n\n        // Register a table with Spark - this will hold the incoming events from Kafka\n        df.createOrReplaceTempView(&quot;rating_modifiers_incoming&quot;);\n\n        // key, value, timestamp are the columns available to me\n        // Note that you must cast value to a string in order to get readable results\n        //\n        // This rather ungainly SQL statement is a way to parse a CSV string into individual columns.\n        // The innermost cast is necessary to make the kafka value field readable.\n        // The next-outer substring_index goes to the delimiter just past the target field, and takes everything\n        // to the left of it.\n        // The outermost substring_index takes the intermediate result and plucks out the rightmost field,\n        // which is my final target.\n        Dataset&lt;Row&gt; results = session.sql(\n                &quot;select    substring_index( cast(value as string), &#39;,&#39;, 1)                     as instrument_symbol, &quot;\n                        + &quot;substring_index( substring_index( cast(value as string), &#39;,&#39;, 2), &#39;,&#39;, -1) as event_sequence_number, &quot;\n                        + &quot;substring_index( substring_index( cast(value as string), &#39;,&#39;, 3), &#39;,&#39;, -1) as ethical_category, &quot;\n                        + &quot;substring_index( substring_index( cast(value as string), &#39;,&#39;, 4), &#39;,&#39;, -1) as source_url, &quot;\n                        + &quot;substring_index( substring_index( cast(value as string), &#39;,&#39;, 5), &#39;,&#39;, -1) as source_weight, &quot;\n                        + &quot;substring_index( substring_index( cast(value as string), &#39;,&#39;, 6), &#39;,&#39;, -1) as source_sentiment &quot;\n                        + &quot;from rating_modifiers_incoming&quot;);\n\n        results.createOrReplaceTempView(&quot;all_events&quot;);\n        Dataset&lt;Row&gt; goodNews = session.sql(\n                &quot;select event_sequence_number as key, &quot;\n                        + &quot;concat_ws(&#39;,&#39;, instrument_symbol, event_sequence_number, ethical_category, source_url, source_weight, source_sentiment) as value &quot;\n                        + &quot;from all_events where source_weight * source_sentiment &gt; 399&quot;);\n\n        Dataset&lt;Row&gt; badNews = session.sql(\n                &quot;select event_sequence_number as key, &quot;\n                        + &quot;concat_ws(&#39;,&#39;, instrument_symbol, event_sequence_number, ethical_category, source_url, source_weight, source_sentiment) as value &quot;\n                        + &quot;from all_events where source_weight * source_sentiment &lt; -399&quot;);\n\n        // Create sinks\n        // See https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#output-sinks for standard sink types\n        // Here we use a Kafka sink\n        StreamingQuery goodNewsQuery = goodNews.selectExpr(&quot;CAST(key AS STRING)&quot;, &quot;CAST(value AS STRING)&quot;)\n                                                .writeStream()\n                                                .format(&quot;kafka&quot;)\n                                                .option(&quot;kafka.bootstrap.servers&quot;, &quot;node0:9092&quot;)\n                                                .option(&quot;topic&quot;, &quot;good_news&quot;)\n                                                .option(&quot;checkpointLocation&quot;, &quot;dsefs://node0:5598/checkpoint/lab3pipergoodnews/&quot;) // enables query restart after a failure\n                                                .start();\n\n        StreamingQuery badNewsQuery = badNews.selectExpr(&quot;CAST(key AS STRING)&quot;, &quot;CAST(value AS STRING)&quot;)\n                .writeStream()\n                .format(&quot;kafka&quot;)\n                .option(&quot;kafka.bootstrap.servers&quot;, &quot;node0:9092&quot;)\n                .option(&quot;topic&quot;, &quot;bad_news&quot;)\n                .option(&quot;checkpointLocation&quot;, &quot;dsefs://node0:5598/checkpoint/lab3piperbadnews/&quot;) // enables query restart after a failure\n                .start();\n\n        // Rinse and repeat\n        goodNewsQuery.awaitTermination();\n        badNewsQuery.awaitTermination();\n    }\n}\n\n</code></pre>\n<p>In the code above, we parse the incoming event stream so we can deal with individual columns. We then view this parsed stream as a table named all_events. We query all_events for exceptional good-news and bad-news events, storing the results in Dataframes called goodNews and badNews. </p>\n<p>To put goodNews and badNews back into a Kafka-friendly format, we use the concat_ws (concatenate with separator) function, using a comma as the separator. Then we can publish them to the appropriate Kafka topics.</p>\n<h3>2 - Processing the Good News events</h3>\n<p>The code that listens for good_news events is very similar to the code we built for Lab 1. It reads the event stream, parses each event to a set of columns, and writes to a Cassandra table. Here&rsquo;s the code:</p>\n<pre><code>package com.datastax.kellett;\n\nimport org.apache.log4j.Level;\nimport org.apache.log4j.Logger;\nimport org.apache.spark.sql.Dataset;\nimport org.apache.spark.sql.Row;\nimport org.apache.spark.sql.SparkSession;\nimport org.apache.spark.sql.streaming.OutputMode;\nimport org.apache.spark.sql.streaming.StreamingQuery;\nimport org.apache.spark.sql.streaming.StreamingQueryException;\n\npublic class Lab3GoodNews {\n\n    public static void main(String[] args) throws StreamingQueryException {\n\n        Logger.getLogger(&quot;org.apache&quot;).setLevel(Level.WARN);\n        Logger.getLogger(&quot;org.apache.spark.storage&quot;).setLevel(Level.ERROR);\n\n        // Connect to Spark\n        System.out.println(&quot;DataStax Spark Streaming Workshop Lab 3 Good News&quot;);\n        SparkSession session = SparkSession.builder()\n                //.master(&quot;local[*]&quot;)  // use for debugging if necessary\n                .appName(&quot;Spark Structured Streaming Lab3GoodNews&quot;) // any name you like --- displays in Spark Management UI\n                .getOrCreate();\n\n        // Read available incoming events into a data frame\n        Dataset&lt;Row&gt; df = session.readStream()\n                .format(&quot;kafka&quot;) // If any trouble resolving &quot;kafka&quot; try long name &quot;org.apache.spark.sql.kafka010.KafkaSourceProvider&quot;\n                .option(&quot;kafka.bootstrap.servers&quot;, &quot;node0:9092&quot;) // point to kafka instance\n                .option(&quot;subscribe&quot;, &quot;good_news&quot;) // kafka topic we want to subscribe\n                .load();\n\n        // Register a table with Spark - this will hold the incoming events from Kafka\n        df.createOrReplaceTempView(&quot;good_news_incoming&quot;);\n\n        // key, value, timestamp are the columns available to me\n        // Note that you must cast value to a string in order to get readable results\n        //\n        // This rather ungainly SQL statement is a way to parse a CSV string into individual columns.\n        // The innermost cast is necessary to make the kafka value field readable.\n        // The next-outer substring_index goes to the delimiter just past the target field, and takes everything\n        // to the left of it.\n        // The outermost substring_index takes the intermediate result and plucks out the rightmost field,\n        // which is my final target.\n        Dataset&lt;Row&gt; results = session.sql(\n                &quot;select    substring_index( cast(value as string), &#39;,&#39;, 1)                     as instrument_symbol, &quot;\n                        + &quot;substring_index( substring_index( cast(value as string), &#39;,&#39;, 2), &#39;,&#39;, -1) as event_sequence_number, &quot;\n                        + &quot;substring_index( substring_index( cast(value as string), &#39;,&#39;, 3), &#39;,&#39;, -1) as ethical_category, &quot;\n                        + &quot;substring_index( substring_index( cast(value as string), &#39;,&#39;, 4), &#39;,&#39;, -1) as source_url, &quot;\n                        + &quot;substring_index( substring_index( cast(value as string), &#39;,&#39;, 5), &#39;,&#39;, -1) as source_weight, &quot;\n                        + &quot;substring_index( substring_index( cast(value as string), &#39;,&#39;, 6), &#39;,&#39;, -1) as source_sentiment &quot;\n                        + &quot;from good_news_incoming&quot;);\n\n        // Create a sink\n        // See https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#output-sinks for standard sink types\n        // Here we use DSE&#39;s Cassandra sink\n        StreamingQuery query = results.writeStream().format(&quot;org.apache.spark.sql.cassandra&quot;)\n                .outputMode(OutputMode.Append()) // values are Complete, Update, and Append - use Append for non-aggregation queries\n                .option(&quot;keyspace&quot;, &quot;streaming_workshop&quot;)\n                .option(&quot;table&quot;, &quot;ratings_modifiers_good_news&quot;)\n                .option(&quot;checkpointLocation&quot;, &quot;dsefs://node0:5598/checkpoint/lab3goodnews/&quot;) // enables query restart after a failure\n                .start();\n\n        // Rinse and repeat\n        query.awaitTermination();\n    }\n}\n\n</code></pre>\n<h3>3 - Processing the Bad News events</h3>\n<p>The Bad News code is almost identical to the Good News code, except it reads from a different Kafka topic, and writes to a different Cassandra table.</p>\n<pre><code>package com.datastax.kellett;\n\nimport org.apache.log4j.Level;\nimport org.apache.log4j.Logger;\nimport org.apache.spark.sql.Dataset;\nimport org.apache.spark.sql.Row;\nimport org.apache.spark.sql.SparkSession;\nimport org.apache.spark.sql.streaming.OutputMode;\nimport org.apache.spark.sql.streaming.StreamingQuery;\nimport org.apache.spark.sql.streaming.StreamingQueryException;\n\npublic class Lab3BadNews {\n\n    public static void main(String[] args) throws StreamingQueryException {\n\n        Logger.getLogger(&quot;org.apache&quot;).setLevel(Level.WARN);\n        Logger.getLogger(&quot;org.apache.spark.storage&quot;).setLevel(Level.ERROR);\n\n        // Connect to Spark\n        System.out.println(&quot;DataStax Spark Streaming Workshop Lab 3 Bad News&quot;);\n        SparkSession session = SparkSession.builder()\n                //.master(&quot;local[*]&quot;)  // use for debugging if necessary\n                .appName(&quot;Spark Structured Streaming Lab3BadNews&quot;) // any name you like --- displays in Spark Management UI\n                .getOrCreate();\n\n        // Read available incoming events into a data frame\n        Dataset&lt;Row&gt; df = session.readStream()\n                .format(&quot;kafka&quot;) // If any trouble resolving &quot;kafka&quot; try long name &quot;org.apache.spark.sql.kafka010.KafkaSourceProvider&quot;\n                .option(&quot;kafka.bootstrap.servers&quot;, &quot;node0:9092&quot;) // point to kafka instance\n                .option(&quot;subscribe&quot;, &quot;bad_news&quot;) // kafka topic we want to subscribe\n                .load();\n\n        // Register a table with Spark - this will hold the incoming events from Kafka\n        df.createOrReplaceTempView(&quot;bad_news_incoming&quot;);\n\n        // key, value, timestamp are the columns available to me\n        // Note that you must cast value to a string in order to get readable results\n        //\n        // This rather ungainly SQL statement is a way to parse a CSV string into individual columns.\n        // The innermost cast is necessary to make the kafka value field readable.\n        // The next-outer substring_index goes to the delimiter just past the target field, and takes everything\n        // to the left of it.\n        // The outermost substring_index takes the intermediate result and plucks out the rightmost field,\n        // which is my final target.\n        Dataset&lt;Row&gt; results = session.sql(\n                &quot;select    substring_index( cast(value as string), &#39;,&#39;, 1)                     as instrument_symbol, &quot;\n                        + &quot;substring_index( substring_index( cast(value as string), &#39;,&#39;, 2), &#39;,&#39;, -1) as event_sequence_number, &quot;\n                        + &quot;substring_index( substring_index( cast(value as string), &#39;,&#39;, 3), &#39;,&#39;, -1) as ethical_category, &quot;\n                        + &quot;substring_index( substring_index( cast(value as string), &#39;,&#39;, 4), &#39;,&#39;, -1) as source_url, &quot;\n                        + &quot;substring_index( substring_index( cast(value as string), &#39;,&#39;, 5), &#39;,&#39;, -1) as source_weight, &quot;\n                        + &quot;substring_index( substring_index( cast(value as string), &#39;,&#39;, 6), &#39;,&#39;, -1) as source_sentiment &quot;\n                        + &quot;from bad_news_incoming&quot;);\n\n        // Create a sink\n        // See https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#output-sinks for standard sink types\n        // Here we use DSE&#39;s Cassandra sink\n        StreamingQuery query = results.writeStream().format(&quot;org.apache.spark.sql.cassandra&quot;)\n                .outputMode(OutputMode.Append()) // values are Complete, Update, and Append - use Append for non-aggregation queries\n                .option(&quot;keyspace&quot;, &quot;streaming_workshop&quot;)\n                .option(&quot;table&quot;, &quot;ratings_modifiers_bad_news&quot;)\n                .option(&quot;checkpointLocation&quot;, &quot;dsefs://node0:5598/checkpoint/lab3badnews/&quot;) // enables query restart after a failure\n                .start();\n\n        // Rinse and repeat\n        query.awaitTermination();\n    }\n}\n\n</code></pre>\n<h2><strong><em>OK, let&rsquo;s run it!</em></strong></h2>\n</div>"}]},"apps":[],"jobName":"paragraph_1563123580047_40506890","id":"20190710-160628_1108150746","dateCreated":"2019-07-14T16:59:40+0000","dateStarted":"2019-07-14T19:33:39+0000","dateFinished":"2019-07-14T19:33:39+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:8797"},{"text":"%sh\n\n# cell 2\n\n# First, let's make sure Kafka is running\n\n# This utility asks Kafka to list its available topics.  We should see the topic \"rating_modifiers\"\n\ncd /tmp/datastax-spark-streaming-workshop/kafka_2.12-2.2.0/bin\n\n./kafka-topics.sh --list --bootstrap-server localhost:9092","user":"anonymous","dateUpdated":"2019-07-14T19:35:23+0000","config":{"colWidth":12,"editorMode":"ace/mode/sh","results":{},"enabled":true,"editorSetting":{"language":"sh","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1563123580048_50895110","id":"20190710-161109_1919623156","dateCreated":"2019-07-14T16:59:40+0000","dateStarted":"2019-07-14T19:35:23+0000","dateFinished":"2019-07-14T19:35:25+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:8798","errorMessage":""},{"text":"%cassandra\n\n// cell 3\n\n// Now let's examine our target tables on Cassandra\n// ratings_modifier_good_news and ratings_modifiers_bad_news are the tables of interest\n\ndescribe keyspace streaming_workshop;\n","user":"anonymous","dateUpdated":"2019-07-14T19:36:41+0000","config":{"colWidth":12,"editorMode":"ace/mode/undefined","results":{},"enabled":true,"editorSetting":{"language":"text","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1563123580049_50510361","id":"20190710-170813_341225013","dateCreated":"2019-07-14T16:59:40+0000","dateStarted":"2019-07-14T19:36:41+0000","dateFinished":"2019-07-14T19:36:49+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:8799","errorMessage":""},{"text":"%cassandra\n\n// cell 4\n\n// Make sure our target tables are empty\n\ntruncate table streaming_workshop.ratings_modifiers_good_news;\ntruncate table streaming_workshop.ratings_modifiers_bad_news;","user":"anonymous","dateUpdated":"2019-07-14T19:36:55+0000","config":{"colWidth":12,"editorMode":"ace/mode/undefined","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1563123580051_51279859","id":"20190710-200604_1139360496","dateCreated":"2019-07-14T16:59:40+0000","dateStarted":"2019-07-14T19:36:55+0000","dateFinished":"2019-07-14T19:36:55+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:8800","errorMessage":""},{"text":"%sh\n\n# cell 5\n\n# Make sure DSE Analytics is running\n\ndse client-tool spark leader-address","user":"anonymous","dateUpdated":"2019-07-14T19:37:02+0000","config":{"colWidth":12,"editorMode":"ace/mode/sh","results":{},"enabled":true,"editorSetting":{"language":"sh","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1563123580052_49356115","id":"20190711-131621_1003199963","dateCreated":"2019-07-14T16:59:40+0000","dateStarted":"2019-07-14T19:37:02+0000","dateFinished":"2019-07-14T19:37:08+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:8801","errorMessage":""},{"text":"%md\n\ncell 6\n\n### Run the program that consumes all Kafka events and creates the pipeline\n\nTo start the Spark job that listens for all Kafka events, filters out extreme good-news and bad-news events, and publishes these back to Kafka, execute the following in a terminal window.\nNote that this will not work if you try to run from a shell in this notebook... Zeppelin will have a memory error.\n\nNote that we are limiting the cores in this execution to 2 per worker node.  This ensures we don't run out of processing power on our small test cluster.\n\n```\ncd /tmp/datastax-spark-streaming-workshop/executables\n\ndse spark-submit --total-executor-cores 2 --packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.2.3,org.apache.spark:spark-streaming-kafka-0-10_2.11:2.2.3 --class com.datastax.kellett.Lab3Piper datastax-spark-streaming-v2-0.1.jar\n```","dateUpdated":"2019-07-14T19:47:37+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>cell 6</p>\n<h3>Run the program that consumes all Kafka events and creates the pipeline</h3>\n<p>To start the Spark job that listens for all Kafka events, filters out extreme good-news and bad-news events, and publishes these back to Kafka, execute the following in a terminal window.<br/>Note that this will not work if you try to run from a shell in this notebook&hellip; Zeppelin will have a memory error.</p>\n<p>Note that we are limiting the cores in this execution to 2 per worker node. This ensures we don&rsquo;t run out of processing power on our small test cluster.</p>\n<pre><code>cd /tmp/datastax-spark-streaming-workshop/executables\n\ndse spark-submit --total-executor-cores 2 --packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.2.3,org.apache.spark:spark-streaming-kafka-0-10_2.11:2.2.3 --class com.datastax.kellett.Lab3Piper datastax-spark-streaming-v2-0.1.jar\n</code></pre>\n</div>"}]},"apps":[],"jobName":"paragraph_1563123580053_48971366","id":"20190710-200950_2008260297","dateCreated":"2019-07-14T16:59:40+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:8802","user":"anonymous","dateFinished":"2019-07-14T19:47:37+0000","dateStarted":"2019-07-14T19:47:37+0000"},{"text":"%md\n\ncell 7\n\n### Run the program that consumes good-news events and writes them to Cassandra\n\nTo start the Spark job that listens for Kafka good-news events and writes them to Cassandra, execute the following in a terminal window.\nNote that this will not work if you try to run from a shell in this notebook... Zeppelin will have a memory error.\n\nNote that we are limiting the cores in this execution to 2 per worker node. This ensures we don’t run out of processing power on our small test cluster.\n\n```\ncd /tmp/datastax-spark-streaming-workshop/executables\n\ndse spark-submit --total-executor-cores 2 --packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.2.3,org.apache.spark:spark-streaming-kafka-0-10_2.11:2.2.3 --class com.datastax.kellett.Lab3GoodNews datastax-spark-streaming-v2-0.1.jar\n```","dateUpdated":"2019-07-14T19:50:00+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>cell 7</p>\n<h3>Run the program that consumes good-news events and writes them to Cassandra</h3>\n<p>To start the Spark job that listens for Kafka good-news events and writes them to Cassandra, execute the following in a terminal window.<br/>Note that this will not work if you try to run from a shell in this notebook&hellip; Zeppelin will have a memory error.</p>\n<p>Note that we are limiting the cores in this execution to 2 per worker node. This ensures we don’t run out of processing power on our small test cluster.</p>\n<pre><code>cd /tmp/datastax-spark-streaming-workshop/executables\n\ndse spark-submit --total-executor-cores 2 --packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.2.3,org.apache.spark:spark-streaming-kafka-0-10_2.11:2.2.3 --class com.datastax.kellett.Lab3GoodNews datastax-spark-streaming-v2-0.1.jar\n</code></pre>\n</div>"}]},"apps":[],"jobName":"paragraph_1563123580054_50125612","id":"20190713-173250_1319839929","dateCreated":"2019-07-14T16:59:40+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:8803","user":"anonymous","dateFinished":"2019-07-14T19:50:00+0000","dateStarted":"2019-07-14T19:50:00+0000"},{"text":"%md\n\ncell 8\n\n### Run the program that consumes bad-news events and writes them to Cassandra\n\nTo start the Spark job that listens for Kafka bad-news events and writes them to Cassandra, execute the following in a terminal window.\nNote that this will not work if you try to run from a shell in this notebook... Zeppelin will have a memory error.\n\nNote that we are limiting the cores in this execution to 2 per worker node. This ensures we don’t run out of processing power on our small test cluster.\n\n```\ncd /tmp/datastax-spark-streaming-workshop/executables\n\ndse spark-submit --total-executor-cores 2 --packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.2.3,org.apache.spark:spark-streaming-kafka-0-10_2.11:2.2.3 --class com.datastax.kellett.Lab3BadNews datastax-spark-streaming-v2-0.1.jar\n```","dateUpdated":"2019-07-14T19:50:34+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>cell 8</p>\n<h3>Run the program that consumes bad-news events and writes them to Cassandra</h3>\n<p>To start the Spark job that listens for Kafka bad-news events and writes them to Cassandra, execute the following in a terminal window.<br/>Note that this will not work if you try to run from a shell in this notebook&hellip; Zeppelin will have a memory error.</p>\n<p>Note that we are limiting the cores in this execution to 2 per worker node. This ensures we don’t run out of processing power on our small test cluster.</p>\n<pre><code>cd /tmp/datastax-spark-streaming-workshop/executables\n\ndse spark-submit --total-executor-cores 2 --packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.2.3,org.apache.spark:spark-streaming-kafka-0-10_2.11:2.2.3 --class com.datastax.kellett.Lab3BadNews datastax-spark-streaming-v2-0.1.jar\n</code></pre>\n</div>"}]},"apps":[],"jobName":"paragraph_1563123580055_49740864","id":"20190713-184153_2146465621","dateCreated":"2019-07-14T16:59:40+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:8804","user":"anonymous","dateFinished":"2019-07-14T19:50:34+0000","dateStarted":"2019-07-14T19:50:34+0000"},{"text":"%md\n\ncell 9\n\n### Run the program that generates Kafka events\n\nStart this program in a **separate** terminal window.\nNote that this will not work if you try to run from a shell in this notebook... Zeppelin will have a memory error.\n\nThis standalone Java program reads a file of simulated event data, and publishes to Kafka at the rate of 2 events per second.\n\n```\ncd /tmp/datastax-spark-streaming-workshop\n\njava -jar executables/kafka-event-generator-1.0-SNAPSHOT-jar-with-dependencies.jar\n```\n\n#### Halt the event generator\n\nWatch the console window that is running the event generator.  After a few hundred events have been generated, halt the event generator with **CTRL-C**.\n\n#### Halt the event consumers\n\nIn all console windows that are running event consumers, halt the program with **CTRL-C**.","dateUpdated":"2019-07-14T19:51:59+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>cell 9</p>\n<h3>Run the program that generates Kafka events</h3>\n<p>Start this program in a <strong>separate</strong> terminal window.<br/>Note that this will not work if you try to run from a shell in this notebook&hellip; Zeppelin will have a memory error.</p>\n<p>This standalone Java program reads a file of simulated event data, and publishes to Kafka at the rate of 2 events per second.</p>\n<pre><code>cd /tmp/datastax-spark-streaming-workshop\n\njava -jar executables/kafka-event-generator-1.0-SNAPSHOT-jar-with-dependencies.jar\n</code></pre>\n<h4>Halt the event generator</h4>\n<p>Watch the console window that is running the event generator. After a few hundred events have been generated, halt the event generator with <strong>CTRL-C</strong>.</p>\n<h4>Halt the event consumers</h4>\n<p>In all console windows that are running event consumers, halt the program with <strong>CTRL-C</strong>.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1563123580056_47817119","id":"20190711-132704_1461625321","dateCreated":"2019-07-14T16:59:40+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:8805","user":"anonymous","dateFinished":"2019-07-14T19:51:59+0000","dateStarted":"2019-07-14T19:51:59+0000"},{"text":"%cassandra\n\n// cell 10\n\n// Now read from the Good News Cassandra table to verify that events have been persisted to Cassandra.\n\nselect * from streaming_workshop.ratings_modifiers_good_news;\n","dateUpdated":"2019-07-14T19:53:05+0000","config":{"colWidth":12,"editorMode":"ace/mode/undefined","results":{},"enabled":true,"editorSetting":{"language":"text","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1563123580056_47817119","id":"20190710-202010_1573903280","dateCreated":"2019-07-14T16:59:40+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:8806","user":"anonymous","dateFinished":"2019-07-14T19:53:05+0000","dateStarted":"2019-07-14T19:53:05+0000"},{"text":"%cassandra\n\n// cell 11\n\n// Verify the count of Good News records written to Cassandra\n\nselect count(*) from streaming_workshop.ratings_modifiers_good_news;","dateUpdated":"2019-07-14T19:53:55+0000","config":{"colWidth":12,"editorMode":"ace/mode/undefined","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1563123580057_47432370","id":"20190711-133326_184074770","dateCreated":"2019-07-14T16:59:40+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:8807","user":"anonymous","dateFinished":"2019-07-14T19:53:55+0000","dateStarted":"2019-07-14T19:53:55+0000"},{"text":"%cassandra\n\n// cell 12\n\n// Now read from the Bad News Cassandra table to verify that events have been persisted to Cassandra.\n\nselect * from streaming_workshop.ratings_modifiers_bad_news;","dateUpdated":"2019-07-14T19:55:10+0000","config":{"colWidth":12,"editorMode":"ace/mode/undefined","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1563123580058_48586617","id":"20190713-184738_1565945305","dateCreated":"2019-07-14T16:59:40+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:8808","user":"anonymous","dateFinished":"2019-07-14T19:55:10+0000","dateStarted":"2019-07-14T19:55:10+0000"},{"text":"%cassandra\n\n// cell 13\n\n// Verify the count of Bad News records written to Cassandra\n\nselect count(*) from streaming_workshop.ratings_modifiers_bad_news;","dateUpdated":"2019-07-14T19:55:56+0000","config":{"colWidth":12,"editorMode":"ace/mode/undefined","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1563123580059_48201868","id":"20190713-184647_404211901","dateCreated":"2019-07-14T16:59:40+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:8809","user":"anonymous","dateFinished":"2019-07-14T19:55:56+0000","dateStarted":"2019-07-14T19:55:56+0000"},{"text":"%md\n\ncell 14\n\n### Summary... what just happened?\n\nIn this lab, we showed how to create a Kafka event pipeline to make workflow-style applications.  We ran a Spark program that selected certain transactions and published them to new Kafka topic queues.  Then we ran Spark jobs that consumed these queues and wrote the events to Cassandra tables.  This is a very simple example, but you could use this model to build very sophisticated applications.\n","dateUpdated":"2019-07-14T20:00:11+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>cell 14</p>\n<h3>Summary&hellip; what just happened?</h3>\n<p>In this lab, we showed how to create a Kafka event pipeline to make workflow-style applications. We ran a Spark program that selected certain transactions and published them to new Kafka topic queues. Then we ran Spark jobs that consumed these queues and wrote the events to Cassandra tables. This is a very simple example, but you could use this model to build very sophisticated applications.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1563123580067_32811912","id":"20190711-134124_2125497515","dateCreated":"2019-07-14T16:59:40+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:8819","user":"anonymous","dateFinished":"2019-07-14T20:00:11+0000","dateStarted":"2019-07-14T20:00:11+0000"},{"text":"%md\n","dateUpdated":"2019-07-14T16:59:40+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","results":{},"enabled":true,"editorSetting":{"language":"markdown","editOnDblClick":true}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1563123580068_30888167","id":"20190711-155658_1924356506","dateCreated":"2019-07-14T16:59:40+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:8820"}],"name":"Lab_3_-_Pipelining","id":"2EJ8DHYAN","angularObjects":{"2EG28KSU2:shared_process":[],"2EEXE9CCU:shared_process":[],"2EJFUM7KS:shared_process":[],"2EF1RQHWR:shared_process":[],"2EHY4N7GG:shared_process":[],"2EJ7B2WSV:shared_process":[],"2EJMP9YQP:shared_process":[],"2EH3K52KA:shared_process":[],"2EFJNJBVD:shared_process":[],"2EJQVRPW1:shared_process":[],"2EFTY74NT:shared_process":[],"2EGD7HQ76:shared_process":[],"2EJ2CKRUM:shared_process":[],"2EJDMSTUW:shared_process":[],"2EJBGHKBX:shared_process":[],"2EJC62C8G:shared_process":[],"2EHKTB5GU:shared_process":[],"2EGV9VJAW:shared_process":[],"2EJS5PD46:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}